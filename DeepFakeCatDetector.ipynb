{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKc8K9_57UEz"
      },
      "source": [
        "# CS224 - Spring 2024 - HW2 - Deepfake Cat Detector\n",
        "Submit **PDF** of completed IPython notebook on Canvas\n",
        "\n",
        "**Due**: February 22, 2024 @ 11:59pm PDT\n",
        "\n",
        "**Maximum points**: 15 (each HW is %15 of total grade)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsywQWEI8pzj"
      },
      "source": [
        "# Overview\n",
        "In this assignment you will implement some classifiers to predict whether or not images of cats are \"deepfakes\", i.e., generated by AI. (I used SD 1.5, and down-sampled to match CIFAR-10, which we use for real images.)\n",
        "\n",
        "For this assignment we will use the functionality of PyTorch, HuggingFace \"transformers\" library for getting pretrained models, scikit-learn (for cross validation utility and for baseline logistic regression), matplotlib for visualization. Before you start, make sure you have installed all those packages in your local Jupyter instance. Or use Google Colab (which has everything you need pre-installed).\n",
        "\n",
        "Read **all** cells carefully and answer **all** parts (both text and missing code). You will complete all the parts marked `TODO` and print desired results. (In some cases, this just means getting the code to work so the TODO section prints the correct result.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM7-Cx4H-jTt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# The following functions were discussed in week 4 demo\n",
        "import torch.nn as nn  # neural net layers and activations\n",
        "from torch.optim import SGD  # Our chosen optimizer\n",
        "from torch.utils.data import DataLoader, TensorDataset  # Super useful data utilities!\n",
        "\n",
        "# We discussed all these in week 3 demo:\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Used for visualization\n",
        "import torchvision.utils as vutils\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Turn off some annoying convergence warnings from sklearn\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR37r18FtEPo"
      },
      "source": [
        "## **Obtain and inspect data [3 points]**\n",
        "You can download the data file here:\n",
        " https://elearn.ucr.edu/courses/125165/files/12619307/download?download_frd=1\n",
        "You'll have to make them available locally or upload them to your colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "p2-zIA5vYOHA",
        "outputId": "84c45f3a-d8d5-401a-dfc1-81eb9bbdc25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Data shapes before flattening:\n",
            "X: torch.Size([2000, 3, 32, 32])\n",
            "y: torch.Size([2000])\n",
            "X shape after flattening: torch.Size([2000, 3072])\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load dataset and visualize\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "X, y = torch.load('/content/drive/MyDrive/CS224-FunadamentalsOfMachineLearning/HW2-DeepFakeCatDetector/hw2_data.pt')\n",
        "\n",
        "# X, y = torch.load('hw2_data.pt')\n",
        "\n",
        "print('Data shapes before flattening:')\n",
        "print('X:', X.shape)  # 2000, 3, 32, 32, 2000 images, channel, height width\n",
        "print('y:', y.shape)  # 2000 binary labels 0 is real, 1 is fake\n",
        "\n",
        "# Print examples from each class\n",
        "grid = vutils.make_grid(X[y==0][:8], nrow=4, padding=2, normalize=True)\n",
        "fig, axs = plt.subplots(2, 1, figsize=(8, 8))\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('REAL Cat images')\n",
        "axs[0].imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "grid = vutils.make_grid(X[y==1][:8], nrow=4, padding=2, normalize=True)\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('FAKE Cat images')\n",
        "axs[1].imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "\n",
        "X = X.flatten(start_dim=1)  # From now on, we work with the flattened vector\n",
        "print(f\"X shape after flattening: {X.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-3iLy_ax-KU",
        "outputId": "617b88e6-30d8-4118-df0a-662054da2fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy of each fold - [0.991875, 0.99375, 0.993125, 0.99, 0.99125]\n",
            "Test Accuracy of each fold - [0.625, 0.5825, 0.6475, 0.6225, 0.63]\n",
            "Train Accuracy and standard error:\t 0.992 +/- 0.001\n",
            "Validation Accuracy and standard error:\t 0.622 +/- 0.011\n"
          ]
        }
      ],
      "source": [
        "# TODO [3 points]:\n",
        "# Use scikit-learn logistic regression (with default hyper-parameters)\n",
        "# with 5-fold CV to get the train and validation accuracies\n",
        "# for a simple linear classifier - a good baseline for our MLP\n",
        "n_folds = 5\n",
        "val_accs = []  # store validation accuracy for each fold\n",
        "train_accs = []  # store training accuracy for each fold\n",
        "\n",
        "# TODO: iterate over folds, remember to use \"shuffle=True\", as datapoints are not shuffled\n",
        "\n",
        "# Used this website as a reference - https://www.askpython.com/python/examples/k-fold-cross-validation\n",
        "\n",
        "myKFold = KFold(n_splits = n_folds, shuffle = True, random_state = 0)\n",
        "model = LogisticRegression(solver = 'liblinear')\n",
        "\n",
        "# Dimensions of X = 2000*3072\n",
        "# Dimensions of Y = 2000\n",
        "for trainIndex, testIndex in myKFold.split(X):\n",
        "    XTrain, XTest = X[trainIndex,:], X[testIndex,:]\n",
        "    yTrain, yTest = y[trainIndex], y[testIndex]\n",
        "\n",
        "    # TODO: Fit model on training data\n",
        "    model.fit(XTrain, yTrain)\n",
        "    yTrainPred = model.predict(XTrain)\n",
        "    yTestPred = model.predict(XTest)\n",
        "\n",
        "    # TODO: Compute and store accuracy on train data\n",
        "\n",
        "    train_acc = accuracy_score(yTrainPred, yTrain)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # TODO: Compute and store accuracy on validation data\n",
        "\n",
        "    val_acc = accuracy_score(yTestPred, yTest)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "train_std, train_mean = torch.std_mean(torch.tensor(train_accs))\n",
        "val_std, val_mean = torch.std_mean(torch.tensor(val_accs))\n",
        "\n",
        "# Standard error is standard deviation / sqrt(n), it is more typical to report this\n",
        "rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds\n",
        "\n",
        "print('Train Accuracy of each fold - {}'.format(train_accs))\n",
        "print('Test Accuracy of each fold - {}'.format(val_accs))\n",
        "\n",
        "print(f'Train Accuracy and standard error:\\t {train_mean:.3f} +/- {train_std / rootn:.3f}')\n",
        "print(f'Validation Accuracy and standard error:\\t {val_mean:.3f} +/- {val_std / rootn:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBodiVzwx-KV"
      },
      "source": [
        "## Define the model [3 points]\n",
        "\n",
        "- As always, implement an __init__ function and a forward function\n",
        "- Use Linear layers with ReLU activations for the hidden layers\n",
        "- 2 layers of hidden units. First layer has 128 hidden units, second layer has 64 hidden units.\n",
        "- Output represents *binary* logits (must have correct shape to do that!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "265qk4h2YUrc"
      },
      "outputs": [],
      "source": [
        "class MyMLP(nn.Module):\n",
        "    # TODO: Define a multilayer perceptron [3 points]. Criteria above\n",
        "    # Referred my own HW1 extra credit part an took help from TA\n",
        "\n",
        "    def __init__(self, inputDimension = 3072, numOfClass = 2):\n",
        "        super(MyMLP, self).__init__()\n",
        "        self.d = inputDimension\n",
        "        self.fc1 = nn.Linear(self.d, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, numOfClass)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs8i6aVIx-KW"
      },
      "source": [
        "## Train function [6 points]\n",
        "Make a function to train your neural net, following week 4 example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ijbh6rJx-KW"
      },
      "outputs": [],
      "source": [
        "# TODO [3 points]: a function to train your model\n",
        "# (this will called for each hyper-parameter and fold)\n",
        "# Don't forget to set model.train() during training, then model.eval() after done\n",
        "# It doesn't matter in this case, but is good practice to prevent future bugs.\n",
        "\n",
        "# Referenced this website - https://neptune.ai/blog/pytorch-loss-functions\n",
        "\n",
        "def train(model, train_loader, val_loader, n_epochs, optimizer, criterion, verbose=False):\n",
        "    \"\"\"Train model using data from train_loader over n_epochs,\n",
        "    using a Pytorch \"optimizer\" object (SGD in this case)\n",
        "    and \"criterion\" as the loss function (CrossEntropyLoss in this case).\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # TODO: Train loop\n",
        "        print (\"This is \", epoch + 1, \"th epoch\")\n",
        "        # Train mode\n",
        "        model.train()\n",
        "\n",
        "        # Set the current loss and validation loss value to zero\n",
        "        currentLoss = 0\n",
        "        validationLoss = 0\n",
        "\n",
        "        #Iterate over training data\n",
        "        for i, data in enumerate(train_loader, 0):    # trainloader, 0 -> 0 signifies training from scratch for each epoch\n",
        "            inputs, trueLabels = data\n",
        "\n",
        "            # Zero the gradients from the last loss.backward() to start anew for each batch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get predictions for inputs\n",
        "            predLabels = model(inputs)\n",
        "\n",
        "            # Calculate the loss using the criterion\n",
        "            loss = criterion(predLabels, trueLabels)\n",
        "\n",
        "            # Use backpropogation to find the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            print(\"Batch Training Loss = \",loss.item())\n",
        "\n",
        "        if verbose:\n",
        "            # Optional: Validation loop\n",
        "            # Print out train/val loss during development\n",
        "            # User verbose=False to turn off output of this in the submitted PDF\n",
        "\n",
        "            # Eval mode\n",
        "            model.eval()\n",
        "\n",
        "            # To find validation loss\n",
        "            # Get prediction on validation set and then call loss function\n",
        "            # But first torch.no_grad()\n",
        "            with torch.no_grad():\n",
        "                for data in val_loader:\n",
        "                    inputs, trueLabels = data\n",
        "                    predLabels = model(inputs)\n",
        "\n",
        "                    # Calculate the loss using the criterion\n",
        "                    loss = criterion(predLabels, trueLabels)\n",
        "                    validationLoss += loss.item() / len(val_loader)\n",
        "                    print('Validation Loss in this epoch is %.3f' %validationLoss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5warGzNqx-KW"
      },
      "source": [
        "Loop over hyper-parameters and do 5-fold cross-validation for each setting, saving the train and validation mean accuracy and standard error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-VNWv1ex-KX",
        "outputId": "7828f11b-1696-4197-be64-347893411d6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch Training Loss =  0.009962117299437523\n",
            "Validation Loss in this epoch is 0.610\n",
            "This is  54 th epoch\n",
            "Batch Training Loss =  0.014653343707323074\n",
            "Batch Training Loss =  0.021667368710041046\n",
            "Batch Training Loss =  0.01874551922082901\n",
            "Batch Training Loss =  0.010121243074536324\n",
            "Batch Training Loss =  0.009626262821257114\n",
            "Batch Training Loss =  0.01956731267273426\n",
            "Batch Training Loss =  0.011670887470245361\n",
            "Batch Training Loss =  0.013614343479275703\n",
            "Batch Training Loss =  0.008769767358899117\n",
            "Batch Training Loss =  0.011160261929035187\n",
            "Batch Training Loss =  0.019418034702539444\n",
            "Batch Training Loss =  0.025508452206850052\n",
            "Batch Training Loss =  0.03344376012682915\n",
            "Batch Training Loss =  0.039954256266355515\n",
            "Batch Training Loss =  0.014760162681341171\n",
            "Batch Training Loss =  0.011721516959369183\n",
            "Batch Training Loss =  0.010644908994436264\n",
            "Batch Training Loss =  0.014633812010288239\n",
            "Batch Training Loss =  0.014834785833954811\n",
            "Batch Training Loss =  0.011783375404775143\n",
            "Batch Training Loss =  0.01880166307091713\n",
            "Batch Training Loss =  0.012633725069463253\n",
            "Batch Training Loss =  0.011675586923956871\n",
            "Batch Training Loss =  0.014093365520238876\n",
            "Batch Training Loss =  0.014959764666855335\n",
            "Batch Training Loss =  0.015484883449971676\n",
            "Batch Training Loss =  0.006939966697245836\n",
            "Batch Training Loss =  0.0118908965960145\n",
            "Batch Training Loss =  0.017869997769594193\n",
            "Batch Training Loss =  0.042814966291189194\n",
            "Batch Training Loss =  0.03160420060157776\n",
            "Batch Training Loss =  0.013394679874181747\n",
            "Validation Loss in this epoch is 0.643\n",
            "This is  55 th epoch\n",
            "Batch Training Loss =  0.016747625544667244\n",
            "Batch Training Loss =  0.014280831441283226\n",
            "Batch Training Loss =  0.013166610151529312\n",
            "Batch Training Loss =  0.0105123994871974\n",
            "Batch Training Loss =  0.016271919012069702\n",
            "Batch Training Loss =  0.009172327816486359\n",
            "Batch Training Loss =  0.009341181255877018\n",
            "Batch Training Loss =  0.00954343006014824\n",
            "Batch Training Loss =  0.011409248225390911\n",
            "Batch Training Loss =  0.011098835617303848\n",
            "Batch Training Loss =  0.010140460915863514\n",
            "Batch Training Loss =  0.010757927782833576\n",
            "Batch Training Loss =  0.00969112478196621\n",
            "Batch Training Loss =  0.012197569943964481\n",
            "Batch Training Loss =  0.007690479978919029\n",
            "Batch Training Loss =  0.014959190040826797\n",
            "Batch Training Loss =  0.04832158237695694\n",
            "Batch Training Loss =  0.026808137074112892\n",
            "Batch Training Loss =  0.030338618904352188\n",
            "Batch Training Loss =  0.014803996309638023\n",
            "Batch Training Loss =  0.008211769163608551\n",
            "Batch Training Loss =  0.017235375940799713\n",
            "Batch Training Loss =  0.04152271896600723\n",
            "Batch Training Loss =  0.019138388335704803\n",
            "Batch Training Loss =  0.011768791824579239\n",
            "Batch Training Loss =  0.012101376429200172\n",
            "Batch Training Loss =  0.013182014226913452\n",
            "Batch Training Loss =  0.03370333090424538\n",
            "Batch Training Loss =  0.03009173460304737\n",
            "Batch Training Loss =  0.011109423823654652\n",
            "Batch Training Loss =  0.012625474482774734\n",
            "Batch Training Loss =  0.011856467463076115\n",
            "Validation Loss in this epoch is 0.623\n",
            "This is  56 th epoch\n",
            "Batch Training Loss =  0.010960825718939304\n",
            "Batch Training Loss =  0.012373625300824642\n",
            "Batch Training Loss =  0.013269500806927681\n",
            "Batch Training Loss =  0.010127860121428967\n",
            "Batch Training Loss =  0.009563851170241833\n",
            "Batch Training Loss =  0.01231192983686924\n",
            "Batch Training Loss =  0.014283432625234127\n",
            "Batch Training Loss =  0.01217888668179512\n",
            "Batch Training Loss =  0.006859919521957636\n",
            "Batch Training Loss =  0.01088267657905817\n",
            "Batch Training Loss =  0.011566316708922386\n",
            "Batch Training Loss =  0.010629666969180107\n",
            "Batch Training Loss =  0.018264811486005783\n",
            "Batch Training Loss =  0.05113576725125313\n",
            "Batch Training Loss =  0.014274614863097668\n",
            "Batch Training Loss =  0.020072095096111298\n",
            "Batch Training Loss =  0.013806969858705997\n",
            "Batch Training Loss =  0.01050403993576765\n",
            "Batch Training Loss =  0.023623961955308914\n",
            "Batch Training Loss =  0.013342060148715973\n",
            "Batch Training Loss =  0.014947440475225449\n",
            "Batch Training Loss =  0.010388974100351334\n",
            "Batch Training Loss =  0.030193457379937172\n",
            "Batch Training Loss =  0.02173887938261032\n",
            "Batch Training Loss =  0.01992241106927395\n",
            "Batch Training Loss =  0.013709446415305138\n",
            "Batch Training Loss =  0.014447345398366451\n",
            "Batch Training Loss =  0.012195536866784096\n",
            "Batch Training Loss =  0.012955325655639172\n",
            "Batch Training Loss =  0.008713207207620144\n",
            "Batch Training Loss =  0.011926133185625076\n",
            "Batch Training Loss =  0.009459624998271465\n",
            "Validation Loss in this epoch is 0.614\n",
            "This is  57 th epoch\n",
            "Batch Training Loss =  0.02322881482541561\n",
            "Batch Training Loss =  0.014125518500804901\n",
            "Batch Training Loss =  0.026421571150422096\n",
            "Batch Training Loss =  0.009953253902494907\n",
            "Batch Training Loss =  0.01296914741396904\n",
            "Batch Training Loss =  0.011554565280675888\n",
            "Batch Training Loss =  0.010921154171228409\n",
            "Batch Training Loss =  0.012003770098090172\n",
            "Batch Training Loss =  0.01508081704378128\n",
            "Batch Training Loss =  0.008613894693553448\n",
            "Batch Training Loss =  0.011841041035950184\n",
            "Batch Training Loss =  0.00984000414609909\n",
            "Batch Training Loss =  0.00807679072022438\n",
            "Batch Training Loss =  0.010001045651733875\n",
            "Batch Training Loss =  0.009899714030325413\n",
            "Batch Training Loss =  0.014074224978685379\n",
            "Batch Training Loss =  0.011381533928215504\n",
            "Batch Training Loss =  0.01004176214337349\n",
            "Batch Training Loss =  0.011724130250513554\n",
            "Batch Training Loss =  0.010051454417407513\n",
            "Batch Training Loss =  0.01423570979386568\n",
            "Batch Training Loss =  0.016821037977933884\n",
            "Batch Training Loss =  0.009390157647430897\n",
            "Batch Training Loss =  0.011399737559258938\n",
            "Batch Training Loss =  0.008864893577992916\n",
            "Batch Training Loss =  0.043251655995845795\n",
            "Batch Training Loss =  0.031329963356256485\n",
            "Batch Training Loss =  0.015168985351920128\n",
            "Batch Training Loss =  0.037955109030008316\n",
            "Batch Training Loss =  0.01368213165551424\n",
            "Batch Training Loss =  0.02756931073963642\n",
            "Batch Training Loss =  0.01565665565431118\n",
            "Validation Loss in this epoch is 0.603\n",
            "This is  58 th epoch\n",
            "Batch Training Loss =  0.010793386958539486\n",
            "Batch Training Loss =  0.013306893408298492\n",
            "Batch Training Loss =  0.012258087284862995\n",
            "Batch Training Loss =  0.011653950437903404\n",
            "Batch Training Loss =  0.009869744069874287\n",
            "Batch Training Loss =  0.014367371797561646\n",
            "Batch Training Loss =  0.01211882010102272\n",
            "Batch Training Loss =  0.011294242925941944\n",
            "Batch Training Loss =  0.007867916487157345\n",
            "Batch Training Loss =  0.011613372713327408\n",
            "Batch Training Loss =  0.015045671723783016\n",
            "Batch Training Loss =  0.013669644482433796\n",
            "Batch Training Loss =  0.012058353051543236\n",
            "Batch Training Loss =  0.038740143179893494\n",
            "Batch Training Loss =  0.029824400320649147\n",
            "Batch Training Loss =  0.020568296313285828\n",
            "Batch Training Loss =  0.016701074317097664\n",
            "Batch Training Loss =  0.01770412176847458\n",
            "Batch Training Loss =  0.018579736351966858\n",
            "Batch Training Loss =  0.025355566293001175\n",
            "Batch Training Loss =  0.01994493044912815\n",
            "Batch Training Loss =  0.025243334472179413\n",
            "Batch Training Loss =  0.016940642148256302\n",
            "Batch Training Loss =  0.008339245803654194\n",
            "Batch Training Loss =  0.01793533004820347\n",
            "Batch Training Loss =  0.014082290232181549\n",
            "Batch Training Loss =  0.023036707192659378\n",
            "Batch Training Loss =  0.035771049559116364\n",
            "Batch Training Loss =  0.015660831704735756\n",
            "Batch Training Loss =  0.014723225496709347\n",
            "Batch Training Loss =  0.023382384330034256\n",
            "Batch Training Loss =  0.01990853250026703\n",
            "Validation Loss in this epoch is 0.624\n",
            "This is  59 th epoch\n",
            "Batch Training Loss =  0.022525541484355927\n",
            "Batch Training Loss =  0.012763852253556252\n",
            "Batch Training Loss =  0.010571939870715141\n",
            "Batch Training Loss =  0.00924663431942463\n",
            "Batch Training Loss =  0.01879381388425827\n",
            "Batch Training Loss =  0.01033509336411953\n",
            "Batch Training Loss =  0.011230451986193657\n",
            "Batch Training Loss =  0.015413114801049232\n",
            "Batch Training Loss =  0.009954784996807575\n",
            "Batch Training Loss =  0.02892163209617138\n",
            "Batch Training Loss =  0.013558962382376194\n",
            "Batch Training Loss =  0.014194589108228683\n",
            "Batch Training Loss =  0.010274183936417103\n",
            "Batch Training Loss =  0.010805618017911911\n",
            "Batch Training Loss =  0.009789136238396168\n",
            "Batch Training Loss =  0.011814474128186703\n",
            "Batch Training Loss =  0.01333521492779255\n",
            "Batch Training Loss =  0.014272191561758518\n",
            "Batch Training Loss =  0.013175273314118385\n",
            "Batch Training Loss =  0.03262883797287941\n",
            "Batch Training Loss =  0.03476898744702339\n",
            "Batch Training Loss =  0.012079472653567791\n",
            "Batch Training Loss =  0.05320843681693077\n",
            "Batch Training Loss =  0.04954095184803009\n",
            "Batch Training Loss =  0.033038269728422165\n",
            "Batch Training Loss =  0.01705101691186428\n",
            "Batch Training Loss =  0.01489484403282404\n",
            "Batch Training Loss =  0.01266586221754551\n",
            "Batch Training Loss =  0.01224445179104805\n",
            "Batch Training Loss =  0.01645938865840435\n",
            "Batch Training Loss =  0.016589734703302383\n",
            "Batch Training Loss =  0.010525163263082504\n",
            "Validation Loss in this epoch is 0.592\n",
            "This is  60 th epoch\n",
            "Batch Training Loss =  0.007740978617221117\n",
            "Batch Training Loss =  0.008651524782180786\n",
            "Batch Training Loss =  0.015316121280193329\n",
            "Batch Training Loss =  0.038451213389635086\n",
            "Batch Training Loss =  0.02906452678143978\n",
            "Batch Training Loss =  0.01363453734666109\n",
            "Batch Training Loss =  0.012590897269546986\n",
            "Batch Training Loss =  0.017757270485162735\n",
            "Batch Training Loss =  0.020160820335149765\n",
            "Batch Training Loss =  0.0108706746250391\n",
            "Batch Training Loss =  0.010486410930752754\n",
            "Batch Training Loss =  0.014076389372348785\n",
            "Batch Training Loss =  0.013006549328565598\n",
            "Batch Training Loss =  0.014574835076928139\n",
            "Batch Training Loss =  0.012545420788228512\n",
            "Batch Training Loss =  0.013030754402279854\n",
            "Batch Training Loss =  0.057231102138757706\n",
            "Batch Training Loss =  0.032174717634916306\n",
            "Batch Training Loss =  0.022470327094197273\n",
            "Batch Training Loss =  0.017158590257167816\n",
            "Batch Training Loss =  0.019122973084449768\n",
            "Batch Training Loss =  0.009562813676893711\n",
            "Batch Training Loss =  0.02052195370197296\n",
            "Batch Training Loss =  0.0168626569211483\n",
            "Batch Training Loss =  0.023361986503005028\n",
            "Batch Training Loss =  0.017436396330595016\n",
            "Batch Training Loss =  0.016049426048994064\n",
            "Batch Training Loss =  0.0150047792121768\n",
            "Batch Training Loss =  0.012191965244710445\n",
            "Batch Training Loss =  0.01250060461461544\n",
            "Batch Training Loss =  0.014341074042022228\n",
            "Batch Training Loss =  0.010734017007052898\n",
            "Validation Loss in this epoch is 0.590\n",
            "This is  61 th epoch\n",
            "Batch Training Loss =  0.012628856115043163\n",
            "Batch Training Loss =  0.010379055514931679\n",
            "Batch Training Loss =  0.012805047444999218\n",
            "Batch Training Loss =  0.026483280584216118\n",
            "Batch Training Loss =  0.011960054747760296\n",
            "Batch Training Loss =  0.019446736201643944\n",
            "Batch Training Loss =  0.009390633553266525\n",
            "Batch Training Loss =  0.0214920025318861\n",
            "Batch Training Loss =  0.012591718696057796\n",
            "Batch Training Loss =  0.01008872501552105\n",
            "Batch Training Loss =  0.01332066673785448\n",
            "Batch Training Loss =  0.009528259746730328\n",
            "Batch Training Loss =  0.01048894040286541\n",
            "Batch Training Loss =  0.008026260882616043\n",
            "Batch Training Loss =  0.011372030712664127\n",
            "Batch Training Loss =  0.02067701891064644\n",
            "Batch Training Loss =  0.013931814581155777\n",
            "Batch Training Loss =  0.02339443936944008\n",
            "Batch Training Loss =  0.011809740215539932\n",
            "Batch Training Loss =  0.013933170586824417\n",
            "Batch Training Loss =  0.0072944010607898235\n",
            "Batch Training Loss =  0.01007553469389677\n",
            "Batch Training Loss =  0.011408809572458267\n",
            "Batch Training Loss =  0.009569094516336918\n",
            "Batch Training Loss =  0.013397886417806149\n",
            "Batch Training Loss =  0.011317373253405094\n",
            "Batch Training Loss =  0.010010319761931896\n",
            "Batch Training Loss =  0.014491269364953041\n",
            "Batch Training Loss =  0.009752304293215275\n",
            "Batch Training Loss =  0.017429940402507782\n",
            "Batch Training Loss =  0.010079540312290192\n",
            "Batch Training Loss =  0.009736345149576664\n",
            "Validation Loss in this epoch is 0.590\n",
            "This is  62 th epoch\n",
            "Batch Training Loss =  0.0071676005609333515\n",
            "Batch Training Loss =  0.013948441483080387\n",
            "Batch Training Loss =  0.05226072669029236\n",
            "Batch Training Loss =  0.024270689114928246\n",
            "Batch Training Loss =  0.012060373090207577\n",
            "Batch Training Loss =  0.027233725413680077\n",
            "Batch Training Loss =  0.017773009836673737\n",
            "Batch Training Loss =  0.01078485231846571\n",
            "Batch Training Loss =  0.012445291504263878\n",
            "Batch Training Loss =  0.019609134644269943\n",
            "Batch Training Loss =  0.02283690683543682\n",
            "Batch Training Loss =  0.015134785324335098\n",
            "Batch Training Loss =  0.015575346536934376\n",
            "Batch Training Loss =  0.012663355097174644\n",
            "Batch Training Loss =  0.011434362269937992\n",
            "Batch Training Loss =  0.011552789248526096\n",
            "Batch Training Loss =  0.009738359600305557\n",
            "Batch Training Loss =  0.013390365056693554\n",
            "Batch Training Loss =  0.011985979042947292\n",
            "Batch Training Loss =  0.009278528392314911\n",
            "Batch Training Loss =  0.010438906028866768\n",
            "Batch Training Loss =  0.010150419548153877\n",
            "Batch Training Loss =  0.01706176996231079\n",
            "Batch Training Loss =  0.009270839393138885\n",
            "Batch Training Loss =  0.019128048792481422\n",
            "Batch Training Loss =  0.013938801363110542\n",
            "Batch Training Loss =  0.011002922430634499\n",
            "Batch Training Loss =  0.01576470583677292\n",
            "Batch Training Loss =  0.07178113609552383\n",
            "Batch Training Loss =  0.05790428817272186\n",
            "Batch Training Loss =  0.018772371113300323\n",
            "Batch Training Loss =  0.023051023483276367\n",
            "Validation Loss in this epoch is 0.611\n",
            "This is  63 th epoch\n",
            "Batch Training Loss =  0.017543019726872444\n",
            "Batch Training Loss =  0.01449179369956255\n",
            "Batch Training Loss =  0.014309119433164597\n",
            "Batch Training Loss =  0.03244537487626076\n",
            "Batch Training Loss =  0.023498600348830223\n",
            "Batch Training Loss =  0.018115956336259842\n",
            "Batch Training Loss =  0.015539845451712608\n",
            "Batch Training Loss =  0.018102936446666718\n",
            "Batch Training Loss =  0.04401113837957382\n",
            "Batch Training Loss =  0.01145616639405489\n",
            "Batch Training Loss =  0.018186504021286964\n",
            "Batch Training Loss =  0.01634598895907402\n",
            "Batch Training Loss =  0.015365982428193092\n",
            "Batch Training Loss =  0.021127231419086456\n",
            "Batch Training Loss =  0.01391935907304287\n",
            "Batch Training Loss =  0.010487357154488564\n",
            "Batch Training Loss =  0.012375755235552788\n",
            "Batch Training Loss =  0.014533063396811485\n",
            "Batch Training Loss =  0.012269617058336735\n",
            "Batch Training Loss =  0.0657564327120781\n",
            "Batch Training Loss =  0.014133505523204803\n",
            "Batch Training Loss =  0.01730613224208355\n",
            "Batch Training Loss =  0.014358647167682648\n",
            "Batch Training Loss =  0.020844722166657448\n",
            "Batch Training Loss =  0.014776041731238365\n",
            "Batch Training Loss =  0.016788063570857048\n",
            "Batch Training Loss =  0.012783218175172806\n",
            "Batch Training Loss =  0.023621058091521263\n",
            "Batch Training Loss =  0.016937697306275368\n",
            "Batch Training Loss =  0.009988714009523392\n",
            "Batch Training Loss =  0.013823655433952808\n",
            "Batch Training Loss =  0.012744870036840439\n",
            "Validation Loss in this epoch is 0.610\n",
            "This is  64 th epoch\n",
            "Batch Training Loss =  0.01295457687228918\n",
            "Batch Training Loss =  0.010795224457979202\n",
            "Batch Training Loss =  0.009865430183708668\n",
            "Batch Training Loss =  0.013374409638345242\n",
            "Batch Training Loss =  0.013810020871460438\n",
            "Batch Training Loss =  0.013673015870153904\n",
            "Batch Training Loss =  0.013274943456053734\n",
            "Batch Training Loss =  0.010696527548134327\n",
            "Batch Training Loss =  0.009659456089138985\n",
            "Batch Training Loss =  0.0495922714471817\n",
            "Batch Training Loss =  0.012213454581797123\n",
            "Batch Training Loss =  0.011293914169073105\n",
            "Batch Training Loss =  0.014037144370377064\n",
            "Batch Training Loss =  0.010788981802761555\n",
            "Batch Training Loss =  0.010773543268442154\n",
            "Batch Training Loss =  0.018232431262731552\n",
            "Batch Training Loss =  0.017784902825951576\n",
            "Batch Training Loss =  0.018985021859407425\n",
            "Batch Training Loss =  0.012000305578112602\n",
            "Batch Training Loss =  0.012174799107015133\n",
            "Batch Training Loss =  0.010604753158986568\n",
            "Batch Training Loss =  0.009826172143220901\n",
            "Batch Training Loss =  0.037255436182022095\n",
            "Batch Training Loss =  0.049487847834825516\n",
            "Batch Training Loss =  0.08907816559076309\n",
            "Batch Training Loss =  0.02613060548901558\n",
            "Batch Training Loss =  0.04043905809521675\n",
            "Batch Training Loss =  0.04672950133681297\n",
            "Batch Training Loss =  0.11386533081531525\n",
            "Batch Training Loss =  0.19208921492099762\n",
            "Batch Training Loss =  2.472886085510254\n",
            "Batch Training Loss =  3.4329965114593506\n",
            "Validation Loss in this epoch is 0.992\n",
            "This is  65 th epoch\n",
            "Batch Training Loss =  0.5162919759750366\n",
            "Batch Training Loss =  0.2687438726425171\n",
            "Batch Training Loss =  0.20182207226753235\n",
            "Batch Training Loss =  0.248832568526268\n",
            "Batch Training Loss =  0.21915945410728455\n",
            "Batch Training Loss =  0.15084029734134674\n",
            "Batch Training Loss =  0.09447026997804642\n",
            "Batch Training Loss =  0.21298928558826447\n",
            "Batch Training Loss =  0.20138849318027496\n",
            "Batch Training Loss =  0.1919803023338318\n",
            "Batch Training Loss =  0.17208053171634674\n",
            "Batch Training Loss =  0.35299086570739746\n",
            "Batch Training Loss =  0.23280152678489685\n",
            "Batch Training Loss =  0.24304752051830292\n",
            "Batch Training Loss =  0.2556004524230957\n",
            "Batch Training Loss =  0.11244703084230423\n",
            "Batch Training Loss =  0.1621619462966919\n",
            "Batch Training Loss =  0.21923819184303284\n",
            "Batch Training Loss =  0.19654545187950134\n",
            "Batch Training Loss =  0.2749401032924652\n",
            "Batch Training Loss =  0.2458069622516632\n",
            "Batch Training Loss =  0.08757314085960388\n",
            "Batch Training Loss =  0.14408205449581146\n",
            "Batch Training Loss =  0.1776435226202011\n",
            "Batch Training Loss =  0.12496832013130188\n",
            "Batch Training Loss =  0.3237959146499634\n",
            "Batch Training Loss =  0.10490108281373978\n",
            "Batch Training Loss =  0.23000282049179077\n",
            "Batch Training Loss =  0.18916165828704834\n",
            "Batch Training Loss =  0.07781077921390533\n",
            "Batch Training Loss =  0.11851003021001816\n",
            "Batch Training Loss =  0.07548657804727554\n",
            "Validation Loss in this epoch is 0.593\n",
            "This is  66 th epoch\n",
            "Batch Training Loss =  0.05794825032353401\n",
            "Batch Training Loss =  0.07381047308444977\n",
            "Batch Training Loss =  0.03883560001850128\n",
            "Batch Training Loss =  0.052643660455942154\n",
            "Batch Training Loss =  0.04415968433022499\n",
            "Batch Training Loss =  0.05312208831310272\n",
            "Batch Training Loss =  0.04126238450407982\n",
            "Batch Training Loss =  0.03018856979906559\n",
            "Batch Training Loss =  0.14796072244644165\n",
            "Batch Training Loss =  0.04023335501551628\n",
            "Batch Training Loss =  0.047728247940540314\n",
            "Batch Training Loss =  0.084315225481987\n",
            "Batch Training Loss =  0.04835409298539162\n",
            "Batch Training Loss =  0.035670146346092224\n",
            "Batch Training Loss =  0.03885767608880997\n",
            "Batch Training Loss =  0.044459082186222076\n",
            "Batch Training Loss =  0.03737964853644371\n",
            "Batch Training Loss =  0.03434211388230324\n",
            "Batch Training Loss =  0.03850255534052849\n",
            "Batch Training Loss =  0.042467955499887466\n",
            "Batch Training Loss =  0.02700567990541458\n",
            "Batch Training Loss =  0.02949799783527851\n",
            "Batch Training Loss =  0.04197154566645622\n",
            "Batch Training Loss =  0.019331781193614006\n",
            "Batch Training Loss =  0.15526340901851654\n",
            "Batch Training Loss =  0.10359368473291397\n",
            "Batch Training Loss =  0.0441012866795063\n",
            "Batch Training Loss =  0.04801430180668831\n",
            "Batch Training Loss =  0.059794988483190536\n",
            "Batch Training Loss =  0.044123563915491104\n",
            "Batch Training Loss =  0.040488023310899734\n",
            "Batch Training Loss =  0.04862872511148453\n",
            "Validation Loss in this epoch is 0.630\n",
            "This is  67 th epoch\n",
            "Batch Training Loss =  0.02331686206161976\n",
            "Batch Training Loss =  0.026446212083101273\n",
            "Batch Training Loss =  0.034564532339572906\n",
            "Batch Training Loss =  0.025012308731675148\n",
            "Batch Training Loss =  0.03584668040275574\n",
            "Batch Training Loss =  0.023470453917980194\n",
            "Batch Training Loss =  0.02569645829498768\n",
            "Batch Training Loss =  0.021476566791534424\n",
            "Batch Training Loss =  0.021516675129532814\n",
            "Batch Training Loss =  0.08767418563365936\n",
            "Batch Training Loss =  0.01483363751322031\n",
            "Batch Training Loss =  0.04518129304051399\n",
            "Batch Training Loss =  0.039156172424554825\n",
            "Batch Training Loss =  0.04781686142086983\n",
            "Batch Training Loss =  0.03065510280430317\n",
            "Batch Training Loss =  0.025753216817975044\n",
            "Batch Training Loss =  0.028662340715527534\n",
            "Batch Training Loss =  0.026662496849894524\n",
            "Batch Training Loss =  0.020016491413116455\n",
            "Batch Training Loss =  0.04496040195226669\n",
            "Batch Training Loss =  0.04064525291323662\n",
            "Batch Training Loss =  0.0209397841244936\n",
            "Batch Training Loss =  0.04742716625332832\n",
            "Batch Training Loss =  0.018205737695097923\n",
            "Batch Training Loss =  0.023435523733496666\n",
            "Batch Training Loss =  0.03966892138123512\n",
            "Batch Training Loss =  0.014649740420281887\n",
            "Batch Training Loss =  0.025654731318354607\n",
            "Batch Training Loss =  0.050553444772958755\n",
            "Batch Training Loss =  0.021868526935577393\n",
            "Batch Training Loss =  0.01660667173564434\n",
            "Batch Training Loss =  0.015527630224823952\n",
            "Validation Loss in this epoch is 0.607\n",
            "This is  68 th epoch\n",
            "Batch Training Loss =  0.012436985969543457\n",
            "Batch Training Loss =  0.013316468335688114\n",
            "Batch Training Loss =  0.015020795166492462\n",
            "Batch Training Loss =  0.01405090931802988\n",
            "Batch Training Loss =  0.019694466143846512\n",
            "Batch Training Loss =  0.016265615820884705\n",
            "Batch Training Loss =  0.021391119807958603\n",
            "Batch Training Loss =  0.016071632504463196\n",
            "Batch Training Loss =  0.0350508913397789\n",
            "Batch Training Loss =  0.04587295651435852\n",
            "Batch Training Loss =  0.020115604624152184\n",
            "Batch Training Loss =  0.022270873188972473\n",
            "Batch Training Loss =  0.013486756011843681\n",
            "Batch Training Loss =  0.014111357741057873\n",
            "Batch Training Loss =  0.03460693359375\n",
            "Batch Training Loss =  0.014143959619104862\n",
            "Batch Training Loss =  0.017690176144242287\n",
            "Batch Training Loss =  0.025363115593791008\n",
            "Batch Training Loss =  0.02063446305692196\n",
            "Batch Training Loss =  0.019713139161467552\n",
            "Batch Training Loss =  0.02549120970070362\n",
            "Batch Training Loss =  0.010005835443735123\n",
            "Batch Training Loss =  0.024569127708673477\n",
            "Batch Training Loss =  0.011267315596342087\n",
            "Batch Training Loss =  0.01602063700556755\n",
            "Batch Training Loss =  0.015474888496100903\n",
            "Batch Training Loss =  0.011333146132528782\n",
            "Batch Training Loss =  0.01418160181492567\n",
            "Batch Training Loss =  0.020776627585291862\n",
            "Batch Training Loss =  0.01016941387206316\n",
            "Batch Training Loss =  0.0214067529886961\n",
            "Batch Training Loss =  0.011446437798440456\n",
            "Validation Loss in this epoch is 0.628\n",
            "This is  69 th epoch\n",
            "Batch Training Loss =  0.012666493654251099\n",
            "Batch Training Loss =  0.009422061033546925\n",
            "Batch Training Loss =  0.015994422137737274\n",
            "Batch Training Loss =  0.05267230048775673\n",
            "Batch Training Loss =  0.02848740853369236\n",
            "Batch Training Loss =  0.01802024245262146\n",
            "Batch Training Loss =  0.00853448174893856\n",
            "Batch Training Loss =  0.03778697922825813\n",
            "Batch Training Loss =  0.025936700403690338\n",
            "Batch Training Loss =  0.01129032950848341\n",
            "Batch Training Loss =  0.021869130432605743\n",
            "Batch Training Loss =  0.014226596802473068\n",
            "Batch Training Loss =  0.011303826235234737\n",
            "Batch Training Loss =  0.015761718153953552\n",
            "Batch Training Loss =  0.015929730609059334\n",
            "Batch Training Loss =  0.01223006471991539\n",
            "Batch Training Loss =  0.03898043930530548\n",
            "Batch Training Loss =  0.012671388685703278\n",
            "Batch Training Loss =  0.01176698599010706\n",
            "Batch Training Loss =  0.017488017678260803\n",
            "Batch Training Loss =  0.015265248715877533\n",
            "Batch Training Loss =  0.032054103910923004\n",
            "Batch Training Loss =  0.02090851590037346\n",
            "Batch Training Loss =  0.009168918244540691\n",
            "Batch Training Loss =  0.034024715423583984\n",
            "Batch Training Loss =  0.01898411475121975\n",
            "Batch Training Loss =  0.02991442009806633\n",
            "Batch Training Loss =  0.014045918360352516\n",
            "Batch Training Loss =  0.015588363632559776\n",
            "Batch Training Loss =  0.01712977886199951\n",
            "Batch Training Loss =  0.014938204549252987\n",
            "Batch Training Loss =  0.01418539322912693\n",
            "Validation Loss in this epoch is 0.628\n",
            "This is  70 th epoch\n",
            "Batch Training Loss =  0.017814060673117638\n",
            "Batch Training Loss =  0.00892950315028429\n",
            "Batch Training Loss =  0.010875032283365726\n",
            "Batch Training Loss =  0.00849863514304161\n",
            "Batch Training Loss =  0.007775710429996252\n",
            "Batch Training Loss =  0.008564296178519726\n",
            "Batch Training Loss =  0.010082813911139965\n",
            "Batch Training Loss =  0.008852683939039707\n",
            "Batch Training Loss =  0.012783655896782875\n",
            "Batch Training Loss =  0.009397369809448719\n",
            "Batch Training Loss =  0.012467148713767529\n",
            "Batch Training Loss =  0.07013338804244995\n",
            "Batch Training Loss =  0.06155123561620712\n",
            "Batch Training Loss =  0.06799112260341644\n",
            "Batch Training Loss =  0.16279126703739166\n",
            "Batch Training Loss =  1.016846776008606\n",
            "Batch Training Loss =  3.1969423294067383\n",
            "Batch Training Loss =  1.050825595855713\n",
            "Batch Training Loss =  0.70339035987854\n",
            "Batch Training Loss =  0.34682080149650574\n",
            "Batch Training Loss =  0.3859900236129761\n",
            "Batch Training Loss =  0.36462515592575073\n",
            "Batch Training Loss =  0.2729906737804413\n",
            "Batch Training Loss =  0.22747117280960083\n",
            "Batch Training Loss =  0.20350781083106995\n",
            "Batch Training Loss =  0.21457616984844208\n",
            "Batch Training Loss =  0.19767440855503082\n",
            "Batch Training Loss =  0.12499942630529404\n",
            "Batch Training Loss =  0.10167527198791504\n",
            "Batch Training Loss =  0.19435064494609833\n",
            "Batch Training Loss =  0.15676432847976685\n",
            "Batch Training Loss =  0.1372750699520111\n",
            "Validation Loss in this epoch is 0.549\n",
            "This is  71 th epoch\n",
            "Batch Training Loss =  0.047991953790187836\n",
            "Batch Training Loss =  0.08602551370859146\n",
            "Batch Training Loss =  0.09664802253246307\n",
            "Batch Training Loss =  0.18347106873989105\n",
            "Batch Training Loss =  0.09477987885475159\n",
            "Batch Training Loss =  0.09951882064342499\n",
            "Batch Training Loss =  0.06570881605148315\n",
            "Batch Training Loss =  0.07530062645673752\n",
            "Batch Training Loss =  0.05651706084609032\n",
            "Batch Training Loss =  0.05129655450582504\n",
            "Batch Training Loss =  0.060529742389917374\n",
            "Batch Training Loss =  0.05820592865347862\n",
            "Batch Training Loss =  0.13381443917751312\n",
            "Batch Training Loss =  0.07352329790592194\n",
            "Batch Training Loss =  0.05548027157783508\n",
            "Batch Training Loss =  0.05379185825586319\n",
            "Batch Training Loss =  0.06380864977836609\n",
            "Batch Training Loss =  0.10841049253940582\n",
            "Batch Training Loss =  0.05816764384508133\n",
            "Batch Training Loss =  0.03340614587068558\n",
            "Batch Training Loss =  0.1516313999891281\n",
            "Batch Training Loss =  0.09580226987600327\n",
            "Batch Training Loss =  0.05944449082016945\n",
            "Batch Training Loss =  0.13966718316078186\n",
            "Batch Training Loss =  0.09843891859054565\n",
            "Batch Training Loss =  0.07468874007463455\n",
            "Batch Training Loss =  0.04838032275438309\n",
            "Batch Training Loss =  0.04711088165640831\n",
            "Batch Training Loss =  0.07845412194728851\n",
            "Batch Training Loss =  0.10316832363605499\n",
            "Batch Training Loss =  0.05268045514822006\n",
            "Batch Training Loss =  0.04074687510728836\n",
            "Validation Loss in this epoch is 0.587\n",
            "This is  72 th epoch\n",
            "Batch Training Loss =  0.02927417680621147\n",
            "Batch Training Loss =  0.019621677696704865\n",
            "Batch Training Loss =  0.0316644161939621\n",
            "Batch Training Loss =  0.10329971462488174\n",
            "Batch Training Loss =  0.04290272295475006\n",
            "Batch Training Loss =  0.04448941349983215\n",
            "Batch Training Loss =  0.033548880368471146\n",
            "Batch Training Loss =  0.03754843771457672\n",
            "Batch Training Loss =  0.02333706058561802\n",
            "Batch Training Loss =  0.08114920556545258\n",
            "Batch Training Loss =  0.03064492717385292\n",
            "Batch Training Loss =  0.0331932008266449\n",
            "Batch Training Loss =  0.019936678931117058\n",
            "Batch Training Loss =  0.0228103157132864\n",
            "Batch Training Loss =  0.020540907979011536\n",
            "Batch Training Loss =  0.02290986105799675\n",
            "Batch Training Loss =  0.018131854012608528\n",
            "Batch Training Loss =  0.025388289242982864\n",
            "Batch Training Loss =  0.021078059449791908\n",
            "Batch Training Loss =  0.021593479439616203\n",
            "Batch Training Loss =  0.01742689311504364\n",
            "Batch Training Loss =  0.08415566384792328\n",
            "Batch Training Loss =  0.0210089311003685\n",
            "Batch Training Loss =  0.02560172975063324\n",
            "Batch Training Loss =  0.02653907798230648\n",
            "Batch Training Loss =  0.05444313585758209\n",
            "Batch Training Loss =  0.07945989817380905\n",
            "Batch Training Loss =  0.02558835595846176\n",
            "Batch Training Loss =  0.030079329386353493\n",
            "Batch Training Loss =  0.03606021776795387\n",
            "Batch Training Loss =  0.029917573556303978\n",
            "Batch Training Loss =  0.0321793407201767\n",
            "Validation Loss in this epoch is 0.646\n",
            "This is  73 th epoch\n",
            "Batch Training Loss =  0.027990663424134254\n",
            "Batch Training Loss =  0.04933813586831093\n",
            "Batch Training Loss =  0.02695956639945507\n",
            "Batch Training Loss =  0.01774291880428791\n",
            "Batch Training Loss =  0.016552869230508804\n",
            "Batch Training Loss =  0.01477812509983778\n",
            "Batch Training Loss =  0.020940124988555908\n",
            "Batch Training Loss =  0.01906573586165905\n",
            "Batch Training Loss =  0.019103555008769035\n",
            "Batch Training Loss =  0.022316526621580124\n",
            "Batch Training Loss =  0.014144941233098507\n",
            "Batch Training Loss =  0.025944096967577934\n",
            "Batch Training Loss =  0.01492953859269619\n",
            "Batch Training Loss =  0.012439876794815063\n",
            "Batch Training Loss =  0.009587429463863373\n",
            "Batch Training Loss =  0.01309775747358799\n",
            "Batch Training Loss =  0.014436161145567894\n",
            "Batch Training Loss =  0.025589222088456154\n",
            "Batch Training Loss =  0.03034944459795952\n",
            "Batch Training Loss =  0.010198626667261124\n",
            "Batch Training Loss =  0.013576149940490723\n",
            "Batch Training Loss =  0.06962630152702332\n",
            "Batch Training Loss =  0.0447070486843586\n",
            "Batch Training Loss =  0.024788670241832733\n",
            "Batch Training Loss =  0.027784043923020363\n",
            "Batch Training Loss =  0.03545917570590973\n",
            "Batch Training Loss =  0.016197286546230316\n",
            "Batch Training Loss =  0.01993241161108017\n",
            "Batch Training Loss =  0.021479010581970215\n",
            "Batch Training Loss =  0.02798590250313282\n",
            "Batch Training Loss =  0.0172947458922863\n",
            "Batch Training Loss =  0.013328195549547672\n",
            "Validation Loss in this epoch is 0.650\n",
            "This is  74 th epoch\n",
            "Batch Training Loss =  0.010105293244123459\n",
            "Batch Training Loss =  0.018010469153523445\n",
            "Batch Training Loss =  0.06348680704832077\n",
            "Batch Training Loss =  0.035417258739471436\n",
            "Batch Training Loss =  0.03699515014886856\n",
            "Batch Training Loss =  0.018371963873505592\n",
            "Batch Training Loss =  0.02346879243850708\n",
            "Batch Training Loss =  0.020789433270692825\n",
            "Batch Training Loss =  0.011032206937670708\n",
            "Batch Training Loss =  0.016414761543273926\n",
            "Batch Training Loss =  0.014274337328970432\n",
            "Batch Training Loss =  0.013016542419791222\n",
            "Batch Training Loss =  0.013233515433967113\n",
            "Batch Training Loss =  0.01301051490008831\n",
            "Batch Training Loss =  0.0372854620218277\n",
            "Batch Training Loss =  0.018995538353919983\n",
            "Batch Training Loss =  0.013512169942259789\n",
            "Batch Training Loss =  0.020512554794549942\n",
            "Batch Training Loss =  0.015112820081412792\n",
            "Batch Training Loss =  0.023547247052192688\n",
            "Batch Training Loss =  0.01949285715818405\n",
            "Batch Training Loss =  0.05158160254359245\n",
            "Batch Training Loss =  0.035173285752534866\n",
            "Batch Training Loss =  0.011791391298174858\n",
            "Batch Training Loss =  0.020519377663731575\n",
            "Batch Training Loss =  0.015572688542306423\n",
            "Batch Training Loss =  0.013715519569814205\n",
            "Batch Training Loss =  0.016788041219115257\n",
            "Batch Training Loss =  0.01911788433790207\n",
            "Batch Training Loss =  0.013432634994387627\n",
            "Batch Training Loss =  0.021369565278291702\n",
            "Batch Training Loss =  0.012565148063004017\n",
            "Validation Loss in this epoch is 0.642\n",
            "This is  75 th epoch\n",
            "Batch Training Loss =  0.013430425897240639\n",
            "Batch Training Loss =  0.013833732344210148\n",
            "Batch Training Loss =  0.0080470135435462\n",
            "Batch Training Loss =  0.009932026267051697\n",
            "Batch Training Loss =  0.011199913918972015\n",
            "Batch Training Loss =  0.047192059457302094\n",
            "Batch Training Loss =  0.04370834678411484\n",
            "Batch Training Loss =  0.017194023355841637\n",
            "Batch Training Loss =  0.015085211955010891\n",
            "Batch Training Loss =  0.014029699377715588\n",
            "Batch Training Loss =  0.013128824532032013\n",
            "Batch Training Loss =  0.012567452155053616\n",
            "Batch Training Loss =  0.012359380722045898\n",
            "Batch Training Loss =  0.012898807413876057\n",
            "Batch Training Loss =  0.010848556645214558\n",
            "Batch Training Loss =  0.012877379544079304\n",
            "Batch Training Loss =  0.012701579369604588\n",
            "Batch Training Loss =  0.011994357220828533\n",
            "Batch Training Loss =  0.012180439196527004\n",
            "Batch Training Loss =  0.0205402709543705\n",
            "Batch Training Loss =  0.012666339054703712\n",
            "Batch Training Loss =  0.006541108712553978\n",
            "Batch Training Loss =  0.013489138334989548\n",
            "Batch Training Loss =  0.06572190672159195\n",
            "Batch Training Loss =  0.01857968606054783\n",
            "Batch Training Loss =  0.014289507642388344\n",
            "Batch Training Loss =  0.03280467912554741\n",
            "Batch Training Loss =  0.023399407044053078\n",
            "Batch Training Loss =  0.03935207054018974\n",
            "Batch Training Loss =  0.030964214354753494\n",
            "Batch Training Loss =  0.028408842161297798\n",
            "Batch Training Loss =  0.02369135618209839\n",
            "Validation Loss in this epoch is 0.646\n",
            "This is  76 th epoch\n",
            "Batch Training Loss =  0.02706901542842388\n",
            "Batch Training Loss =  0.014420616440474987\n",
            "Batch Training Loss =  0.01366438902914524\n",
            "Batch Training Loss =  0.01151342410594225\n",
            "Batch Training Loss =  0.009206810966134071\n",
            "Batch Training Loss =  0.010692418552935123\n",
            "Batch Training Loss =  0.01210237666964531\n",
            "Batch Training Loss =  0.026523346081376076\n",
            "Batch Training Loss =  0.01672525703907013\n",
            "Batch Training Loss =  0.008857215754687786\n",
            "Batch Training Loss =  0.012205226346850395\n",
            "Batch Training Loss =  0.011507488787174225\n",
            "Batch Training Loss =  0.021543875336647034\n",
            "Batch Training Loss =  0.04405476525425911\n",
            "Batch Training Loss =  0.0233440138399601\n",
            "Batch Training Loss =  0.022076137363910675\n",
            "Batch Training Loss =  0.011327408254146576\n",
            "Batch Training Loss =  0.012753773480653763\n",
            "Batch Training Loss =  0.017416495829820633\n",
            "Batch Training Loss =  0.012284043245017529\n",
            "Batch Training Loss =  0.014762336388230324\n",
            "Batch Training Loss =  0.010195254348218441\n",
            "Batch Training Loss =  0.012367818504571915\n",
            "Batch Training Loss =  0.010241886600852013\n",
            "Batch Training Loss =  0.010209602303802967\n",
            "Batch Training Loss =  0.013678792864084244\n",
            "Batch Training Loss =  0.012636704370379448\n",
            "Batch Training Loss =  0.011984172277152538\n",
            "Batch Training Loss =  0.015619408339262009\n",
            "Batch Training Loss =  0.016309665516018867\n",
            "Batch Training Loss =  0.05619613826274872\n",
            "Batch Training Loss =  0.025472581386566162\n",
            "Validation Loss in this epoch is 0.648\n",
            "This is  77 th epoch\n",
            "Batch Training Loss =  0.019675755873322487\n",
            "Batch Training Loss =  0.042931221425533295\n",
            "Batch Training Loss =  0.011391209438443184\n",
            "Batch Training Loss =  0.013320885598659515\n",
            "Batch Training Loss =  0.012457775883376598\n",
            "Batch Training Loss =  0.014505278319120407\n",
            "Batch Training Loss =  0.011824531480669975\n",
            "Batch Training Loss =  0.007348718587309122\n",
            "Batch Training Loss =  0.010540962219238281\n",
            "Batch Training Loss =  0.04532318189740181\n",
            "Batch Training Loss =  0.032740749418735504\n",
            "Batch Training Loss =  0.02247549593448639\n",
            "Batch Training Loss =  0.011664644815027714\n",
            "Batch Training Loss =  0.010548545978963375\n",
            "Batch Training Loss =  0.009962075389921665\n",
            "Batch Training Loss =  0.00785853061825037\n",
            "Batch Training Loss =  0.012411253526806831\n",
            "Batch Training Loss =  0.009793136268854141\n",
            "Batch Training Loss =  0.0071839685551822186\n",
            "Batch Training Loss =  0.011789124459028244\n",
            "Batch Training Loss =  0.01513533852994442\n",
            "Batch Training Loss =  0.013168267905712128\n",
            "Batch Training Loss =  0.010367273353040218\n",
            "Batch Training Loss =  0.024416472762823105\n",
            "Batch Training Loss =  0.016587181016802788\n",
            "Batch Training Loss =  0.00984079297631979\n",
            "Batch Training Loss =  0.016936810687184334\n",
            "Batch Training Loss =  0.01268797367811203\n",
            "Batch Training Loss =  0.017886459827423096\n",
            "Batch Training Loss =  0.009749669581651688\n",
            "Batch Training Loss =  0.009943250566720963\n",
            "Batch Training Loss =  0.017250338569283485\n",
            "Validation Loss in this epoch is 0.660\n",
            "This is  78 th epoch\n",
            "Batch Training Loss =  0.00769572239369154\n",
            "Batch Training Loss =  0.006805936805903912\n",
            "Batch Training Loss =  0.016584070399403572\n",
            "Batch Training Loss =  0.012438955716788769\n",
            "Batch Training Loss =  0.008293509483337402\n",
            "Batch Training Loss =  0.0090683214366436\n",
            "Batch Training Loss =  0.00921466201543808\n",
            "Batch Training Loss =  0.03701896220445633\n",
            "Batch Training Loss =  0.011568733491003513\n",
            "Batch Training Loss =  0.02111644297838211\n",
            "Batch Training Loss =  0.02445894293487072\n",
            "Batch Training Loss =  0.01734953187406063\n",
            "Batch Training Loss =  0.01272707898169756\n",
            "Batch Training Loss =  0.020325856283307076\n",
            "Batch Training Loss =  0.010906921699643135\n",
            "Batch Training Loss =  0.014115562662482262\n",
            "Batch Training Loss =  0.018559671938419342\n",
            "Batch Training Loss =  0.010026281699538231\n",
            "Batch Training Loss =  0.021341698244214058\n",
            "Batch Training Loss =  0.010791114531457424\n",
            "Batch Training Loss =  0.012181405909359455\n",
            "Batch Training Loss =  0.016793912276625633\n",
            "Batch Training Loss =  0.010969692841172218\n",
            "Batch Training Loss =  0.016166910529136658\n",
            "Batch Training Loss =  0.01584814116358757\n",
            "Batch Training Loss =  0.012323442846536636\n",
            "Batch Training Loss =  0.01206354983150959\n",
            "Batch Training Loss =  0.01433037593960762\n",
            "Batch Training Loss =  0.013401624746620655\n",
            "Batch Training Loss =  0.013469926081597805\n",
            "Batch Training Loss =  0.01998801715672016\n",
            "Batch Training Loss =  0.01914786547422409\n",
            "Validation Loss in this epoch is 0.656\n",
            "This is  79 th epoch\n",
            "Batch Training Loss =  0.011774101294577122\n",
            "Batch Training Loss =  0.02383957803249359\n",
            "Batch Training Loss =  0.008586280979216099\n",
            "Batch Training Loss =  0.020317738875746727\n",
            "Batch Training Loss =  0.014454325661063194\n",
            "Batch Training Loss =  0.027156932279467583\n",
            "Batch Training Loss =  0.017069831490516663\n",
            "Batch Training Loss =  0.005445416551083326\n",
            "Batch Training Loss =  0.008367430418729782\n",
            "Batch Training Loss =  0.009842036291956902\n",
            "Batch Training Loss =  0.03136148303747177\n",
            "Batch Training Loss =  0.014848095364868641\n",
            "Batch Training Loss =  0.007732181344181299\n",
            "Batch Training Loss =  0.017306849360466003\n",
            "Batch Training Loss =  0.009071039035916328\n",
            "Batch Training Loss =  0.010102332569658756\n",
            "Batch Training Loss =  0.008509826846420765\n",
            "Batch Training Loss =  0.019256891682744026\n",
            "Batch Training Loss =  0.011060061864554882\n",
            "Batch Training Loss =  0.018864480778574944\n",
            "Batch Training Loss =  0.018124466761946678\n",
            "Batch Training Loss =  0.01338139921426773\n",
            "Batch Training Loss =  0.009807570837438107\n",
            "Batch Training Loss =  0.009468096308410168\n",
            "Batch Training Loss =  0.012125158682465553\n",
            "Batch Training Loss =  0.009670833125710487\n",
            "Batch Training Loss =  0.010997768491506577\n",
            "Batch Training Loss =  0.021825332194566727\n",
            "Batch Training Loss =  0.013441826216876507\n",
            "Batch Training Loss =  0.011253937147557735\n",
            "Batch Training Loss =  0.010907319374382496\n",
            "Batch Training Loss =  0.014046412892639637\n",
            "Validation Loss in this epoch is 0.630\n",
            "This is  80 th epoch\n",
            "Batch Training Loss =  0.011314152739942074\n",
            "Batch Training Loss =  0.011006038635969162\n",
            "Batch Training Loss =  0.014705751091241837\n",
            "Batch Training Loss =  0.011152557097375393\n",
            "Batch Training Loss =  0.022301485762000084\n",
            "Batch Training Loss =  0.02883041836321354\n",
            "Batch Training Loss =  0.024374160915613174\n",
            "Batch Training Loss =  0.01380290649831295\n",
            "Batch Training Loss =  0.011687546037137508\n",
            "Batch Training Loss =  0.010793951340019703\n",
            "Batch Training Loss =  0.010153119452297688\n",
            "Batch Training Loss =  0.011500396765768528\n",
            "Batch Training Loss =  0.011954785324633121\n",
            "Batch Training Loss =  0.01278009545058012\n",
            "Batch Training Loss =  0.009528334252536297\n",
            "Batch Training Loss =  0.01279003731906414\n",
            "Batch Training Loss =  0.008721054531633854\n",
            "Batch Training Loss =  0.010850236751139164\n",
            "Batch Training Loss =  0.010440826416015625\n",
            "Batch Training Loss =  0.011023030616343021\n",
            "Batch Training Loss =  0.014794708229601383\n",
            "Batch Training Loss =  0.008420253172516823\n",
            "Batch Training Loss =  0.012346566654741764\n",
            "Batch Training Loss =  0.012408494018018246\n",
            "Batch Training Loss =  0.004627815447747707\n",
            "Batch Training Loss =  0.012465395033359528\n",
            "Batch Training Loss =  0.01776609569787979\n",
            "Batch Training Loss =  0.009812678210437298\n",
            "Batch Training Loss =  0.01102178543806076\n",
            "Batch Training Loss =  0.07603026926517487\n",
            "Batch Training Loss =  0.16649551689624786\n",
            "Batch Training Loss =  0.6515868306159973\n",
            "Validation Loss in this epoch is 2.167\n",
            "This is  81 th epoch\n",
            "Batch Training Loss =  1.7735488414764404\n",
            "Batch Training Loss =  0.2085597813129425\n",
            "Batch Training Loss =  0.15350714325904846\n",
            "Batch Training Loss =  0.15277191996574402\n",
            "Batch Training Loss =  0.1265840381383896\n",
            "Batch Training Loss =  0.19734174013137817\n",
            "Batch Training Loss =  0.10247130692005157\n",
            "Batch Training Loss =  0.08328016102313995\n",
            "Batch Training Loss =  0.1201828196644783\n",
            "Batch Training Loss =  0.17456869781017303\n",
            "Batch Training Loss =  0.074209064245224\n",
            "Batch Training Loss =  0.0683818906545639\n",
            "Batch Training Loss =  0.06562800705432892\n",
            "Batch Training Loss =  0.14789262413978577\n",
            "Batch Training Loss =  0.08363724499940872\n",
            "Batch Training Loss =  0.07683862745761871\n",
            "Batch Training Loss =  0.08643815666437149\n",
            "Batch Training Loss =  0.07358413934707642\n",
            "Batch Training Loss =  0.06122605502605438\n",
            "Batch Training Loss =  0.09740012139081955\n",
            "Batch Training Loss =  0.09682533890008926\n",
            "Batch Training Loss =  0.10569453984498978\n",
            "Batch Training Loss =  0.09253256767988205\n",
            "Batch Training Loss =  0.052333980798721313\n",
            "Batch Training Loss =  0.0478360615670681\n",
            "Batch Training Loss =  0.05318011716008186\n",
            "Batch Training Loss =  0.05223130062222481\n",
            "Batch Training Loss =  0.03849964961409569\n",
            "Batch Training Loss =  0.06629617512226105\n",
            "Batch Training Loss =  0.020225798711180687\n",
            "Batch Training Loss =  0.08233614265918732\n",
            "Batch Training Loss =  0.0674048364162445\n",
            "Validation Loss in this epoch is 0.666\n",
            "This is  82 th epoch\n",
            "Batch Training Loss =  0.07117804139852524\n",
            "Batch Training Loss =  0.04471879079937935\n",
            "Batch Training Loss =  0.11262327432632446\n",
            "Batch Training Loss =  0.04215683043003082\n",
            "Batch Training Loss =  0.03293818235397339\n",
            "Batch Training Loss =  0.06511836498975754\n",
            "Batch Training Loss =  0.07477575540542603\n",
            "Batch Training Loss =  0.04207133874297142\n",
            "Batch Training Loss =  0.056715041399002075\n",
            "Batch Training Loss =  0.029225684702396393\n",
            "Batch Training Loss =  0.0226876363158226\n",
            "Batch Training Loss =  0.023044703528285027\n",
            "Batch Training Loss =  0.020017851144075394\n",
            "Batch Training Loss =  0.02027856558561325\n",
            "Batch Training Loss =  0.03504035249352455\n",
            "Batch Training Loss =  0.07643409073352814\n",
            "Batch Training Loss =  0.03482716903090477\n",
            "Batch Training Loss =  0.027033811435103416\n",
            "Batch Training Loss =  0.03567364439368248\n",
            "Batch Training Loss =  0.055216290056705475\n",
            "Batch Training Loss =  0.0271268580108881\n",
            "Batch Training Loss =  0.023598963394761086\n",
            "Batch Training Loss =  0.015208239667117596\n",
            "Batch Training Loss =  0.031763702630996704\n",
            "Batch Training Loss =  0.02304636314511299\n",
            "Batch Training Loss =  0.013802898116409779\n",
            "Batch Training Loss =  0.013586376793682575\n",
            "Batch Training Loss =  0.011993944644927979\n",
            "Batch Training Loss =  0.023566942662000656\n",
            "Batch Training Loss =  0.016845859587192535\n",
            "Batch Training Loss =  0.09330368041992188\n",
            "Batch Training Loss =  0.05551207438111305\n",
            "Validation Loss in this epoch is 0.668\n",
            "This is  83 th epoch\n",
            "Batch Training Loss =  0.017045781016349792\n",
            "Batch Training Loss =  0.015920808538794518\n",
            "Batch Training Loss =  0.016842547804117203\n",
            "Batch Training Loss =  0.01845499500632286\n",
            "Batch Training Loss =  0.015488449484109879\n",
            "Batch Training Loss =  0.046493422240018845\n",
            "Batch Training Loss =  0.03821847587823868\n",
            "Batch Training Loss =  0.015223681926727295\n",
            "Batch Training Loss =  0.016877751797437668\n",
            "Batch Training Loss =  0.017014585435390472\n",
            "Batch Training Loss =  0.018655532971024513\n",
            "Batch Training Loss =  0.011986064724624157\n",
            "Batch Training Loss =  0.015208977274596691\n",
            "Batch Training Loss =  0.0169431883841753\n",
            "Batch Training Loss =  0.01395259890705347\n",
            "Batch Training Loss =  0.010999675840139389\n",
            "Batch Training Loss =  0.01777574233710766\n",
            "Batch Training Loss =  0.013672595843672752\n",
            "Batch Training Loss =  0.012859474867582321\n",
            "Batch Training Loss =  0.025186842307448387\n",
            "Batch Training Loss =  0.011556998826563358\n",
            "Batch Training Loss =  0.011573754251003265\n",
            "Batch Training Loss =  0.009039252065122128\n",
            "Batch Training Loss =  0.013092433102428913\n",
            "Batch Training Loss =  0.008408562280237675\n",
            "Batch Training Loss =  0.011516833677887917\n",
            "Batch Training Loss =  0.01318400353193283\n",
            "Batch Training Loss =  0.011739451438188553\n",
            "Batch Training Loss =  0.014802043326199055\n",
            "Batch Training Loss =  0.013511726632714272\n",
            "Batch Training Loss =  0.06986339390277863\n",
            "Batch Training Loss =  0.031492721289396286\n",
            "Validation Loss in this epoch is 0.645\n",
            "This is  84 th epoch\n",
            "Batch Training Loss =  0.02075279876589775\n",
            "Batch Training Loss =  0.015651745721697807\n",
            "Batch Training Loss =  0.010873823426663876\n",
            "Batch Training Loss =  0.01069139875471592\n",
            "Batch Training Loss =  0.012583154253661633\n",
            "Batch Training Loss =  0.014798318035900593\n",
            "Batch Training Loss =  0.011140269227325916\n",
            "Batch Training Loss =  0.011913989670574665\n",
            "Batch Training Loss =  0.013711423613131046\n",
            "Batch Training Loss =  0.008588521741330624\n",
            "Batch Training Loss =  0.01174643263220787\n",
            "Batch Training Loss =  0.011504807509481907\n",
            "Batch Training Loss =  0.010849753394722939\n",
            "Batch Training Loss =  0.009606004692614079\n",
            "Batch Training Loss =  0.012223644182085991\n",
            "Batch Training Loss =  0.008120612241327763\n",
            "Batch Training Loss =  0.05787963792681694\n",
            "Batch Training Loss =  0.023688901215791702\n",
            "Batch Training Loss =  0.009317454881966114\n",
            "Batch Training Loss =  0.015103188343346119\n",
            "Batch Training Loss =  0.011626562103629112\n",
            "Batch Training Loss =  0.010795585811138153\n",
            "Batch Training Loss =  0.03371935337781906\n",
            "Batch Training Loss =  0.027570296078920364\n",
            "Batch Training Loss =  0.012140760198235512\n",
            "Batch Training Loss =  0.006267460994422436\n",
            "Batch Training Loss =  0.008910637348890305\n",
            "Batch Training Loss =  0.014351168647408485\n",
            "Batch Training Loss =  0.027489395812153816\n",
            "Batch Training Loss =  0.016317442059516907\n",
            "Batch Training Loss =  0.027078844606876373\n",
            "Batch Training Loss =  0.010914924554526806\n",
            "Validation Loss in this epoch is 0.641\n",
            "This is  85 th epoch\n",
            "Batch Training Loss =  0.009947653859853745\n",
            "Batch Training Loss =  0.009852403774857521\n",
            "Batch Training Loss =  0.00880243256688118\n",
            "Batch Training Loss =  0.01217237114906311\n",
            "Batch Training Loss =  0.0063050007447600365\n",
            "Batch Training Loss =  0.012505189515650272\n",
            "Batch Training Loss =  0.009671034291386604\n",
            "Batch Training Loss =  0.008770913816988468\n",
            "Batch Training Loss =  0.010318037122488022\n",
            "Batch Training Loss =  0.010004261508584023\n",
            "Batch Training Loss =  0.009248429909348488\n",
            "Batch Training Loss =  0.006532005500048399\n",
            "Batch Training Loss =  0.00864396896213293\n",
            "Batch Training Loss =  0.011762402951717377\n",
            "Batch Training Loss =  0.03377508744597435\n",
            "Batch Training Loss =  0.011350328102707863\n",
            "Batch Training Loss =  0.01147068664431572\n",
            "Batch Training Loss =  0.023996856063604355\n",
            "Batch Training Loss =  0.012810668908059597\n",
            "Batch Training Loss =  0.016570765525102615\n",
            "Batch Training Loss =  0.008725971914827824\n",
            "Batch Training Loss =  0.010006849654018879\n",
            "Batch Training Loss =  0.01660389080643654\n",
            "Batch Training Loss =  0.0517096184194088\n",
            "Batch Training Loss =  0.03742613270878792\n",
            "Batch Training Loss =  0.011347640305757523\n",
            "Batch Training Loss =  0.01862696185708046\n",
            "Batch Training Loss =  0.011950607411563396\n",
            "Batch Training Loss =  0.013591532595455647\n",
            "Batch Training Loss =  0.026054294779896736\n",
            "Batch Training Loss =  0.011923989281058311\n",
            "Batch Training Loss =  0.011929496191442013\n",
            "Validation Loss in this epoch is 0.643\n",
            "This is  86 th epoch\n",
            "Batch Training Loss =  0.009987141005694866\n",
            "Batch Training Loss =  0.013545794412493706\n",
            "Batch Training Loss =  0.007860402576625347\n",
            "Batch Training Loss =  0.00863521546125412\n",
            "Batch Training Loss =  0.009681038558483124\n",
            "Batch Training Loss =  0.009342308156192303\n",
            "Batch Training Loss =  0.011264261789619923\n",
            "Batch Training Loss =  0.010462526232004166\n",
            "Batch Training Loss =  0.008090658113360405\n",
            "Batch Training Loss =  0.007115408778190613\n",
            "Batch Training Loss =  0.010229888372123241\n",
            "Batch Training Loss =  0.009223162196576595\n",
            "Batch Training Loss =  0.011873739771544933\n",
            "Batch Training Loss =  0.010496850125491619\n",
            "Batch Training Loss =  0.011570964939892292\n",
            "Batch Training Loss =  0.013513561338186264\n",
            "Batch Training Loss =  0.009923207573592663\n",
            "Batch Training Loss =  0.013471257872879505\n",
            "Batch Training Loss =  0.013829339295625687\n",
            "Batch Training Loss =  0.015133858658373356\n",
            "Batch Training Loss =  0.010034047067165375\n",
            "Batch Training Loss =  0.010628321208059788\n",
            "Batch Training Loss =  0.009466972202062607\n",
            "Batch Training Loss =  0.0563945472240448\n",
            "Batch Training Loss =  0.017325151711702347\n",
            "Batch Training Loss =  0.014354065991938114\n",
            "Batch Training Loss =  0.03452669456601143\n",
            "Batch Training Loss =  0.03266553580760956\n",
            "Batch Training Loss =  0.015499609522521496\n",
            "Batch Training Loss =  0.01852279156446457\n",
            "Batch Training Loss =  0.01223085168749094\n",
            "Batch Training Loss =  0.007824641652405262\n",
            "Validation Loss in this epoch is 0.638\n",
            "This is  87 th epoch\n",
            "Batch Training Loss =  0.01197177916765213\n",
            "Batch Training Loss =  0.008914931677281857\n",
            "Batch Training Loss =  0.017533352598547935\n",
            "Batch Training Loss =  0.008013149723410606\n",
            "Batch Training Loss =  0.01177989598363638\n",
            "Batch Training Loss =  0.014203892089426517\n",
            "Batch Training Loss =  0.015130799263715744\n",
            "Batch Training Loss =  0.00858311913907528\n",
            "Batch Training Loss =  0.009827233850955963\n",
            "Batch Training Loss =  0.010035288520157337\n",
            "Batch Training Loss =  0.008042102679610252\n",
            "Batch Training Loss =  0.011403956450521946\n",
            "Batch Training Loss =  0.050742410123348236\n",
            "Batch Training Loss =  0.013945287093520164\n",
            "Batch Training Loss =  0.009803522378206253\n",
            "Batch Training Loss =  0.013124746270477772\n",
            "Batch Training Loss =  0.0087626026943326\n",
            "Batch Training Loss =  0.012301057577133179\n",
            "Batch Training Loss =  0.01711408421397209\n",
            "Batch Training Loss =  0.013724617660045624\n",
            "Batch Training Loss =  0.009120498783886433\n",
            "Batch Training Loss =  0.013035940937697887\n",
            "Batch Training Loss =  0.013087201863527298\n",
            "Batch Training Loss =  0.009492453187704086\n",
            "Batch Training Loss =  0.012284815311431885\n",
            "Batch Training Loss =  0.016368502750992775\n",
            "Batch Training Loss =  0.012870490550994873\n",
            "Batch Training Loss =  0.011192991398274899\n",
            "Batch Training Loss =  0.03185424208641052\n",
            "Batch Training Loss =  0.027155334129929543\n",
            "Batch Training Loss =  0.02187284268438816\n",
            "Batch Training Loss =  0.014806478284299374\n",
            "Validation Loss in this epoch is 0.613\n",
            "This is  88 th epoch\n",
            "Batch Training Loss =  0.01203357893973589\n",
            "Batch Training Loss =  0.009027397260069847\n",
            "Batch Training Loss =  0.010651801712810993\n",
            "Batch Training Loss =  0.01950124278664589\n",
            "Batch Training Loss =  0.012035020627081394\n",
            "Batch Training Loss =  0.010097142308950424\n",
            "Batch Training Loss =  0.009747388772666454\n",
            "Batch Training Loss =  0.008326265960931778\n",
            "Batch Training Loss =  0.00830984115600586\n",
            "Batch Training Loss =  0.010458226315677166\n",
            "Batch Training Loss =  0.010145937092602253\n",
            "Batch Training Loss =  0.008067123591899872\n",
            "Batch Training Loss =  0.01599165052175522\n",
            "Batch Training Loss =  0.011251803487539291\n",
            "Batch Training Loss =  0.013375396840274334\n",
            "Batch Training Loss =  0.07999903708696365\n",
            "Batch Training Loss =  0.10955667495727539\n",
            "Batch Training Loss =  0.11542007327079773\n",
            "Batch Training Loss =  0.08504921942949295\n",
            "Batch Training Loss =  0.13622735440731049\n",
            "Batch Training Loss =  0.07207738608121872\n",
            "Batch Training Loss =  0.020685682073235512\n",
            "Batch Training Loss =  0.029355483129620552\n",
            "Batch Training Loss =  0.06949261575937271\n",
            "Batch Training Loss =  0.028047623112797737\n",
            "Batch Training Loss =  0.022424226626753807\n",
            "Batch Training Loss =  0.03323016315698624\n",
            "Batch Training Loss =  0.0349256694316864\n",
            "Batch Training Loss =  0.019697846844792366\n",
            "Batch Training Loss =  0.015203909017145634\n",
            "Batch Training Loss =  0.02356511540710926\n",
            "Batch Training Loss =  0.03348769620060921\n",
            "Validation Loss in this epoch is 0.609\n",
            "This is  89 th epoch\n",
            "Batch Training Loss =  0.021737394854426384\n",
            "Batch Training Loss =  0.011246094480156898\n",
            "Batch Training Loss =  0.015509350225329399\n",
            "Batch Training Loss =  0.011478088796138763\n",
            "Batch Training Loss =  0.01607826165854931\n",
            "Batch Training Loss =  0.009545103646814823\n",
            "Batch Training Loss =  0.01861165463924408\n",
            "Batch Training Loss =  0.013594070449471474\n",
            "Batch Training Loss =  0.04175066575407982\n",
            "Batch Training Loss =  0.0215404462069273\n",
            "Batch Training Loss =  0.010872657410800457\n",
            "Batch Training Loss =  0.03178860992193222\n",
            "Batch Training Loss =  0.011643456295132637\n",
            "Batch Training Loss =  0.013332437723875046\n",
            "Batch Training Loss =  0.012964975088834763\n",
            "Batch Training Loss =  0.01075256709009409\n",
            "Batch Training Loss =  0.011410899460315704\n",
            "Batch Training Loss =  0.009461450390517712\n",
            "Batch Training Loss =  0.011317623779177666\n",
            "Batch Training Loss =  0.015099802985787392\n",
            "Batch Training Loss =  0.014946289360523224\n",
            "Batch Training Loss =  0.011770356446504593\n",
            "Batch Training Loss =  0.010510154999792576\n",
            "Batch Training Loss =  0.01633545570075512\n",
            "Batch Training Loss =  0.01496821641921997\n",
            "Batch Training Loss =  0.016964292153716087\n",
            "Batch Training Loss =  0.015792841091752052\n",
            "Batch Training Loss =  0.05776702240109444\n",
            "Batch Training Loss =  0.01729637198150158\n",
            "Batch Training Loss =  0.020809834823012352\n",
            "Batch Training Loss =  0.014288774691522121\n",
            "Batch Training Loss =  0.03316710516810417\n",
            "Validation Loss in this epoch is 0.625\n",
            "This is  90 th epoch\n",
            "Batch Training Loss =  0.014023185707628727\n",
            "Batch Training Loss =  0.010310783050954342\n",
            "Batch Training Loss =  0.011706914752721786\n",
            "Batch Training Loss =  0.015318326652050018\n",
            "Batch Training Loss =  0.01048637181520462\n",
            "Batch Training Loss =  0.0226941779255867\n",
            "Batch Training Loss =  0.01685631461441517\n",
            "Batch Training Loss =  0.009333856403827667\n",
            "Batch Training Loss =  0.011928954161703587\n",
            "Batch Training Loss =  0.00903410091996193\n",
            "Batch Training Loss =  0.014288175851106644\n",
            "Batch Training Loss =  0.010103820823132992\n",
            "Batch Training Loss =  0.009850374422967434\n",
            "Batch Training Loss =  0.015973001718521118\n",
            "Batch Training Loss =  0.01029868796467781\n",
            "Batch Training Loss =  0.00753538915887475\n",
            "Batch Training Loss =  0.033359095454216\n",
            "Batch Training Loss =  0.015194800682365894\n",
            "Batch Training Loss =  0.027353953570127487\n",
            "Batch Training Loss =  0.014337565749883652\n",
            "Batch Training Loss =  0.012007280252873898\n",
            "Batch Training Loss =  0.01379721611738205\n",
            "Batch Training Loss =  0.00962241180241108\n",
            "Batch Training Loss =  0.011220341548323631\n",
            "Batch Training Loss =  0.03609069064259529\n",
            "Batch Training Loss =  0.02166477032005787\n",
            "Batch Training Loss =  0.020583560690283775\n",
            "Batch Training Loss =  0.013574087992310524\n",
            "Batch Training Loss =  0.01371772401034832\n",
            "Batch Training Loss =  0.00989205576479435\n",
            "Batch Training Loss =  0.007340535521507263\n",
            "Batch Training Loss =  0.009228200651705265\n",
            "Validation Loss in this epoch is 0.621\n",
            "This is  91 th epoch\n",
            "Batch Training Loss =  0.010435197502374649\n",
            "Batch Training Loss =  0.008966699242591858\n",
            "Batch Training Loss =  0.01166065689176321\n",
            "Batch Training Loss =  0.010092927142977715\n",
            "Batch Training Loss =  0.008372316136956215\n",
            "Batch Training Loss =  0.007361007388681173\n",
            "Batch Training Loss =  0.00828853901475668\n",
            "Batch Training Loss =  0.011831173673272133\n",
            "Batch Training Loss =  0.007904885336756706\n",
            "Batch Training Loss =  0.00793513748794794\n",
            "Batch Training Loss =  0.016940034925937653\n",
            "Batch Training Loss =  0.04547860473394394\n",
            "Batch Training Loss =  0.03290557116270065\n",
            "Batch Training Loss =  0.018725836649537086\n",
            "Batch Training Loss =  0.02167772687971592\n",
            "Batch Training Loss =  0.008910167030990124\n",
            "Batch Training Loss =  0.03806439787149429\n",
            "Batch Training Loss =  0.009363450109958649\n",
            "Batch Training Loss =  0.009433839470148087\n",
            "Batch Training Loss =  0.011695362627506256\n",
            "Batch Training Loss =  0.026728976517915726\n",
            "Batch Training Loss =  0.018028421327471733\n",
            "Batch Training Loss =  0.010119659826159477\n",
            "Batch Training Loss =  0.01479034498333931\n",
            "Batch Training Loss =  0.012237015180289745\n",
            "Batch Training Loss =  0.013917723670601845\n",
            "Batch Training Loss =  0.011232900433242321\n",
            "Batch Training Loss =  0.012344435788691044\n",
            "Batch Training Loss =  0.012987066060304642\n",
            "Batch Training Loss =  0.010344223119318485\n",
            "Batch Training Loss =  0.007516617886722088\n",
            "Batch Training Loss =  0.009558035992085934\n",
            "Validation Loss in this epoch is 0.628\n",
            "This is  92 th epoch\n",
            "Batch Training Loss =  0.01115623489022255\n",
            "Batch Training Loss =  0.010788610205054283\n",
            "Batch Training Loss =  0.011302590370178223\n",
            "Batch Training Loss =  0.009479503147304058\n",
            "Batch Training Loss =  0.008484899997711182\n",
            "Batch Training Loss =  0.01218490395694971\n",
            "Batch Training Loss =  0.008865397423505783\n",
            "Batch Training Loss =  0.01091692317277193\n",
            "Batch Training Loss =  0.009367592632770538\n",
            "Batch Training Loss =  0.011982938274741173\n",
            "Batch Training Loss =  0.009393354877829552\n",
            "Batch Training Loss =  0.009734218940138817\n",
            "Batch Training Loss =  0.008659003302454948\n",
            "Batch Training Loss =  0.009149336256086826\n",
            "Batch Training Loss =  0.007720235735177994\n",
            "Batch Training Loss =  0.010563323274254799\n",
            "Batch Training Loss =  0.00932499673217535\n",
            "Batch Training Loss =  0.00953059084713459\n",
            "Batch Training Loss =  0.010515782050788403\n",
            "Batch Training Loss =  0.04521273076534271\n",
            "Batch Training Loss =  0.0237541776150465\n",
            "Batch Training Loss =  0.02467622049152851\n",
            "Batch Training Loss =  0.01627614162862301\n",
            "Batch Training Loss =  0.013214770704507828\n",
            "Batch Training Loss =  0.020090147852897644\n",
            "Batch Training Loss =  0.01547576766461134\n",
            "Batch Training Loss =  0.013226956129074097\n",
            "Batch Training Loss =  0.011837191879749298\n",
            "Batch Training Loss =  0.021855447441339493\n",
            "Batch Training Loss =  0.0074179768562316895\n",
            "Batch Training Loss =  0.03345172479748726\n",
            "Batch Training Loss =  0.009914763271808624\n",
            "Validation Loss in this epoch is 0.588\n",
            "This is  93 th epoch\n",
            "Batch Training Loss =  0.01889115944504738\n",
            "Batch Training Loss =  0.011023886501789093\n",
            "Batch Training Loss =  0.008404264226555824\n",
            "Batch Training Loss =  0.014682669192552567\n",
            "Batch Training Loss =  0.02310948446393013\n",
            "Batch Training Loss =  0.009685955010354519\n",
            "Batch Training Loss =  0.009768327698111534\n",
            "Batch Training Loss =  0.008604359813034534\n",
            "Batch Training Loss =  0.05135704204440117\n",
            "Batch Training Loss =  0.02778511680662632\n",
            "Batch Training Loss =  0.023220213130116463\n",
            "Batch Training Loss =  0.010193120688199997\n",
            "Batch Training Loss =  0.01819501630961895\n",
            "Batch Training Loss =  0.012485132552683353\n",
            "Batch Training Loss =  0.010305612348020077\n",
            "Batch Training Loss =  0.009454574435949326\n",
            "Batch Training Loss =  0.011791693978011608\n",
            "Batch Training Loss =  0.010026098228991032\n",
            "Batch Training Loss =  0.011079629883170128\n",
            "Batch Training Loss =  0.013301457278430462\n",
            "Batch Training Loss =  0.013851121999323368\n",
            "Batch Training Loss =  0.012066707946360111\n",
            "Batch Training Loss =  0.04962119087576866\n",
            "Batch Training Loss =  0.014992215670645237\n",
            "Batch Training Loss =  0.024086659774184227\n",
            "Batch Training Loss =  0.02448263391852379\n",
            "Batch Training Loss =  0.021442532539367676\n",
            "Batch Training Loss =  0.018427126109600067\n",
            "Batch Training Loss =  0.011150923557579517\n",
            "Batch Training Loss =  0.008456757292151451\n",
            "Batch Training Loss =  0.014429854229092598\n",
            "Batch Training Loss =  0.021671421825885773\n",
            "Validation Loss in this epoch is 0.598\n",
            "This is  94 th epoch\n",
            "Batch Training Loss =  0.010487757623195648\n",
            "Batch Training Loss =  0.008049672469496727\n",
            "Batch Training Loss =  0.006934504956007004\n",
            "Batch Training Loss =  0.01285588275641203\n",
            "Batch Training Loss =  0.012833417393267155\n",
            "Batch Training Loss =  0.007669443264603615\n",
            "Batch Training Loss =  0.009316369891166687\n",
            "Batch Training Loss =  0.013997214846313\n",
            "Batch Training Loss =  0.010191040113568306\n",
            "Batch Training Loss =  0.011737460270524025\n",
            "Batch Training Loss =  0.011488544754683971\n",
            "Batch Training Loss =  0.014321234077215195\n",
            "Batch Training Loss =  0.014287672005593777\n",
            "Batch Training Loss =  0.05667094886302948\n",
            "Batch Training Loss =  0.029129724949598312\n",
            "Batch Training Loss =  0.01838870346546173\n",
            "Batch Training Loss =  0.017791103571653366\n",
            "Batch Training Loss =  0.014463072642683983\n",
            "Batch Training Loss =  0.025753261521458626\n",
            "Batch Training Loss =  0.012064378708600998\n",
            "Batch Training Loss =  0.011202109977602959\n",
            "Batch Training Loss =  0.014216835610568523\n",
            "Batch Training Loss =  0.010812448337674141\n",
            "Batch Training Loss =  0.013114559464156628\n",
            "Batch Training Loss =  0.010352659039199352\n",
            "Batch Training Loss =  0.028819585219025612\n",
            "Batch Training Loss =  0.014059283770620823\n",
            "Batch Training Loss =  0.009344595484435558\n",
            "Batch Training Loss =  0.010460682213306427\n",
            "Batch Training Loss =  0.020808320492506027\n",
            "Batch Training Loss =  0.007335701957345009\n",
            "Batch Training Loss =  0.014460326172411442\n",
            "Validation Loss in this epoch is 0.604\n",
            "This is  95 th epoch\n",
            "Batch Training Loss =  0.009008622728288174\n",
            "Batch Training Loss =  0.010601561516523361\n",
            "Batch Training Loss =  0.010984327644109726\n",
            "Batch Training Loss =  0.007713401224464178\n",
            "Batch Training Loss =  0.019688058644533157\n",
            "Batch Training Loss =  0.03551416099071503\n",
            "Batch Training Loss =  0.03278472647070885\n",
            "Batch Training Loss =  0.01366248819977045\n",
            "Batch Training Loss =  0.01073218323290348\n",
            "Batch Training Loss =  0.015313978306949139\n",
            "Batch Training Loss =  0.010800833813846111\n",
            "Batch Training Loss =  0.03037419728934765\n",
            "Batch Training Loss =  0.01656276360154152\n",
            "Batch Training Loss =  0.0094774654135108\n",
            "Batch Training Loss =  0.013805942609906197\n",
            "Batch Training Loss =  0.019869886338710785\n",
            "Batch Training Loss =  0.017349688336253166\n",
            "Batch Training Loss =  0.02142953872680664\n",
            "Batch Training Loss =  0.010803998447954655\n",
            "Batch Training Loss =  0.01581277884542942\n",
            "Batch Training Loss =  0.014073468744754791\n",
            "Batch Training Loss =  0.009550212882459164\n",
            "Batch Training Loss =  0.019427429884672165\n",
            "Batch Training Loss =  0.014413099735975266\n",
            "Batch Training Loss =  0.00894937478005886\n",
            "Batch Training Loss =  0.011398890987038612\n",
            "Batch Training Loss =  0.01464037038385868\n",
            "Batch Training Loss =  0.013412249274551868\n",
            "Batch Training Loss =  0.010886167176067829\n",
            "Batch Training Loss =  0.010701552964746952\n",
            "Batch Training Loss =  0.01018416415899992\n",
            "Batch Training Loss =  0.01313906442373991\n",
            "Validation Loss in this epoch is 0.593\n",
            "This is  96 th epoch\n",
            "Batch Training Loss =  0.0077423169277608395\n",
            "Batch Training Loss =  0.010967915877699852\n",
            "Batch Training Loss =  0.010485955514013767\n",
            "Batch Training Loss =  0.009288722649216652\n",
            "Batch Training Loss =  0.007660225033760071\n",
            "Batch Training Loss =  0.009081011638045311\n",
            "Batch Training Loss =  0.009408366866409779\n",
            "Batch Training Loss =  0.012123247608542442\n",
            "Batch Training Loss =  0.03130928799510002\n",
            "Batch Training Loss =  0.02324685826897621\n",
            "Batch Training Loss =  0.009593475610017776\n",
            "Batch Training Loss =  0.02078261412680149\n",
            "Batch Training Loss =  0.011512667872011662\n",
            "Batch Training Loss =  0.013478091917932034\n",
            "Batch Training Loss =  0.009030311368405819\n",
            "Batch Training Loss =  0.010432103648781776\n",
            "Batch Training Loss =  0.0106120053678751\n",
            "Batch Training Loss =  0.01357848010957241\n",
            "Batch Training Loss =  0.012932316400110722\n",
            "Batch Training Loss =  0.012161828577518463\n",
            "Batch Training Loss =  0.03566122055053711\n",
            "Batch Training Loss =  0.025980262085795403\n",
            "Batch Training Loss =  0.014704409055411816\n",
            "Batch Training Loss =  0.019329627975821495\n",
            "Batch Training Loss =  0.016833340749144554\n",
            "Batch Training Loss =  0.012677636928856373\n",
            "Batch Training Loss =  0.008488093502819538\n",
            "Batch Training Loss =  0.013990090228617191\n",
            "Batch Training Loss =  0.010279519483447075\n",
            "Batch Training Loss =  0.012261968106031418\n",
            "Batch Training Loss =  0.01219209935516119\n",
            "Batch Training Loss =  0.02959352731704712\n",
            "Validation Loss in this epoch is 0.576\n",
            "This is  97 th epoch\n",
            "Batch Training Loss =  0.02014441043138504\n",
            "Batch Training Loss =  0.018364446237683296\n",
            "Batch Training Loss =  0.010934988036751747\n",
            "Batch Training Loss =  0.012664109468460083\n",
            "Batch Training Loss =  0.007846919819712639\n",
            "Batch Training Loss =  0.01710498332977295\n",
            "Batch Training Loss =  0.008115184493362904\n",
            "Batch Training Loss =  0.01117987185716629\n",
            "Batch Training Loss =  0.007063882891088724\n",
            "Batch Training Loss =  0.009547562338411808\n",
            "Batch Training Loss =  0.015062379650771618\n",
            "Batch Training Loss =  0.012861311435699463\n",
            "Batch Training Loss =  0.009172355756163597\n",
            "Batch Training Loss =  0.010939613915979862\n",
            "Batch Training Loss =  0.010629283264279366\n",
            "Batch Training Loss =  0.008704086765646935\n",
            "Batch Training Loss =  0.008722319267690182\n",
            "Batch Training Loss =  0.03572339937090874\n",
            "Batch Training Loss =  0.017002809792757034\n",
            "Batch Training Loss =  0.011845359578728676\n",
            "Batch Training Loss =  0.02150096371769905\n",
            "Batch Training Loss =  0.01099912729114294\n",
            "Batch Training Loss =  0.014948501251637936\n",
            "Batch Training Loss =  0.013869733549654484\n",
            "Batch Training Loss =  0.012327086180448532\n",
            "Batch Training Loss =  0.010095671750605106\n",
            "Batch Training Loss =  0.016212845221161842\n",
            "Batch Training Loss =  0.012494788505136967\n",
            "Batch Training Loss =  0.011912035755813122\n",
            "Batch Training Loss =  0.03366558998823166\n",
            "Batch Training Loss =  0.02354721538722515\n",
            "Batch Training Loss =  0.01555259246379137\n",
            "Validation Loss in this epoch is 0.575\n",
            "This is  98 th epoch\n",
            "Batch Training Loss =  0.030917735770344734\n",
            "Batch Training Loss =  0.008682253770530224\n",
            "Batch Training Loss =  0.020236440002918243\n",
            "Batch Training Loss =  0.009139477275311947\n",
            "Batch Training Loss =  0.02195175178349018\n",
            "Batch Training Loss =  0.01667887344956398\n",
            "Batch Training Loss =  0.01086913701146841\n",
            "Batch Training Loss =  0.01296276319772005\n",
            "Batch Training Loss =  0.011542966589331627\n",
            "Batch Training Loss =  0.01270101685076952\n",
            "Batch Training Loss =  0.010830676183104515\n",
            "Batch Training Loss =  0.013476133346557617\n",
            "Batch Training Loss =  0.00919391494244337\n",
            "Batch Training Loss =  0.006988219451159239\n",
            "Batch Training Loss =  0.00783053319901228\n",
            "Batch Training Loss =  0.00718949967995286\n",
            "Batch Training Loss =  0.006971111986786127\n",
            "Batch Training Loss =  0.011140694841742516\n",
            "Batch Training Loss =  0.01191815733909607\n",
            "Batch Training Loss =  0.012303601950407028\n",
            "Batch Training Loss =  0.010311197489500046\n",
            "Batch Training Loss =  0.014095365069806576\n",
            "Batch Training Loss =  0.012938162311911583\n",
            "Batch Training Loss =  0.010526218451559544\n",
            "Batch Training Loss =  0.013907491229474545\n",
            "Batch Training Loss =  0.011803713627159595\n",
            "Batch Training Loss =  0.011284486390650272\n",
            "Batch Training Loss =  0.011246307753026485\n",
            "Batch Training Loss =  0.011393379420042038\n",
            "Batch Training Loss =  0.011430916376411915\n",
            "Batch Training Loss =  0.05279409512877464\n",
            "Batch Training Loss =  0.035555675625801086\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  99 th epoch\n",
            "Batch Training Loss =  0.013051426038146019\n",
            "Batch Training Loss =  0.024956142529845238\n",
            "Batch Training Loss =  0.010127436369657516\n",
            "Batch Training Loss =  0.018742822110652924\n",
            "Batch Training Loss =  0.014432244002819061\n",
            "Batch Training Loss =  0.010179620236158371\n",
            "Batch Training Loss =  0.012238379567861557\n",
            "Batch Training Loss =  0.010721828788518906\n",
            "Batch Training Loss =  0.010708984918892384\n",
            "Batch Training Loss =  0.036613572388887405\n",
            "Batch Training Loss =  0.016185792163014412\n",
            "Batch Training Loss =  0.028983015567064285\n",
            "Batch Training Loss =  0.012653299607336521\n",
            "Batch Training Loss =  0.027276499196887016\n",
            "Batch Training Loss =  0.009860320948064327\n",
            "Batch Training Loss =  0.014918110333383083\n",
            "Batch Training Loss =  0.011544865556061268\n",
            "Batch Training Loss =  0.013044467195868492\n",
            "Batch Training Loss =  0.015064967796206474\n",
            "Batch Training Loss =  0.015265269204974174\n",
            "Batch Training Loss =  0.04033474996685982\n",
            "Batch Training Loss =  0.032751306891441345\n",
            "Batch Training Loss =  0.012382414191961288\n",
            "Batch Training Loss =  0.010727907530963421\n",
            "Batch Training Loss =  0.028258103877305984\n",
            "Batch Training Loss =  0.021372193470597267\n",
            "Batch Training Loss =  0.015633393079042435\n",
            "Batch Training Loss =  0.014193895272910595\n",
            "Batch Training Loss =  0.008717441000044346\n",
            "Batch Training Loss =  0.009767112322151661\n",
            "Batch Training Loss =  0.011957002803683281\n",
            "Batch Training Loss =  0.008744907565414906\n",
            "Validation Loss in this epoch is 0.599\n",
            "This is  100 th epoch\n",
            "Batch Training Loss =  0.011152595281600952\n",
            "Batch Training Loss =  0.010514501482248306\n",
            "Batch Training Loss =  0.011309407651424408\n",
            "Batch Training Loss =  0.010689424350857735\n",
            "Batch Training Loss =  0.012355532497167587\n",
            "Batch Training Loss =  0.010426563210785389\n",
            "Batch Training Loss =  0.0176936574280262\n",
            "Batch Training Loss =  0.010188681073486805\n",
            "Batch Training Loss =  0.013100329786539078\n",
            "Batch Training Loss =  0.010173472575843334\n",
            "Batch Training Loss =  0.01320105791091919\n",
            "Batch Training Loss =  0.01190928090363741\n",
            "Batch Training Loss =  0.011370886117219925\n",
            "Batch Training Loss =  0.006982156541198492\n",
            "Batch Training Loss =  0.008904845453798771\n",
            "Batch Training Loss =  0.03392445296049118\n",
            "Batch Training Loss =  0.011221297085285187\n",
            "Batch Training Loss =  0.01117999292910099\n",
            "Batch Training Loss =  0.01252503041177988\n",
            "Batch Training Loss =  0.01217444147914648\n",
            "Batch Training Loss =  0.009028499014675617\n",
            "Batch Training Loss =  0.01117924228310585\n",
            "Batch Training Loss =  0.01228339597582817\n",
            "Batch Training Loss =  0.014082803390920162\n",
            "Batch Training Loss =  0.015677589923143387\n",
            "Batch Training Loss =  0.02862439677119255\n",
            "Batch Training Loss =  0.03409663215279579\n",
            "Batch Training Loss =  0.01985669881105423\n",
            "Batch Training Loss =  0.0191673431545496\n",
            "Batch Training Loss =  0.01546756736934185\n",
            "Batch Training Loss =  0.013713222928345203\n",
            "Batch Training Loss =  0.00969608873128891\n",
            "Validation Loss in this epoch is 0.582\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6963181495666504\n",
            "Batch Training Loss =  0.6943956613540649\n",
            "Batch Training Loss =  0.6912640929222107\n",
            "Batch Training Loss =  0.6832984089851379\n",
            "Batch Training Loss =  0.6817351579666138\n",
            "Batch Training Loss =  0.6872583031654358\n",
            "Batch Training Loss =  0.6765788197517395\n",
            "Batch Training Loss =  0.6746006011962891\n",
            "Batch Training Loss =  0.6701606512069702\n",
            "Batch Training Loss =  0.6726769804954529\n",
            "Batch Training Loss =  0.6580902934074402\n",
            "Batch Training Loss =  0.6432673931121826\n",
            "Batch Training Loss =  0.6560500860214233\n",
            "Batch Training Loss =  0.6254451274871826\n",
            "Batch Training Loss =  0.6367303729057312\n",
            "Batch Training Loss =  0.6174015998840332\n",
            "Batch Training Loss =  0.6255578994750977\n",
            "Batch Training Loss =  0.6138546466827393\n",
            "Batch Training Loss =  0.6285220384597778\n",
            "Batch Training Loss =  0.6237659454345703\n",
            "Batch Training Loss =  0.5740172863006592\n",
            "Batch Training Loss =  0.6188272833824158\n",
            "Batch Training Loss =  0.6327028870582581\n",
            "Batch Training Loss =  0.5680205225944519\n",
            "Batch Training Loss =  0.5818730592727661\n",
            "Batch Training Loss =  0.5661458373069763\n",
            "Batch Training Loss =  0.5803999900817871\n",
            "Batch Training Loss =  0.6005476713180542\n",
            "Batch Training Loss =  0.5628410577774048\n",
            "Batch Training Loss =  0.5827479958534241\n",
            "Batch Training Loss =  0.6094530820846558\n",
            "Batch Training Loss =  0.565849781036377\n",
            "Validation Loss in this epoch is 0.562\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.5620259642601013\n",
            "Batch Training Loss =  0.5021945834159851\n",
            "Batch Training Loss =  0.5206681489944458\n",
            "Batch Training Loss =  0.5288395285606384\n",
            "Batch Training Loss =  0.48355910181999207\n",
            "Batch Training Loss =  0.5322467684745789\n",
            "Batch Training Loss =  0.552707314491272\n",
            "Batch Training Loss =  0.5538738965988159\n",
            "Batch Training Loss =  0.4903756380081177\n",
            "Batch Training Loss =  0.5542685985565186\n",
            "Batch Training Loss =  0.47765395045280457\n",
            "Batch Training Loss =  0.47784897685050964\n",
            "Batch Training Loss =  0.5495232939720154\n",
            "Batch Training Loss =  0.43539685010910034\n",
            "Batch Training Loss =  0.4712086617946625\n",
            "Batch Training Loss =  0.5103204846382141\n",
            "Batch Training Loss =  0.489835649728775\n",
            "Batch Training Loss =  0.43439099192619324\n",
            "Batch Training Loss =  0.4321092963218689\n",
            "Batch Training Loss =  0.43280619382858276\n",
            "Batch Training Loss =  0.3858967125415802\n",
            "Batch Training Loss =  0.5907707810401917\n",
            "Batch Training Loss =  0.40363040566444397\n",
            "Batch Training Loss =  0.4827178716659546\n",
            "Batch Training Loss =  0.44005873799324036\n",
            "Batch Training Loss =  0.45078885555267334\n",
            "Batch Training Loss =  0.3889566659927368\n",
            "Batch Training Loss =  0.4126448929309845\n",
            "Batch Training Loss =  0.5487258434295654\n",
            "Batch Training Loss =  0.5669036507606506\n",
            "Batch Training Loss =  0.4653712511062622\n",
            "Batch Training Loss =  0.42119476199150085\n",
            "Validation Loss in this epoch is 0.430\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.3995392918586731\n",
            "Batch Training Loss =  0.5397523641586304\n",
            "Batch Training Loss =  0.43578240275382996\n",
            "Batch Training Loss =  0.42426803708076477\n",
            "Batch Training Loss =  0.34675106406211853\n",
            "Batch Training Loss =  0.36853206157684326\n",
            "Batch Training Loss =  0.47716784477233887\n",
            "Batch Training Loss =  0.3381844758987427\n",
            "Batch Training Loss =  0.34778767824172974\n",
            "Batch Training Loss =  0.3349553644657135\n",
            "Batch Training Loss =  0.3787514865398407\n",
            "Batch Training Loss =  0.4247667193412781\n",
            "Batch Training Loss =  0.3373185098171234\n",
            "Batch Training Loss =  0.2652766704559326\n",
            "Batch Training Loss =  0.3854370415210724\n",
            "Batch Training Loss =  0.289924681186676\n",
            "Batch Training Loss =  0.35533028841018677\n",
            "Batch Training Loss =  0.379172146320343\n",
            "Batch Training Loss =  0.43830153346061707\n",
            "Batch Training Loss =  0.7337856292724609\n",
            "Batch Training Loss =  0.5163480639457703\n",
            "Batch Training Loss =  0.4400073289871216\n",
            "Batch Training Loss =  0.3497113287448883\n",
            "Batch Training Loss =  0.39886653423309326\n",
            "Batch Training Loss =  0.3267974555492401\n",
            "Batch Training Loss =  0.29825133085250854\n",
            "Batch Training Loss =  0.4388224482536316\n",
            "Batch Training Loss =  0.41362231969833374\n",
            "Batch Training Loss =  0.3516506552696228\n",
            "Batch Training Loss =  0.31688764691352844\n",
            "Batch Training Loss =  0.4293076992034912\n",
            "Batch Training Loss =  0.3312831223011017\n",
            "Validation Loss in this epoch is 0.394\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.2870357632637024\n",
            "Batch Training Loss =  0.3103913962841034\n",
            "Batch Training Loss =  0.3440498411655426\n",
            "Batch Training Loss =  0.2905041575431824\n",
            "Batch Training Loss =  0.34291303157806396\n",
            "Batch Training Loss =  0.29747891426086426\n",
            "Batch Training Loss =  0.2808256447315216\n",
            "Batch Training Loss =  0.3536442220211029\n",
            "Batch Training Loss =  0.2821621000766754\n",
            "Batch Training Loss =  0.41253939270973206\n",
            "Batch Training Loss =  0.42888739705085754\n",
            "Batch Training Loss =  0.2856132388114929\n",
            "Batch Training Loss =  0.2785058915615082\n",
            "Batch Training Loss =  0.48470538854599\n",
            "Batch Training Loss =  0.27128660678863525\n",
            "Batch Training Loss =  0.3576551675796509\n",
            "Batch Training Loss =  0.2728581726551056\n",
            "Batch Training Loss =  0.29128870368003845\n",
            "Batch Training Loss =  0.3612792193889618\n",
            "Batch Training Loss =  0.3542422950267792\n",
            "Batch Training Loss =  0.35788094997406006\n",
            "Batch Training Loss =  0.4269770085811615\n",
            "Batch Training Loss =  0.3188454806804657\n",
            "Batch Training Loss =  0.2906104624271393\n",
            "Batch Training Loss =  0.31828105449676514\n",
            "Batch Training Loss =  0.2969033122062683\n",
            "Batch Training Loss =  0.31774941086769104\n",
            "Batch Training Loss =  0.3040597140789032\n",
            "Batch Training Loss =  0.22900600731372833\n",
            "Batch Training Loss =  0.3476155400276184\n",
            "Batch Training Loss =  0.41765570640563965\n",
            "Batch Training Loss =  0.47553199529647827\n",
            "Validation Loss in this epoch is 0.403\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.23239797353744507\n",
            "Batch Training Loss =  0.3324938118457794\n",
            "Batch Training Loss =  0.1881362944841385\n",
            "Batch Training Loss =  0.25302770733833313\n",
            "Batch Training Loss =  0.2091478407382965\n",
            "Batch Training Loss =  0.28806713223457336\n",
            "Batch Training Loss =  0.34654471278190613\n",
            "Batch Training Loss =  0.3500256836414337\n",
            "Batch Training Loss =  0.3540053069591522\n",
            "Batch Training Loss =  0.287882924079895\n",
            "Batch Training Loss =  0.4136999547481537\n",
            "Batch Training Loss =  0.20592328906059265\n",
            "Batch Training Loss =  0.2774665057659149\n",
            "Batch Training Loss =  0.20452338457107544\n",
            "Batch Training Loss =  0.2583080232143402\n",
            "Batch Training Loss =  0.1736820936203003\n",
            "Batch Training Loss =  0.31856077909469604\n",
            "Batch Training Loss =  0.4727436900138855\n",
            "Batch Training Loss =  0.3271232843399048\n",
            "Batch Training Loss =  0.3090987503528595\n",
            "Batch Training Loss =  0.28263598680496216\n",
            "Batch Training Loss =  0.2787119150161743\n",
            "Batch Training Loss =  0.18136560916900635\n",
            "Batch Training Loss =  0.18585851788520813\n",
            "Batch Training Loss =  0.2858349680900574\n",
            "Batch Training Loss =  0.33351171016693115\n",
            "Batch Training Loss =  0.2900717556476593\n",
            "Batch Training Loss =  0.31629687547683716\n",
            "Batch Training Loss =  0.3054233193397522\n",
            "Batch Training Loss =  0.23614992201328278\n",
            "Batch Training Loss =  0.20638616383075714\n",
            "Batch Training Loss =  0.23658327758312225\n",
            "Validation Loss in this epoch is 0.371\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.13185662031173706\n",
            "Batch Training Loss =  0.18521983921527863\n",
            "Batch Training Loss =  0.3613869547843933\n",
            "Batch Training Loss =  0.15868978202342987\n",
            "Batch Training Loss =  0.1562039703130722\n",
            "Batch Training Loss =  0.22033126652240753\n",
            "Batch Training Loss =  0.1229345053434372\n",
            "Batch Training Loss =  0.21976199746131897\n",
            "Batch Training Loss =  0.22698833048343658\n",
            "Batch Training Loss =  0.14811289310455322\n",
            "Batch Training Loss =  0.18222106993198395\n",
            "Batch Training Loss =  0.27965351939201355\n",
            "Batch Training Loss =  0.1631239950656891\n",
            "Batch Training Loss =  0.28086602687835693\n",
            "Batch Training Loss =  0.28447532653808594\n",
            "Batch Training Loss =  0.20547477900981903\n",
            "Batch Training Loss =  0.38890334963798523\n",
            "Batch Training Loss =  0.1752123236656189\n",
            "Batch Training Loss =  0.1286756694316864\n",
            "Batch Training Loss =  0.2050904631614685\n",
            "Batch Training Loss =  0.13549642264842987\n",
            "Batch Training Loss =  0.3135146200656891\n",
            "Batch Training Loss =  0.3340786397457123\n",
            "Batch Training Loss =  0.4152544438838959\n",
            "Batch Training Loss =  0.2704181373119354\n",
            "Batch Training Loss =  0.3365998864173889\n",
            "Batch Training Loss =  0.3769007623195648\n",
            "Batch Training Loss =  0.1210269182920456\n",
            "Batch Training Loss =  0.3324134945869446\n",
            "Batch Training Loss =  0.23991160094738007\n",
            "Batch Training Loss =  0.23584386706352234\n",
            "Batch Training Loss =  0.33752551674842834\n",
            "Validation Loss in this epoch is 0.381\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.19749179482460022\n",
            "Batch Training Loss =  0.1653132289648056\n",
            "Batch Training Loss =  0.12307576835155487\n",
            "Batch Training Loss =  0.2649727463722229\n",
            "Batch Training Loss =  0.15749819576740265\n",
            "Batch Training Loss =  0.1883634626865387\n",
            "Batch Training Loss =  0.1999724954366684\n",
            "Batch Training Loss =  0.12423907965421677\n",
            "Batch Training Loss =  0.077703095972538\n",
            "Batch Training Loss =  0.16055423021316528\n",
            "Batch Training Loss =  0.1485564112663269\n",
            "Batch Training Loss =  0.18192358314990997\n",
            "Batch Training Loss =  0.19889894127845764\n",
            "Batch Training Loss =  0.1495329886674881\n",
            "Batch Training Loss =  0.1829950362443924\n",
            "Batch Training Loss =  0.37836167216300964\n",
            "Batch Training Loss =  0.39598914980888367\n",
            "Batch Training Loss =  0.31232619285583496\n",
            "Batch Training Loss =  0.5497072339057922\n",
            "Batch Training Loss =  0.6625542640686035\n",
            "Batch Training Loss =  0.21297146379947662\n",
            "Batch Training Loss =  0.294008731842041\n",
            "Batch Training Loss =  0.3386211693286896\n",
            "Batch Training Loss =  0.19087941944599152\n",
            "Batch Training Loss =  0.1374255269765854\n",
            "Batch Training Loss =  0.26624274253845215\n",
            "Batch Training Loss =  0.2605970799922943\n",
            "Batch Training Loss =  0.1971367448568344\n",
            "Batch Training Loss =  0.2292264699935913\n",
            "Batch Training Loss =  0.20453830063343048\n",
            "Batch Training Loss =  0.13120004534721375\n",
            "Batch Training Loss =  0.25901344418525696\n",
            "Validation Loss in this epoch is 0.348\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.10040614008903503\n",
            "Batch Training Loss =  0.09799062460660934\n",
            "Batch Training Loss =  0.0946192741394043\n",
            "Batch Training Loss =  0.09447737038135529\n",
            "Batch Training Loss =  0.1671118289232254\n",
            "Batch Training Loss =  0.138003870844841\n",
            "Batch Training Loss =  0.1354096680879593\n",
            "Batch Training Loss =  0.18817470967769623\n",
            "Batch Training Loss =  0.1405944675207138\n",
            "Batch Training Loss =  0.15958364307880402\n",
            "Batch Training Loss =  0.18617521226406097\n",
            "Batch Training Loss =  0.13441967964172363\n",
            "Batch Training Loss =  0.1003284826874733\n",
            "Batch Training Loss =  0.24023616313934326\n",
            "Batch Training Loss =  0.17293131351470947\n",
            "Batch Training Loss =  0.12661781907081604\n",
            "Batch Training Loss =  0.15219441056251526\n",
            "Batch Training Loss =  0.1856335699558258\n",
            "Batch Training Loss =  0.15894478559494019\n",
            "Batch Training Loss =  0.3450202941894531\n",
            "Batch Training Loss =  0.21737529337406158\n",
            "Batch Training Loss =  0.28654369711875916\n",
            "Batch Training Loss =  0.1859651803970337\n",
            "Batch Training Loss =  0.15522164106369019\n",
            "Batch Training Loss =  0.16066426038742065\n",
            "Batch Training Loss =  0.21447911858558655\n",
            "Batch Training Loss =  0.13023264706134796\n",
            "Batch Training Loss =  0.1847204715013504\n",
            "Batch Training Loss =  0.2239668220281601\n",
            "Batch Training Loss =  0.1897595077753067\n",
            "Batch Training Loss =  0.11927900463342667\n",
            "Batch Training Loss =  0.17344261705875397\n",
            "Validation Loss in this epoch is 0.335\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.18398109078407288\n",
            "Batch Training Loss =  0.10605302453041077\n",
            "Batch Training Loss =  0.13202160596847534\n",
            "Batch Training Loss =  0.06967023760080338\n",
            "Batch Training Loss =  0.09080053120851517\n",
            "Batch Training Loss =  0.20320957899093628\n",
            "Batch Training Loss =  0.15293888747692108\n",
            "Batch Training Loss =  0.08599734306335449\n",
            "Batch Training Loss =  0.07348031550645828\n",
            "Batch Training Loss =  0.07310964167118073\n",
            "Batch Training Loss =  0.06033185124397278\n",
            "Batch Training Loss =  0.07997023314237595\n",
            "Batch Training Loss =  0.10494621098041534\n",
            "Batch Training Loss =  0.06794972717761993\n",
            "Batch Training Loss =  0.13966339826583862\n",
            "Batch Training Loss =  0.08312436193227768\n",
            "Batch Training Loss =  0.07194498181343079\n",
            "Batch Training Loss =  0.058642029762268066\n",
            "Batch Training Loss =  0.06912710517644882\n",
            "Batch Training Loss =  0.11751171201467514\n",
            "Batch Training Loss =  0.11339115351438522\n",
            "Batch Training Loss =  0.2419813573360443\n",
            "Batch Training Loss =  0.07408871501684189\n",
            "Batch Training Loss =  0.14072978496551514\n",
            "Batch Training Loss =  0.16720882058143616\n",
            "Batch Training Loss =  0.09869368374347687\n",
            "Batch Training Loss =  0.06647224724292755\n",
            "Batch Training Loss =  0.14664869010448456\n",
            "Batch Training Loss =  0.32355690002441406\n",
            "Batch Training Loss =  0.21017737686634064\n",
            "Batch Training Loss =  0.29312556982040405\n",
            "Batch Training Loss =  0.23210783302783966\n",
            "Validation Loss in this epoch is 0.385\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.10098056495189667\n",
            "Batch Training Loss =  0.09533782303333282\n",
            "Batch Training Loss =  0.0990321934223175\n",
            "Batch Training Loss =  0.08678150177001953\n",
            "Batch Training Loss =  0.10940957814455032\n",
            "Batch Training Loss =  0.09071508049964905\n",
            "Batch Training Loss =  0.06514251977205276\n",
            "Batch Training Loss =  0.07991193979978561\n",
            "Batch Training Loss =  0.047662388533353806\n",
            "Batch Training Loss =  0.09452633559703827\n",
            "Batch Training Loss =  0.07651352137327194\n",
            "Batch Training Loss =  0.06204335764050484\n",
            "Batch Training Loss =  0.24501067399978638\n",
            "Batch Training Loss =  0.12139452993869781\n",
            "Batch Training Loss =  0.1721438467502594\n",
            "Batch Training Loss =  0.0877867192029953\n",
            "Batch Training Loss =  0.0852745845913887\n",
            "Batch Training Loss =  0.05767052248120308\n",
            "Batch Training Loss =  0.08315209299325943\n",
            "Batch Training Loss =  0.10018022358417511\n",
            "Batch Training Loss =  0.1587444394826889\n",
            "Batch Training Loss =  0.2195812463760376\n",
            "Batch Training Loss =  0.07455846667289734\n",
            "Batch Training Loss =  0.09771085530519485\n",
            "Batch Training Loss =  0.08376400917768478\n",
            "Batch Training Loss =  0.07740077376365662\n",
            "Batch Training Loss =  0.07146536558866501\n",
            "Batch Training Loss =  0.13617652654647827\n",
            "Batch Training Loss =  0.09479179233312607\n",
            "Batch Training Loss =  0.0790654718875885\n",
            "Batch Training Loss =  0.2140345722436905\n",
            "Batch Training Loss =  0.10269656777381897\n",
            "Validation Loss in this epoch is 0.370\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.057217251509428024\n",
            "Batch Training Loss =  0.05300268158316612\n",
            "Batch Training Loss =  0.11788930743932724\n",
            "Batch Training Loss =  0.05933811515569687\n",
            "Batch Training Loss =  0.061768803745508194\n",
            "Batch Training Loss =  0.06213800609111786\n",
            "Batch Training Loss =  0.03000083938241005\n",
            "Batch Training Loss =  0.04129127040505409\n",
            "Batch Training Loss =  0.057501278817653656\n",
            "Batch Training Loss =  0.053135842084884644\n",
            "Batch Training Loss =  0.03528396785259247\n",
            "Batch Training Loss =  0.1466558426618576\n",
            "Batch Training Loss =  0.09187564253807068\n",
            "Batch Training Loss =  0.06892542541027069\n",
            "Batch Training Loss =  0.05381455272436142\n",
            "Batch Training Loss =  0.04227598011493683\n",
            "Batch Training Loss =  0.04396354779601097\n",
            "Batch Training Loss =  0.22704245150089264\n",
            "Batch Training Loss =  0.08687193691730499\n",
            "Batch Training Loss =  0.07797310501337051\n",
            "Batch Training Loss =  0.11598920077085495\n",
            "Batch Training Loss =  0.11304495483636856\n",
            "Batch Training Loss =  0.05473693460226059\n",
            "Batch Training Loss =  0.06314769387245178\n",
            "Batch Training Loss =  0.057581301778554916\n",
            "Batch Training Loss =  0.07060074061155319\n",
            "Batch Training Loss =  0.02531597390770912\n",
            "Batch Training Loss =  0.0427791029214859\n",
            "Batch Training Loss =  0.04855741932988167\n",
            "Batch Training Loss =  0.056305114179849625\n",
            "Batch Training Loss =  0.06232544407248497\n",
            "Batch Training Loss =  0.06086051091551781\n",
            "Validation Loss in this epoch is 0.372\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.03483545407652855\n",
            "Batch Training Loss =  0.04127156361937523\n",
            "Batch Training Loss =  0.1864069104194641\n",
            "Batch Training Loss =  0.09536238759756088\n",
            "Batch Training Loss =  0.05368032306432724\n",
            "Batch Training Loss =  0.083106130361557\n",
            "Batch Training Loss =  0.026298103854060173\n",
            "Batch Training Loss =  0.04616136476397514\n",
            "Batch Training Loss =  0.030700890347361565\n",
            "Batch Training Loss =  0.04569629207253456\n",
            "Batch Training Loss =  0.02579318732023239\n",
            "Batch Training Loss =  0.0737193301320076\n",
            "Batch Training Loss =  0.04044113680720329\n",
            "Batch Training Loss =  0.03649802878499031\n",
            "Batch Training Loss =  0.03070293366909027\n",
            "Batch Training Loss =  0.03058207780122757\n",
            "Batch Training Loss =  0.17894403636455536\n",
            "Batch Training Loss =  0.05548744276165962\n",
            "Batch Training Loss =  0.06701987981796265\n",
            "Batch Training Loss =  0.04146683216094971\n",
            "Batch Training Loss =  0.06684702634811401\n",
            "Batch Training Loss =  0.05410468578338623\n",
            "Batch Training Loss =  0.034427281469106674\n",
            "Batch Training Loss =  0.04856715351343155\n",
            "Batch Training Loss =  0.08053259551525116\n",
            "Batch Training Loss =  0.14049851894378662\n",
            "Batch Training Loss =  0.07505179196596146\n",
            "Batch Training Loss =  0.05550551787018776\n",
            "Batch Training Loss =  0.038897812366485596\n",
            "Batch Training Loss =  0.028467968106269836\n",
            "Batch Training Loss =  0.020875655114650726\n",
            "Batch Training Loss =  0.03320007398724556\n",
            "Validation Loss in this epoch is 0.382\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.03518589958548546\n",
            "Batch Training Loss =  0.020547958090901375\n",
            "Batch Training Loss =  0.137751042842865\n",
            "Batch Training Loss =  0.05576704069972038\n",
            "Batch Training Loss =  0.051535364240407944\n",
            "Batch Training Loss =  0.017860643565654755\n",
            "Batch Training Loss =  0.027912946417927742\n",
            "Batch Training Loss =  0.08812687546014786\n",
            "Batch Training Loss =  0.08557170629501343\n",
            "Batch Training Loss =  0.053949590772390366\n",
            "Batch Training Loss =  0.05812142416834831\n",
            "Batch Training Loss =  0.054183363914489746\n",
            "Batch Training Loss =  0.04841354489326477\n",
            "Batch Training Loss =  0.03443605825304985\n",
            "Batch Training Loss =  0.025301380082964897\n",
            "Batch Training Loss =  0.024822628125548363\n",
            "Batch Training Loss =  0.032861657440662384\n",
            "Batch Training Loss =  0.05634903907775879\n",
            "Batch Training Loss =  0.027993731200695038\n",
            "Batch Training Loss =  0.018154604360461235\n",
            "Batch Training Loss =  0.12175613641738892\n",
            "Batch Training Loss =  0.025710012763738632\n",
            "Batch Training Loss =  0.037735551595687866\n",
            "Batch Training Loss =  0.045364681631326675\n",
            "Batch Training Loss =  0.025417760014533997\n",
            "Batch Training Loss =  0.027296461164951324\n",
            "Batch Training Loss =  0.027099046856164932\n",
            "Batch Training Loss =  0.02858012355864048\n",
            "Batch Training Loss =  0.041502952575683594\n",
            "Batch Training Loss =  0.03045058809220791\n",
            "Batch Training Loss =  0.025193162262439728\n",
            "Batch Training Loss =  0.19110813736915588\n",
            "Validation Loss in this epoch is 0.431\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.08742348849773407\n",
            "Batch Training Loss =  0.1581970602273941\n",
            "Batch Training Loss =  0.03850569203495979\n",
            "Batch Training Loss =  0.03150216117501259\n",
            "Batch Training Loss =  0.0532839260995388\n",
            "Batch Training Loss =  0.04521524906158447\n",
            "Batch Training Loss =  0.02389560267329216\n",
            "Batch Training Loss =  0.018301518633961678\n",
            "Batch Training Loss =  0.023402025923132896\n",
            "Batch Training Loss =  0.04301629215478897\n",
            "Batch Training Loss =  0.042233120650053024\n",
            "Batch Training Loss =  0.030793434008955956\n",
            "Batch Training Loss =  0.022024035453796387\n",
            "Batch Training Loss =  0.018756788223981857\n",
            "Batch Training Loss =  0.02205584943294525\n",
            "Batch Training Loss =  0.022626573219895363\n",
            "Batch Training Loss =  0.07130613178014755\n",
            "Batch Training Loss =  0.09646318107843399\n",
            "Batch Training Loss =  0.02652418427169323\n",
            "Batch Training Loss =  0.03905291482806206\n",
            "Batch Training Loss =  0.024426862597465515\n",
            "Batch Training Loss =  0.13874304294586182\n",
            "Batch Training Loss =  0.16216501593589783\n",
            "Batch Training Loss =  0.046557795256376266\n",
            "Batch Training Loss =  0.03190424293279648\n",
            "Batch Training Loss =  0.030104706063866615\n",
            "Batch Training Loss =  0.12302733212709427\n",
            "Batch Training Loss =  0.07180330157279968\n",
            "Batch Training Loss =  0.03487088531255722\n",
            "Batch Training Loss =  0.03185233846306801\n",
            "Batch Training Loss =  0.023311156779527664\n",
            "Batch Training Loss =  0.09383895993232727\n",
            "Validation Loss in this epoch is 0.383\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.0364503376185894\n",
            "Batch Training Loss =  0.0286378413438797\n",
            "Batch Training Loss =  0.018632963299751282\n",
            "Batch Training Loss =  0.027379225939512253\n",
            "Batch Training Loss =  0.020622264593839645\n",
            "Batch Training Loss =  0.0937679260969162\n",
            "Batch Training Loss =  0.024775605648756027\n",
            "Batch Training Loss =  0.018623992800712585\n",
            "Batch Training Loss =  0.02399960346519947\n",
            "Batch Training Loss =  0.03660636767745018\n",
            "Batch Training Loss =  0.026226162910461426\n",
            "Batch Training Loss =  0.037280838936567307\n",
            "Batch Training Loss =  0.03322318196296692\n",
            "Batch Training Loss =  0.035591837018728256\n",
            "Batch Training Loss =  0.025480182841420174\n",
            "Batch Training Loss =  0.025777414441108704\n",
            "Batch Training Loss =  0.021093212068080902\n",
            "Batch Training Loss =  0.01947072334587574\n",
            "Batch Training Loss =  0.017697295174002647\n",
            "Batch Training Loss =  0.017699699848890305\n",
            "Batch Training Loss =  0.11039410531520844\n",
            "Batch Training Loss =  0.07297244668006897\n",
            "Batch Training Loss =  0.03487446904182434\n",
            "Batch Training Loss =  0.016381995752453804\n",
            "Batch Training Loss =  0.018247192725539207\n",
            "Batch Training Loss =  0.09983723610639572\n",
            "Batch Training Loss =  0.06606388092041016\n",
            "Batch Training Loss =  0.0360344722867012\n",
            "Batch Training Loss =  0.04407370090484619\n",
            "Batch Training Loss =  0.02720344066619873\n",
            "Batch Training Loss =  0.038372647017240524\n",
            "Batch Training Loss =  0.024165675044059753\n",
            "Validation Loss in this epoch is 0.391\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.08146995306015015\n",
            "Batch Training Loss =  0.04937316104769707\n",
            "Batch Training Loss =  0.018606629222631454\n",
            "Batch Training Loss =  0.024881871417164803\n",
            "Batch Training Loss =  0.08750894665718079\n",
            "Batch Training Loss =  0.06231483072042465\n",
            "Batch Training Loss =  0.014448571018874645\n",
            "Batch Training Loss =  0.02429884858429432\n",
            "Batch Training Loss =  0.07958868891000748\n",
            "Batch Training Loss =  0.020689286291599274\n",
            "Batch Training Loss =  0.04358861967921257\n",
            "Batch Training Loss =  0.01325770653784275\n",
            "Batch Training Loss =  0.013299848884344101\n",
            "Batch Training Loss =  0.021404752507805824\n",
            "Batch Training Loss =  0.019791416823863983\n",
            "Batch Training Loss =  0.020642424002289772\n",
            "Batch Training Loss =  0.021612796932458878\n",
            "Batch Training Loss =  0.027996860444545746\n",
            "Batch Training Loss =  0.01777578517794609\n",
            "Batch Training Loss =  0.013781826943159103\n",
            "Batch Training Loss =  0.0313982218503952\n",
            "Batch Training Loss =  0.02558089792728424\n",
            "Batch Training Loss =  0.019343631342053413\n",
            "Batch Training Loss =  0.017043326050043106\n",
            "Batch Training Loss =  0.01924624852836132\n",
            "Batch Training Loss =  0.10858431458473206\n",
            "Batch Training Loss =  0.04300076141953468\n",
            "Batch Training Loss =  0.016276614740490913\n",
            "Batch Training Loss =  0.017729468643665314\n",
            "Batch Training Loss =  0.02890717424452305\n",
            "Batch Training Loss =  0.0210580937564373\n",
            "Batch Training Loss =  0.022867340594530106\n",
            "Validation Loss in this epoch is 0.416\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.016513116657733917\n",
            "Batch Training Loss =  0.017019009217619896\n",
            "Batch Training Loss =  0.012105012312531471\n",
            "Batch Training Loss =  0.02478564716875553\n",
            "Batch Training Loss =  0.015523050911724567\n",
            "Batch Training Loss =  0.027860749512910843\n",
            "Batch Training Loss =  0.016026658937335014\n",
            "Batch Training Loss =  0.013452667742967606\n",
            "Batch Training Loss =  0.027303077280521393\n",
            "Batch Training Loss =  0.0180180873721838\n",
            "Batch Training Loss =  0.010784303769469261\n",
            "Batch Training Loss =  0.11038357019424438\n",
            "Batch Training Loss =  0.01876033842563629\n",
            "Batch Training Loss =  0.043728992342948914\n",
            "Batch Training Loss =  0.07473307847976685\n",
            "Batch Training Loss =  0.03361786529421806\n",
            "Batch Training Loss =  0.020497120916843414\n",
            "Batch Training Loss =  0.03041371889412403\n",
            "Batch Training Loss =  0.015327337197959423\n",
            "Batch Training Loss =  0.061260923743247986\n",
            "Batch Training Loss =  0.03639128431677818\n",
            "Batch Training Loss =  0.03807946294546127\n",
            "Batch Training Loss =  0.021133821457624435\n",
            "Batch Training Loss =  0.02180192992091179\n",
            "Batch Training Loss =  0.02010134421288967\n",
            "Batch Training Loss =  0.023234812542796135\n",
            "Batch Training Loss =  0.019142011180520058\n",
            "Batch Training Loss =  0.020970335230231285\n",
            "Batch Training Loss =  0.011735975742340088\n",
            "Batch Training Loss =  0.017771685495972633\n",
            "Batch Training Loss =  0.012391253374516964\n",
            "Batch Training Loss =  0.021335544064641\n",
            "Validation Loss in this epoch is 0.394\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.015614037401974201\n",
            "Batch Training Loss =  0.00913198385387659\n",
            "Batch Training Loss =  0.016548098996281624\n",
            "Batch Training Loss =  0.0793539509177208\n",
            "Batch Training Loss =  0.07284530997276306\n",
            "Batch Training Loss =  0.04189290106296539\n",
            "Batch Training Loss =  0.01908096671104431\n",
            "Batch Training Loss =  0.02306164801120758\n",
            "Batch Training Loss =  0.02843145839869976\n",
            "Batch Training Loss =  0.021245421841740608\n",
            "Batch Training Loss =  0.013705207966268063\n",
            "Batch Training Loss =  0.01600371114909649\n",
            "Batch Training Loss =  0.013373284600675106\n",
            "Batch Training Loss =  0.019645914435386658\n",
            "Batch Training Loss =  0.023217005655169487\n",
            "Batch Training Loss =  0.014380156062543392\n",
            "Batch Training Loss =  0.01706528849899769\n",
            "Batch Training Loss =  0.009501315653324127\n",
            "Batch Training Loss =  0.011075195856392384\n",
            "Batch Training Loss =  0.016611207276582718\n",
            "Batch Training Loss =  0.00993744470179081\n",
            "Batch Training Loss =  0.12913602590560913\n",
            "Batch Training Loss =  0.04617780074477196\n",
            "Batch Training Loss =  0.06451387703418732\n",
            "Batch Training Loss =  0.016424717381596565\n",
            "Batch Training Loss =  0.018190961331129074\n",
            "Batch Training Loss =  0.021952755749225616\n",
            "Batch Training Loss =  0.01679418981075287\n",
            "Batch Training Loss =  0.029600976034998894\n",
            "Batch Training Loss =  0.053985822945833206\n",
            "Batch Training Loss =  0.021599072962999344\n",
            "Batch Training Loss =  0.02340729720890522\n",
            "Validation Loss in this epoch is 0.406\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.014467427507042885\n",
            "Batch Training Loss =  0.026840314269065857\n",
            "Batch Training Loss =  0.014843905344605446\n",
            "Batch Training Loss =  0.011706396006047726\n",
            "Batch Training Loss =  0.016766540706157684\n",
            "Batch Training Loss =  0.013847789727151394\n",
            "Batch Training Loss =  0.020920736715197563\n",
            "Batch Training Loss =  0.013559005223214626\n",
            "Batch Training Loss =  0.010522199794650078\n",
            "Batch Training Loss =  0.13120914995670319\n",
            "Batch Training Loss =  0.10908115655183792\n",
            "Batch Training Loss =  0.12156128138303757\n",
            "Batch Training Loss =  0.10429874062538147\n",
            "Batch Training Loss =  0.2548668384552002\n",
            "Batch Training Loss =  0.2587108910083771\n",
            "Batch Training Loss =  0.23818376660346985\n",
            "Batch Training Loss =  0.2783496081829071\n",
            "Batch Training Loss =  0.6044535040855408\n",
            "Batch Training Loss =  0.32786843180656433\n",
            "Batch Training Loss =  0.4700261652469635\n",
            "Batch Training Loss =  0.5534364581108093\n",
            "Batch Training Loss =  0.3723510801792145\n",
            "Batch Training Loss =  0.21647432446479797\n",
            "Batch Training Loss =  0.15015468001365662\n",
            "Batch Training Loss =  0.29046449065208435\n",
            "Batch Training Loss =  0.13592827320098877\n",
            "Batch Training Loss =  0.13955983519554138\n",
            "Batch Training Loss =  0.28265613317489624\n",
            "Batch Training Loss =  0.17079563438892365\n",
            "Batch Training Loss =  0.163691908121109\n",
            "Batch Training Loss =  0.10906371474266052\n",
            "Batch Training Loss =  0.07745338976383209\n",
            "Validation Loss in this epoch is 0.432\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.048037659376859665\n",
            "Batch Training Loss =  0.03898221254348755\n",
            "Batch Training Loss =  0.04351314902305603\n",
            "Batch Training Loss =  0.06905898451805115\n",
            "Batch Training Loss =  0.08686093240976334\n",
            "Batch Training Loss =  0.052022650837898254\n",
            "Batch Training Loss =  0.03185577318072319\n",
            "Batch Training Loss =  0.034878700971603394\n",
            "Batch Training Loss =  0.05119742080569267\n",
            "Batch Training Loss =  0.13466040790081024\n",
            "Batch Training Loss =  0.03455575555562973\n",
            "Batch Training Loss =  0.09018257260322571\n",
            "Batch Training Loss =  0.10244240611791611\n",
            "Batch Training Loss =  0.060211196541786194\n",
            "Batch Training Loss =  0.08833469450473785\n",
            "Batch Training Loss =  0.03632578253746033\n",
            "Batch Training Loss =  0.06945609301328659\n",
            "Batch Training Loss =  0.06435272097587585\n",
            "Batch Training Loss =  0.054389309138059616\n",
            "Batch Training Loss =  0.05946334823966026\n",
            "Batch Training Loss =  0.03567732870578766\n",
            "Batch Training Loss =  0.04762216657400131\n",
            "Batch Training Loss =  0.06519801914691925\n",
            "Batch Training Loss =  0.052628107368946075\n",
            "Batch Training Loss =  0.12522445619106293\n",
            "Batch Training Loss =  0.09409305453300476\n",
            "Batch Training Loss =  0.05940688028931618\n",
            "Batch Training Loss =  0.08139549940824509\n",
            "Batch Training Loss =  0.06250417977571487\n",
            "Batch Training Loss =  0.06301955133676529\n",
            "Batch Training Loss =  0.05183614790439606\n",
            "Batch Training Loss =  0.036703769117593765\n",
            "Validation Loss in this epoch is 0.397\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.021237526088953018\n",
            "Batch Training Loss =  0.022138485684990883\n",
            "Batch Training Loss =  0.020729046314954758\n",
            "Batch Training Loss =  0.028358405455946922\n",
            "Batch Training Loss =  0.026260068640112877\n",
            "Batch Training Loss =  0.04295739158987999\n",
            "Batch Training Loss =  0.09185614436864853\n",
            "Batch Training Loss =  0.08054035156965256\n",
            "Batch Training Loss =  0.02289443276822567\n",
            "Batch Training Loss =  0.016660798341035843\n",
            "Batch Training Loss =  0.029935119673609734\n",
            "Batch Training Loss =  0.017612727358937263\n",
            "Batch Training Loss =  0.011146851815283298\n",
            "Batch Training Loss =  0.05119727551937103\n",
            "Batch Training Loss =  0.019327033311128616\n",
            "Batch Training Loss =  0.023367702960968018\n",
            "Batch Training Loss =  0.03186957165598869\n",
            "Batch Training Loss =  0.03386088088154793\n",
            "Batch Training Loss =  0.028733883053064346\n",
            "Batch Training Loss =  0.02507626824080944\n",
            "Batch Training Loss =  0.01293889433145523\n",
            "Batch Training Loss =  0.01545817032456398\n",
            "Batch Training Loss =  0.10006741434335709\n",
            "Batch Training Loss =  0.04980038106441498\n",
            "Batch Training Loss =  0.017860418185591698\n",
            "Batch Training Loss =  0.02413252182304859\n",
            "Batch Training Loss =  0.02196936123073101\n",
            "Batch Training Loss =  0.015310447663068771\n",
            "Batch Training Loss =  0.02156786248087883\n",
            "Batch Training Loss =  0.02068471908569336\n",
            "Batch Training Loss =  0.01723860204219818\n",
            "Batch Training Loss =  0.014140729792416096\n",
            "Validation Loss in this epoch is 0.417\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.018289167433977127\n",
            "Batch Training Loss =  0.01655234396457672\n",
            "Batch Training Loss =  0.017400257289409637\n",
            "Batch Training Loss =  0.011104712262749672\n",
            "Batch Training Loss =  0.020548583939671516\n",
            "Batch Training Loss =  0.01753043755888939\n",
            "Batch Training Loss =  0.015287332236766815\n",
            "Batch Training Loss =  0.017985057085752487\n",
            "Batch Training Loss =  0.023948147892951965\n",
            "Batch Training Loss =  0.027526581659913063\n",
            "Batch Training Loss =  0.007006136234849691\n",
            "Batch Training Loss =  0.0156958419829607\n",
            "Batch Training Loss =  0.013680129311978817\n",
            "Batch Training Loss =  0.011939141899347305\n",
            "Batch Training Loss =  0.016710670664906502\n",
            "Batch Training Loss =  0.010729270987212658\n",
            "Batch Training Loss =  0.011442211456596851\n",
            "Batch Training Loss =  0.10223656892776489\n",
            "Batch Training Loss =  0.05870343744754791\n",
            "Batch Training Loss =  0.01954338327050209\n",
            "Batch Training Loss =  0.05159026011824608\n",
            "Batch Training Loss =  0.03082086332142353\n",
            "Batch Training Loss =  0.01920425333082676\n",
            "Batch Training Loss =  0.011866247281432152\n",
            "Batch Training Loss =  0.018919698894023895\n",
            "Batch Training Loss =  0.08667124062776566\n",
            "Batch Training Loss =  0.02613966539502144\n",
            "Batch Training Loss =  0.014403058215975761\n",
            "Batch Training Loss =  0.02495790272951126\n",
            "Batch Training Loss =  0.023048363626003265\n",
            "Batch Training Loss =  0.11209364980459213\n",
            "Batch Training Loss =  0.04909340292215347\n",
            "Validation Loss in this epoch is 0.407\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.017368940636515617\n",
            "Batch Training Loss =  0.013904770836234093\n",
            "Batch Training Loss =  0.021675297990441322\n",
            "Batch Training Loss =  0.02300167828798294\n",
            "Batch Training Loss =  0.011330300942063332\n",
            "Batch Training Loss =  0.08860345929861069\n",
            "Batch Training Loss =  0.04536359757184982\n",
            "Batch Training Loss =  0.02257598377764225\n",
            "Batch Training Loss =  0.02266152948141098\n",
            "Batch Training Loss =  0.01678840070962906\n",
            "Batch Training Loss =  0.014390111900866032\n",
            "Batch Training Loss =  0.013400821946561337\n",
            "Batch Training Loss =  0.017086928710341454\n",
            "Batch Training Loss =  0.02609138935804367\n",
            "Batch Training Loss =  0.05135228484869003\n",
            "Batch Training Loss =  0.03674043342471123\n",
            "Batch Training Loss =  0.01203730795532465\n",
            "Batch Training Loss =  0.020362336188554764\n",
            "Batch Training Loss =  0.0495930053293705\n",
            "Batch Training Loss =  0.016624631360173225\n",
            "Batch Training Loss =  0.021304694935679436\n",
            "Batch Training Loss =  0.01919153518974781\n",
            "Batch Training Loss =  0.014169399626553059\n",
            "Batch Training Loss =  0.01823088340461254\n",
            "Batch Training Loss =  0.014348810538649559\n",
            "Batch Training Loss =  0.02214634418487549\n",
            "Batch Training Loss =  0.007847138680517673\n",
            "Batch Training Loss =  0.06814052164554596\n",
            "Batch Training Loss =  0.02580861561000347\n",
            "Batch Training Loss =  0.017287038266658783\n",
            "Batch Training Loss =  0.017474699765443802\n",
            "Batch Training Loss =  0.018267953768372536\n",
            "Validation Loss in this epoch is 0.406\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.01460457593202591\n",
            "Batch Training Loss =  0.021965699270367622\n",
            "Batch Training Loss =  0.016344742849469185\n",
            "Batch Training Loss =  0.011723875999450684\n",
            "Batch Training Loss =  0.0038281804881989956\n",
            "Batch Training Loss =  0.07944916933774948\n",
            "Batch Training Loss =  0.047707680612802505\n",
            "Batch Training Loss =  0.020877577364444733\n",
            "Batch Training Loss =  0.02426254190504551\n",
            "Batch Training Loss =  0.009451658464968204\n",
            "Batch Training Loss =  0.020558197051286697\n",
            "Batch Training Loss =  0.011900492012500763\n",
            "Batch Training Loss =  0.01700481027364731\n",
            "Batch Training Loss =  0.0599028617143631\n",
            "Batch Training Loss =  0.015007917769253254\n",
            "Batch Training Loss =  0.03175462409853935\n",
            "Batch Training Loss =  0.01300267968326807\n",
            "Batch Training Loss =  0.022407840937376022\n",
            "Batch Training Loss =  0.010680883191525936\n",
            "Batch Training Loss =  0.013068138621747494\n",
            "Batch Training Loss =  0.022814488038420677\n",
            "Batch Training Loss =  0.011469636112451553\n",
            "Batch Training Loss =  0.008741416968405247\n",
            "Batch Training Loss =  0.011897105723619461\n",
            "Batch Training Loss =  0.05820946395397186\n",
            "Batch Training Loss =  0.01693035662174225\n",
            "Batch Training Loss =  0.01694190688431263\n",
            "Batch Training Loss =  0.025226661935448647\n",
            "Batch Training Loss =  0.022026987746357918\n",
            "Batch Training Loss =  0.026238257065415382\n",
            "Batch Training Loss =  0.04811564087867737\n",
            "Batch Training Loss =  0.020571572706103325\n",
            "Validation Loss in this epoch is 0.405\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.010799955576658249\n",
            "Batch Training Loss =  0.01133029069751501\n",
            "Batch Training Loss =  0.018025992438197136\n",
            "Batch Training Loss =  0.010548844002187252\n",
            "Batch Training Loss =  0.016572490334510803\n",
            "Batch Training Loss =  0.013972263783216476\n",
            "Batch Training Loss =  0.04144018143415451\n",
            "Batch Training Loss =  0.025270259007811546\n",
            "Batch Training Loss =  0.01647298038005829\n",
            "Batch Training Loss =  0.029727427288889885\n",
            "Batch Training Loss =  0.017066944390535355\n",
            "Batch Training Loss =  0.019777869805693626\n",
            "Batch Training Loss =  0.01687053218483925\n",
            "Batch Training Loss =  0.009436158463358879\n",
            "Batch Training Loss =  0.01542042475193739\n",
            "Batch Training Loss =  0.010820770636200905\n",
            "Batch Training Loss =  0.015061584301292896\n",
            "Batch Training Loss =  0.06569087505340576\n",
            "Batch Training Loss =  0.038793157786130905\n",
            "Batch Training Loss =  0.009661220014095306\n",
            "Batch Training Loss =  0.039313312619924545\n",
            "Batch Training Loss =  0.008733777329325676\n",
            "Batch Training Loss =  0.022524990141391754\n",
            "Batch Training Loss =  0.01588127203285694\n",
            "Batch Training Loss =  0.014783596619963646\n",
            "Batch Training Loss =  0.02051776647567749\n",
            "Batch Training Loss =  0.015443671494722366\n",
            "Batch Training Loss =  0.018180647864937782\n",
            "Batch Training Loss =  0.013755368068814278\n",
            "Batch Training Loss =  0.02188999392092228\n",
            "Batch Training Loss =  0.016640035435557365\n",
            "Batch Training Loss =  0.016005100682377815\n",
            "Validation Loss in this epoch is 0.396\n",
            "This is  26 th epoch\n",
            "Batch Training Loss =  0.0149673568084836\n",
            "Batch Training Loss =  0.009836479090154171\n",
            "Batch Training Loss =  0.012830999679863453\n",
            "Batch Training Loss =  0.009566242806613445\n",
            "Batch Training Loss =  0.007966458797454834\n",
            "Batch Training Loss =  0.011369092389941216\n",
            "Batch Training Loss =  0.008517831563949585\n",
            "Batch Training Loss =  0.011266697198152542\n",
            "Batch Training Loss =  0.009678456000983715\n",
            "Batch Training Loss =  0.01437431201338768\n",
            "Batch Training Loss =  0.0798090249300003\n",
            "Batch Training Loss =  0.023309512063860893\n",
            "Batch Training Loss =  0.04376085102558136\n",
            "Batch Training Loss =  0.022939186543226242\n",
            "Batch Training Loss =  0.013419530354440212\n",
            "Batch Training Loss =  0.020196789875626564\n",
            "Batch Training Loss =  0.013720279559493065\n",
            "Batch Training Loss =  0.014645546674728394\n",
            "Batch Training Loss =  0.010499793104827404\n",
            "Batch Training Loss =  0.014081936329603195\n",
            "Batch Training Loss =  0.014196909964084625\n",
            "Batch Training Loss =  0.00740696070715785\n",
            "Batch Training Loss =  0.010983116924762726\n",
            "Batch Training Loss =  0.010295438580214977\n",
            "Batch Training Loss =  0.016614289954304695\n",
            "Batch Training Loss =  0.013681636191904545\n",
            "Batch Training Loss =  0.011998839676380157\n",
            "Batch Training Loss =  0.012511055916547775\n",
            "Batch Training Loss =  0.0121257733553648\n",
            "Batch Training Loss =  0.0490940660238266\n",
            "Batch Training Loss =  0.040651898831129074\n",
            "Batch Training Loss =  0.047410059720277786\n",
            "Validation Loss in this epoch is 0.383\n",
            "This is  27 th epoch\n",
            "Batch Training Loss =  0.017098480835556984\n",
            "Batch Training Loss =  0.012903401628136635\n",
            "Batch Training Loss =  0.01650245487689972\n",
            "Batch Training Loss =  0.020762832835316658\n",
            "Batch Training Loss =  0.03812919184565544\n",
            "Batch Training Loss =  0.01896406151354313\n",
            "Batch Training Loss =  0.008037981577217579\n",
            "Batch Training Loss =  0.030792780220508575\n",
            "Batch Training Loss =  0.011686923913657665\n",
            "Batch Training Loss =  0.008656138554215431\n",
            "Batch Training Loss =  0.026354771107435226\n",
            "Batch Training Loss =  0.014201977290213108\n",
            "Batch Training Loss =  0.014651491306722164\n",
            "Batch Training Loss =  0.013663812540471554\n",
            "Batch Training Loss =  0.01936395838856697\n",
            "Batch Training Loss =  0.015012340620160103\n",
            "Batch Training Loss =  0.030253009870648384\n",
            "Batch Training Loss =  0.030411114916205406\n",
            "Batch Training Loss =  0.01565173640847206\n",
            "Batch Training Loss =  0.0148068368434906\n",
            "Batch Training Loss =  0.011128467507660389\n",
            "Batch Training Loss =  0.014938712120056152\n",
            "Batch Training Loss =  0.013382927514612675\n",
            "Batch Training Loss =  0.014146987348794937\n",
            "Batch Training Loss =  0.008776682429015636\n",
            "Batch Training Loss =  0.010964528657495975\n",
            "Batch Training Loss =  0.011990029364824295\n",
            "Batch Training Loss =  0.011967116966843605\n",
            "Batch Training Loss =  0.023006753996014595\n",
            "Batch Training Loss =  0.014645827934145927\n",
            "Batch Training Loss =  0.01917104423046112\n",
            "Batch Training Loss =  0.01107653696089983\n",
            "Validation Loss in this epoch is 0.423\n",
            "This is  28 th epoch\n",
            "Batch Training Loss =  0.010739065706729889\n",
            "Batch Training Loss =  0.010011148639023304\n",
            "Batch Training Loss =  0.07093014568090439\n",
            "Batch Training Loss =  0.09026101976633072\n",
            "Batch Training Loss =  0.1421460658311844\n",
            "Batch Training Loss =  0.22346948087215424\n",
            "Batch Training Loss =  1.998234510421753\n",
            "Batch Training Loss =  0.6584358811378479\n",
            "Batch Training Loss =  0.3586503565311432\n",
            "Batch Training Loss =  0.285575807094574\n",
            "Batch Training Loss =  0.1720050871372223\n",
            "Batch Training Loss =  0.1305178701877594\n",
            "Batch Training Loss =  0.08109035342931747\n",
            "Batch Training Loss =  0.09620659053325653\n",
            "Batch Training Loss =  0.1656240075826645\n",
            "Batch Training Loss =  0.10745909810066223\n",
            "Batch Training Loss =  0.1086139976978302\n",
            "Batch Training Loss =  0.12653905153274536\n",
            "Batch Training Loss =  0.06266230344772339\n",
            "Batch Training Loss =  0.16750024259090424\n",
            "Batch Training Loss =  0.09859616309404373\n",
            "Batch Training Loss =  0.14837510883808136\n",
            "Batch Training Loss =  0.042777374386787415\n",
            "Batch Training Loss =  0.05729245766997337\n",
            "Batch Training Loss =  0.09531181305646896\n",
            "Batch Training Loss =  0.05881854519248009\n",
            "Batch Training Loss =  0.2454620599746704\n",
            "Batch Training Loss =  0.35715538263320923\n",
            "Batch Training Loss =  0.3014768362045288\n",
            "Batch Training Loss =  0.19342561066150665\n",
            "Batch Training Loss =  0.2843055725097656\n",
            "Batch Training Loss =  0.36222702264785767\n",
            "Validation Loss in this epoch is 0.497\n",
            "This is  29 th epoch\n",
            "Batch Training Loss =  0.06178620830178261\n",
            "Batch Training Loss =  0.09836123138666153\n",
            "Batch Training Loss =  0.05183122679591179\n",
            "Batch Training Loss =  0.16382011771202087\n",
            "Batch Training Loss =  0.12028613686561584\n",
            "Batch Training Loss =  0.14420738816261292\n",
            "Batch Training Loss =  0.07994583994150162\n",
            "Batch Training Loss =  0.25934073328971863\n",
            "Batch Training Loss =  0.19788290560245514\n",
            "Batch Training Loss =  0.10584185272455215\n",
            "Batch Training Loss =  0.22081132233142853\n",
            "Batch Training Loss =  0.14031247794628143\n",
            "Batch Training Loss =  0.15896686911582947\n",
            "Batch Training Loss =  0.12471232563257217\n",
            "Batch Training Loss =  0.13082009553909302\n",
            "Batch Training Loss =  0.062290482223033905\n",
            "Batch Training Loss =  0.0758601576089859\n",
            "Batch Training Loss =  0.03336118534207344\n",
            "Batch Training Loss =  0.039500243961811066\n",
            "Batch Training Loss =  0.04070620238780975\n",
            "Batch Training Loss =  0.1226017102599144\n",
            "Batch Training Loss =  0.1650700718164444\n",
            "Batch Training Loss =  0.14868111908435822\n",
            "Batch Training Loss =  0.03774787485599518\n",
            "Batch Training Loss =  0.043870843946933746\n",
            "Batch Training Loss =  0.06575656682252884\n",
            "Batch Training Loss =  0.03885139524936676\n",
            "Batch Training Loss =  0.05885408818721771\n",
            "Batch Training Loss =  0.09460149705410004\n",
            "Batch Training Loss =  0.0534161701798439\n",
            "Batch Training Loss =  0.07116085290908813\n",
            "Batch Training Loss =  0.06244560703635216\n",
            "Validation Loss in this epoch is 0.423\n",
            "This is  30 th epoch\n",
            "Batch Training Loss =  0.026567714288830757\n",
            "Batch Training Loss =  0.031446151435375214\n",
            "Batch Training Loss =  0.11695394665002823\n",
            "Batch Training Loss =  0.06114022806286812\n",
            "Batch Training Loss =  0.020787570625543594\n",
            "Batch Training Loss =  0.029054412618279457\n",
            "Batch Training Loss =  0.0405706912279129\n",
            "Batch Training Loss =  0.02355681173503399\n",
            "Batch Training Loss =  0.018532373011112213\n",
            "Batch Training Loss =  0.03097597323358059\n",
            "Batch Training Loss =  0.025044631212949753\n",
            "Batch Training Loss =  0.017909754067659378\n",
            "Batch Training Loss =  0.026978086680173874\n",
            "Batch Training Loss =  0.04463179409503937\n",
            "Batch Training Loss =  0.1214635968208313\n",
            "Batch Training Loss =  0.0722724124789238\n",
            "Batch Training Loss =  0.031985439360141754\n",
            "Batch Training Loss =  0.03177770599722862\n",
            "Batch Training Loss =  0.02798011526465416\n",
            "Batch Training Loss =  0.035259220749139786\n",
            "Batch Training Loss =  0.019204488024115562\n",
            "Batch Training Loss =  0.030628222972154617\n",
            "Batch Training Loss =  0.021027004346251488\n",
            "Batch Training Loss =  0.02321959100663662\n",
            "Batch Training Loss =  0.022605549544095993\n",
            "Batch Training Loss =  0.08024243265390396\n",
            "Batch Training Loss =  0.035777173936367035\n",
            "Batch Training Loss =  0.03148574009537697\n",
            "Batch Training Loss =  0.028499415144324303\n",
            "Batch Training Loss =  0.019358331337571144\n",
            "Batch Training Loss =  0.015641789883375168\n",
            "Batch Training Loss =  0.04032806307077408\n",
            "Validation Loss in this epoch is 0.449\n",
            "This is  31 th epoch\n",
            "Batch Training Loss =  0.012555379420518875\n",
            "Batch Training Loss =  0.021219560876488686\n",
            "Batch Training Loss =  0.01616867259144783\n",
            "Batch Training Loss =  0.016017643734812737\n",
            "Batch Training Loss =  0.01298550982028246\n",
            "Batch Training Loss =  0.06437759846448898\n",
            "Batch Training Loss =  0.027775630354881287\n",
            "Batch Training Loss =  0.02973952703177929\n",
            "Batch Training Loss =  0.01711571030318737\n",
            "Batch Training Loss =  0.022231226786971092\n",
            "Batch Training Loss =  0.01040203683078289\n",
            "Batch Training Loss =  0.019714659079909325\n",
            "Batch Training Loss =  0.01456581149250269\n",
            "Batch Training Loss =  0.023821990936994553\n",
            "Batch Training Loss =  0.012145345099270344\n",
            "Batch Training Loss =  0.016719764098525047\n",
            "Batch Training Loss =  0.010554381646215916\n",
            "Batch Training Loss =  0.012792370282113552\n",
            "Batch Training Loss =  0.09590556472539902\n",
            "Batch Training Loss =  0.04672985523939133\n",
            "Batch Training Loss =  0.05443829670548439\n",
            "Batch Training Loss =  0.02294866368174553\n",
            "Batch Training Loss =  0.02528439462184906\n",
            "Batch Training Loss =  0.016830705106258392\n",
            "Batch Training Loss =  0.052127305418252945\n",
            "Batch Training Loss =  0.01967327669262886\n",
            "Batch Training Loss =  0.02664337307214737\n",
            "Batch Training Loss =  0.01578347012400627\n",
            "Batch Training Loss =  0.027440672740340233\n",
            "Batch Training Loss =  0.02210768684744835\n",
            "Batch Training Loss =  0.016086291521787643\n",
            "Batch Training Loss =  0.012257469817996025\n",
            "Validation Loss in this epoch is 0.440\n",
            "This is  32 th epoch\n",
            "Batch Training Loss =  0.06916969269514084\n",
            "Batch Training Loss =  0.013895265758037567\n",
            "Batch Training Loss =  0.01750035583972931\n",
            "Batch Training Loss =  0.014906061813235283\n",
            "Batch Training Loss =  0.017346112057566643\n",
            "Batch Training Loss =  0.062134552747011185\n",
            "Batch Training Loss =  0.02425428479909897\n",
            "Batch Training Loss =  0.018495038151741028\n",
            "Batch Training Loss =  0.02406938746571541\n",
            "Batch Training Loss =  0.03370492160320282\n",
            "Batch Training Loss =  0.031563613563776016\n",
            "Batch Training Loss =  0.013316012918949127\n",
            "Batch Training Loss =  0.014019196853041649\n",
            "Batch Training Loss =  0.022757044062018394\n",
            "Batch Training Loss =  0.01637769490480423\n",
            "Batch Training Loss =  0.012046385556459427\n",
            "Batch Training Loss =  0.011329147033393383\n",
            "Batch Training Loss =  0.017993055284023285\n",
            "Batch Training Loss =  0.013950168155133724\n",
            "Batch Training Loss =  0.018084215000271797\n",
            "Batch Training Loss =  0.019659865647554398\n",
            "Batch Training Loss =  0.014558200724422932\n",
            "Batch Training Loss =  0.014308907091617584\n",
            "Batch Training Loss =  0.015778593719005585\n",
            "Batch Training Loss =  0.009415791369974613\n",
            "Batch Training Loss =  0.015386148355901241\n",
            "Batch Training Loss =  0.010036221705377102\n",
            "Batch Training Loss =  0.0147627554833889\n",
            "Batch Training Loss =  0.016276368871331215\n",
            "Batch Training Loss =  0.009908166714012623\n",
            "Batch Training Loss =  0.017352929338812828\n",
            "Batch Training Loss =  0.019035689532756805\n",
            "Validation Loss in this epoch is 0.430\n",
            "This is  33 th epoch\n",
            "Batch Training Loss =  0.013717605732381344\n",
            "Batch Training Loss =  0.005894522648304701\n",
            "Batch Training Loss =  0.011814761906862259\n",
            "Batch Training Loss =  0.014749276451766491\n",
            "Batch Training Loss =  0.01091182604432106\n",
            "Batch Training Loss =  0.01135957706719637\n",
            "Batch Training Loss =  0.010829505510628223\n",
            "Batch Training Loss =  0.01001310721039772\n",
            "Batch Training Loss =  0.01325378380715847\n",
            "Batch Training Loss =  0.09334434568881989\n",
            "Batch Training Loss =  0.034959644079208374\n",
            "Batch Training Loss =  0.03192663937807083\n",
            "Batch Training Loss =  0.03644953668117523\n",
            "Batch Training Loss =  0.020655745640397072\n",
            "Batch Training Loss =  0.01833909936249256\n",
            "Batch Training Loss =  0.02161869965493679\n",
            "Batch Training Loss =  0.017198830842971802\n",
            "Batch Training Loss =  0.011871044524013996\n",
            "Batch Training Loss =  0.011902105994522572\n",
            "Batch Training Loss =  0.018370967358350754\n",
            "Batch Training Loss =  0.01776217669248581\n",
            "Batch Training Loss =  0.014742590487003326\n",
            "Batch Training Loss =  0.0672021359205246\n",
            "Batch Training Loss =  0.020520858466625214\n",
            "Batch Training Loss =  0.030775053426623344\n",
            "Batch Training Loss =  0.015843506902456284\n",
            "Batch Training Loss =  0.028120161965489388\n",
            "Batch Training Loss =  0.017326507717370987\n",
            "Batch Training Loss =  0.011571783572435379\n",
            "Batch Training Loss =  0.017268864437937737\n",
            "Batch Training Loss =  0.017432905733585358\n",
            "Batch Training Loss =  0.014867769554257393\n",
            "Validation Loss in this epoch is 0.428\n",
            "This is  34 th epoch\n",
            "Batch Training Loss =  0.009925831109285355\n",
            "Batch Training Loss =  0.005320748779922724\n",
            "Batch Training Loss =  0.043051913380622864\n",
            "Batch Training Loss =  0.018900711089372635\n",
            "Batch Training Loss =  0.014038541354238987\n",
            "Batch Training Loss =  0.01534559391438961\n",
            "Batch Training Loss =  0.01593632437288761\n",
            "Batch Training Loss =  0.0101900864392519\n",
            "Batch Training Loss =  0.01615208573639393\n",
            "Batch Training Loss =  0.00945654883980751\n",
            "Batch Training Loss =  0.012920779176056385\n",
            "Batch Training Loss =  0.00849659088999033\n",
            "Batch Training Loss =  0.014714682474732399\n",
            "Batch Training Loss =  0.010316419415175915\n",
            "Batch Training Loss =  0.011348376050591469\n",
            "Batch Training Loss =  0.012898280285298824\n",
            "Batch Training Loss =  0.011526587419211864\n",
            "Batch Training Loss =  0.013460754416882992\n",
            "Batch Training Loss =  0.00936458446085453\n",
            "Batch Training Loss =  0.06342533975839615\n",
            "Batch Training Loss =  0.04752955585718155\n",
            "Batch Training Loss =  0.018413668498396873\n",
            "Batch Training Loss =  0.013090964406728745\n",
            "Batch Training Loss =  0.02801223285496235\n",
            "Batch Training Loss =  0.017239343374967575\n",
            "Batch Training Loss =  0.016090743243694305\n",
            "Batch Training Loss =  0.01657583750784397\n",
            "Batch Training Loss =  0.021744446828961372\n",
            "Batch Training Loss =  0.010955325327813625\n",
            "Batch Training Loss =  0.07133681327104568\n",
            "Batch Training Loss =  0.042021024972200394\n",
            "Batch Training Loss =  0.015522051602602005\n",
            "Validation Loss in this epoch is 0.440\n",
            "This is  35 th epoch\n",
            "Batch Training Loss =  0.01832667551934719\n",
            "Batch Training Loss =  0.01954886130988598\n",
            "Batch Training Loss =  0.01366174966096878\n",
            "Batch Training Loss =  0.018897127360105515\n",
            "Batch Training Loss =  0.01444101333618164\n",
            "Batch Training Loss =  0.04014377295970917\n",
            "Batch Training Loss =  0.015290804207324982\n",
            "Batch Training Loss =  0.01375418622046709\n",
            "Batch Training Loss =  0.013470347039401531\n",
            "Batch Training Loss =  0.008857803419232368\n",
            "Batch Training Loss =  0.013438913971185684\n",
            "Batch Training Loss =  0.0175222959369421\n",
            "Batch Training Loss =  0.008528060279786587\n",
            "Batch Training Loss =  0.013083205558359623\n",
            "Batch Training Loss =  0.03533894196152687\n",
            "Batch Training Loss =  0.01988884247839451\n",
            "Batch Training Loss =  0.0177378561347723\n",
            "Batch Training Loss =  0.015550018288195133\n",
            "Batch Training Loss =  0.014202828519046307\n",
            "Batch Training Loss =  0.007814213633537292\n",
            "Batch Training Loss =  0.014117883518338203\n",
            "Batch Training Loss =  0.012048722244799137\n",
            "Batch Training Loss =  0.01323666051030159\n",
            "Batch Training Loss =  0.04224064201116562\n",
            "Batch Training Loss =  0.02119193971157074\n",
            "Batch Training Loss =  0.017080064862966537\n",
            "Batch Training Loss =  0.012109499424695969\n",
            "Batch Training Loss =  0.0396798811852932\n",
            "Batch Training Loss =  0.0106141222640872\n",
            "Batch Training Loss =  0.012857727706432343\n",
            "Batch Training Loss =  0.02564235031604767\n",
            "Batch Training Loss =  0.00902349129319191\n",
            "Validation Loss in this epoch is 0.433\n",
            "This is  36 th epoch\n",
            "Batch Training Loss =  0.011364391073584557\n",
            "Batch Training Loss =  0.01557882409542799\n",
            "Batch Training Loss =  0.014881687238812447\n",
            "Batch Training Loss =  0.0105954734608531\n",
            "Batch Training Loss =  0.010029690340161324\n",
            "Batch Training Loss =  0.011436880566179752\n",
            "Batch Training Loss =  0.012513804249465466\n",
            "Batch Training Loss =  0.01152007095515728\n",
            "Batch Training Loss =  0.011143194511532784\n",
            "Batch Training Loss =  0.010154826566576958\n",
            "Batch Training Loss =  0.009847166948020458\n",
            "Batch Training Loss =  0.0747966393828392\n",
            "Batch Training Loss =  0.030021868646144867\n",
            "Batch Training Loss =  0.013299533165991306\n",
            "Batch Training Loss =  0.02461254596710205\n",
            "Batch Training Loss =  0.014209390617907047\n",
            "Batch Training Loss =  0.04596424102783203\n",
            "Batch Training Loss =  0.017478613182902336\n",
            "Batch Training Loss =  0.015713825821876526\n",
            "Batch Training Loss =  0.015901874750852585\n",
            "Batch Training Loss =  0.015768181532621384\n",
            "Batch Training Loss =  0.028666771948337555\n",
            "Batch Training Loss =  0.016551129519939423\n",
            "Batch Training Loss =  0.007565172854810953\n",
            "Batch Training Loss =  0.016075532883405685\n",
            "Batch Training Loss =  0.011686855927109718\n",
            "Batch Training Loss =  0.010011369362473488\n",
            "Batch Training Loss =  0.011912290006875992\n",
            "Batch Training Loss =  0.0244870875030756\n",
            "Batch Training Loss =  0.007077902555465698\n",
            "Batch Training Loss =  0.013636591844260693\n",
            "Batch Training Loss =  0.0462907999753952\n",
            "Validation Loss in this epoch is 0.438\n",
            "This is  37 th epoch\n",
            "Batch Training Loss =  0.03983733057975769\n",
            "Batch Training Loss =  0.0166269913315773\n",
            "Batch Training Loss =  0.012166651897132397\n",
            "Batch Training Loss =  0.011992564424872398\n",
            "Batch Training Loss =  0.013771213591098785\n",
            "Batch Training Loss =  0.010438735596835613\n",
            "Batch Training Loss =  0.014433329924941063\n",
            "Batch Training Loss =  0.023205235600471497\n",
            "Batch Training Loss =  0.01373632438480854\n",
            "Batch Training Loss =  0.017977150157094002\n",
            "Batch Training Loss =  0.025877680629491806\n",
            "Batch Training Loss =  0.016945045441389084\n",
            "Batch Training Loss =  0.0183973740786314\n",
            "Batch Training Loss =  0.017027202993631363\n",
            "Batch Training Loss =  0.012463114224374294\n",
            "Batch Training Loss =  0.01597176492214203\n",
            "Batch Training Loss =  0.015887586399912834\n",
            "Batch Training Loss =  0.014230595901608467\n",
            "Batch Training Loss =  0.012615544721484184\n",
            "Batch Training Loss =  0.006433748174458742\n",
            "Batch Training Loss =  0.014710857532918453\n",
            "Batch Training Loss =  0.009195495396852493\n",
            "Batch Training Loss =  0.019034000113606453\n",
            "Batch Training Loss =  0.012194509617984295\n",
            "Batch Training Loss =  0.010529893450438976\n",
            "Batch Training Loss =  0.017665885388851166\n",
            "Batch Training Loss =  0.00914995651692152\n",
            "Batch Training Loss =  0.008027075789868832\n",
            "Batch Training Loss =  0.011492129415273666\n",
            "Batch Training Loss =  0.016349514946341515\n",
            "Batch Training Loss =  0.08373526483774185\n",
            "Batch Training Loss =  0.08985237032175064\n",
            "Validation Loss in this epoch is 0.875\n",
            "This is  38 th epoch\n",
            "Batch Training Loss =  0.07333917915821075\n",
            "Batch Training Loss =  0.27498331665992737\n",
            "Batch Training Loss =  0.47327300906181335\n",
            "Batch Training Loss =  0.10044875741004944\n",
            "Batch Training Loss =  0.11768610030412674\n",
            "Batch Training Loss =  0.05961572751402855\n",
            "Batch Training Loss =  0.04221978038549423\n",
            "Batch Training Loss =  0.08292093127965927\n",
            "Batch Training Loss =  0.09858009219169617\n",
            "Batch Training Loss =  0.06524456292390823\n",
            "Batch Training Loss =  0.08769817650318146\n",
            "Batch Training Loss =  0.049495719373226166\n",
            "Batch Training Loss =  0.03731328248977661\n",
            "Batch Training Loss =  0.05942336469888687\n",
            "Batch Training Loss =  0.02716604620218277\n",
            "Batch Training Loss =  0.030333150178194046\n",
            "Batch Training Loss =  0.040671080350875854\n",
            "Batch Training Loss =  0.02309396304190159\n",
            "Batch Training Loss =  0.02716016210615635\n",
            "Batch Training Loss =  0.032346561551094055\n",
            "Batch Training Loss =  0.01873459666967392\n",
            "Batch Training Loss =  0.01779855042695999\n",
            "Batch Training Loss =  0.028252623975276947\n",
            "Batch Training Loss =  0.01890786550939083\n",
            "Batch Training Loss =  0.037878986448049545\n",
            "Batch Training Loss =  0.024020211771130562\n",
            "Batch Training Loss =  0.014124366454780102\n",
            "Batch Training Loss =  0.06496947258710861\n",
            "Batch Training Loss =  0.023151077330112457\n",
            "Batch Training Loss =  0.026046335697174072\n",
            "Batch Training Loss =  0.034542206674814224\n",
            "Batch Training Loss =  0.044359512627124786\n",
            "Validation Loss in this epoch is 0.415\n",
            "This is  39 th epoch\n",
            "Batch Training Loss =  0.015662843361496925\n",
            "Batch Training Loss =  0.016512248665094376\n",
            "Batch Training Loss =  0.012888943776488304\n",
            "Batch Training Loss =  0.012116431258618832\n",
            "Batch Training Loss =  0.010174518451094627\n",
            "Batch Training Loss =  0.018908130005002022\n",
            "Batch Training Loss =  0.012640904635190964\n",
            "Batch Training Loss =  0.01779291220009327\n",
            "Batch Training Loss =  0.01266357209533453\n",
            "Batch Training Loss =  0.013299188576638699\n",
            "Batch Training Loss =  0.017081212252378464\n",
            "Batch Training Loss =  0.01280581671744585\n",
            "Batch Training Loss =  0.022819682955741882\n",
            "Batch Training Loss =  0.015545954927802086\n",
            "Batch Training Loss =  0.010532078333199024\n",
            "Batch Training Loss =  0.01596677675843239\n",
            "Batch Training Loss =  0.012378846295177937\n",
            "Batch Training Loss =  0.012547238729894161\n",
            "Batch Training Loss =  0.007838437333703041\n",
            "Batch Training Loss =  0.013011166825890541\n",
            "Batch Training Loss =  0.01210275012999773\n",
            "Batch Training Loss =  0.012806151993572712\n",
            "Batch Training Loss =  0.07866811007261276\n",
            "Batch Training Loss =  0.022302856668829918\n",
            "Batch Training Loss =  0.011666259728372097\n",
            "Batch Training Loss =  0.020788192749023438\n",
            "Batch Training Loss =  0.051433876156806946\n",
            "Batch Training Loss =  0.028958072885870934\n",
            "Batch Training Loss =  0.01674533076584339\n",
            "Batch Training Loss =  0.013850338757038116\n",
            "Batch Training Loss =  0.03245139494538307\n",
            "Batch Training Loss =  0.013158443383872509\n",
            "Validation Loss in this epoch is 0.419\n",
            "This is  40 th epoch\n",
            "Batch Training Loss =  0.012917560525238514\n",
            "Batch Training Loss =  0.009168382734060287\n",
            "Batch Training Loss =  0.01105168554931879\n",
            "Batch Training Loss =  0.010913848876953125\n",
            "Batch Training Loss =  0.012158242054283619\n",
            "Batch Training Loss =  0.013696500100195408\n",
            "Batch Training Loss =  0.00870566163212061\n",
            "Batch Training Loss =  0.013708341866731644\n",
            "Batch Training Loss =  0.009039736352860928\n",
            "Batch Training Loss =  0.055667586624622345\n",
            "Batch Training Loss =  0.03090125508606434\n",
            "Batch Training Loss =  0.018128124997019768\n",
            "Batch Training Loss =  0.01381681952625513\n",
            "Batch Training Loss =  0.015983708202838898\n",
            "Batch Training Loss =  0.01823701336979866\n",
            "Batch Training Loss =  0.007616669405251741\n",
            "Batch Training Loss =  0.010187327861785889\n",
            "Batch Training Loss =  0.010561268776655197\n",
            "Batch Training Loss =  0.014849208295345306\n",
            "Batch Training Loss =  0.011421611532568932\n",
            "Batch Training Loss =  0.023330537602305412\n",
            "Batch Training Loss =  0.012453910894691944\n",
            "Batch Training Loss =  0.06046760454773903\n",
            "Batch Training Loss =  0.028564395383000374\n",
            "Batch Training Loss =  0.020673196762800217\n",
            "Batch Training Loss =  0.017885271459817886\n",
            "Batch Training Loss =  0.012936079874634743\n",
            "Batch Training Loss =  0.010730602778494358\n",
            "Batch Training Loss =  0.030634362250566483\n",
            "Batch Training Loss =  0.012846339493989944\n",
            "Batch Training Loss =  0.011355050839483738\n",
            "Batch Training Loss =  0.01148931309580803\n",
            "Validation Loss in this epoch is 0.411\n",
            "This is  41 th epoch\n",
            "Batch Training Loss =  0.013014421798288822\n",
            "Batch Training Loss =  0.015938373282551765\n",
            "Batch Training Loss =  0.008018013089895248\n",
            "Batch Training Loss =  0.010325891897082329\n",
            "Batch Training Loss =  0.01194033119827509\n",
            "Batch Training Loss =  0.013468492776155472\n",
            "Batch Training Loss =  0.007880067452788353\n",
            "Batch Training Loss =  0.02070070430636406\n",
            "Batch Training Loss =  0.012050658464431763\n",
            "Batch Training Loss =  0.009050964377820492\n",
            "Batch Training Loss =  0.009667843580245972\n",
            "Batch Training Loss =  0.05888250470161438\n",
            "Batch Training Loss =  0.02018546871840954\n",
            "Batch Training Loss =  0.030073711648583412\n",
            "Batch Training Loss =  0.03644835948944092\n",
            "Batch Training Loss =  0.013698357157409191\n",
            "Batch Training Loss =  0.016171371564269066\n",
            "Batch Training Loss =  0.009210359305143356\n",
            "Batch Training Loss =  0.011189349927008152\n",
            "Batch Training Loss =  0.009451988153159618\n",
            "Batch Training Loss =  0.013756642118096352\n",
            "Batch Training Loss =  0.013494708575308323\n",
            "Batch Training Loss =  0.011543934233486652\n",
            "Batch Training Loss =  0.010251150466501713\n",
            "Batch Training Loss =  0.006968725938349962\n",
            "Batch Training Loss =  0.010047651827335358\n",
            "Batch Training Loss =  0.045922908931970596\n",
            "Batch Training Loss =  0.019348490983247757\n",
            "Batch Training Loss =  0.009345789439976215\n",
            "Batch Training Loss =  0.014920872636139393\n",
            "Batch Training Loss =  0.020980536937713623\n",
            "Batch Training Loss =  0.020067447796463966\n",
            "Validation Loss in this epoch is 0.408\n",
            "This is  42 th epoch\n",
            "Batch Training Loss =  0.011758275330066681\n",
            "Batch Training Loss =  0.011994626373052597\n",
            "Batch Training Loss =  0.0160381980240345\n",
            "Batch Training Loss =  0.012434948235750198\n",
            "Batch Training Loss =  0.010018174536526203\n",
            "Batch Training Loss =  0.016431160271167755\n",
            "Batch Training Loss =  0.030941061675548553\n",
            "Batch Training Loss =  0.013758578337728977\n",
            "Batch Training Loss =  0.019191905856132507\n",
            "Batch Training Loss =  0.00926532968878746\n",
            "Batch Training Loss =  0.015957998111844063\n",
            "Batch Training Loss =  0.008584319613873959\n",
            "Batch Training Loss =  0.013419064693152905\n",
            "Batch Training Loss =  0.013470438309013844\n",
            "Batch Training Loss =  0.011414161883294582\n",
            "Batch Training Loss =  0.012471658177673817\n",
            "Batch Training Loss =  0.009585251100361347\n",
            "Batch Training Loss =  0.015122350305318832\n",
            "Batch Training Loss =  0.013895723968744278\n",
            "Batch Training Loss =  0.04375241696834564\n",
            "Batch Training Loss =  0.01928909309208393\n",
            "Batch Training Loss =  0.019104618579149246\n",
            "Batch Training Loss =  0.015554064884781837\n",
            "Batch Training Loss =  0.020590104162693024\n",
            "Batch Training Loss =  0.012808108702301979\n",
            "Batch Training Loss =  0.00930499006062746\n",
            "Batch Training Loss =  0.014167248271405697\n",
            "Batch Training Loss =  0.032153524458408356\n",
            "Batch Training Loss =  0.03315743803977966\n",
            "Batch Training Loss =  0.017727909609675407\n",
            "Batch Training Loss =  0.020407645031809807\n",
            "Batch Training Loss =  0.012354030273854733\n",
            "Validation Loss in this epoch is 0.417\n",
            "This is  43 th epoch\n",
            "Batch Training Loss =  0.010650921612977982\n",
            "Batch Training Loss =  0.012693618424236774\n",
            "Batch Training Loss =  0.011693604290485382\n",
            "Batch Training Loss =  0.03357608988881111\n",
            "Batch Training Loss =  0.008989931084215641\n",
            "Batch Training Loss =  0.013004669919610023\n",
            "Batch Training Loss =  0.023444516584277153\n",
            "Batch Training Loss =  0.012435220181941986\n",
            "Batch Training Loss =  0.024756671860814095\n",
            "Batch Training Loss =  0.014007057063281536\n",
            "Batch Training Loss =  0.02284092828631401\n",
            "Batch Training Loss =  0.009520458988845348\n",
            "Batch Training Loss =  0.012248754501342773\n",
            "Batch Training Loss =  0.009087538346648216\n",
            "Batch Training Loss =  0.011066961102187634\n",
            "Batch Training Loss =  0.010687190108001232\n",
            "Batch Training Loss =  0.010781308636069298\n",
            "Batch Training Loss =  0.008702710270881653\n",
            "Batch Training Loss =  0.014192869886755943\n",
            "Batch Training Loss =  0.009645188227295876\n",
            "Batch Training Loss =  0.014426146633923054\n",
            "Batch Training Loss =  0.011339064687490463\n",
            "Batch Training Loss =  0.01192433014512062\n",
            "Batch Training Loss =  0.0122790252789855\n",
            "Batch Training Loss =  0.011410360224545002\n",
            "Batch Training Loss =  0.013746813870966434\n",
            "Batch Training Loss =  0.04991189017891884\n",
            "Batch Training Loss =  0.013581040315330029\n",
            "Batch Training Loss =  0.019316911697387695\n",
            "Batch Training Loss =  0.01139851938933134\n",
            "Batch Training Loss =  0.013309299945831299\n",
            "Batch Training Loss =  0.013498149812221527\n",
            "Validation Loss in this epoch is 0.425\n",
            "This is  44 th epoch\n",
            "Batch Training Loss =  0.010801388882100582\n",
            "Batch Training Loss =  0.021104324609041214\n",
            "Batch Training Loss =  0.04090648517012596\n",
            "Batch Training Loss =  0.017488202080130577\n",
            "Batch Training Loss =  0.021242382004857063\n",
            "Batch Training Loss =  0.013508714735507965\n",
            "Batch Training Loss =  0.011866197921335697\n",
            "Batch Training Loss =  0.014891090802848339\n",
            "Batch Training Loss =  0.011635136790573597\n",
            "Batch Training Loss =  0.013047056272625923\n",
            "Batch Training Loss =  0.015262406319379807\n",
            "Batch Training Loss =  0.009956734254956245\n",
            "Batch Training Loss =  0.011207702569663525\n",
            "Batch Training Loss =  0.012429079972207546\n",
            "Batch Training Loss =  0.0150568513199687\n",
            "Batch Training Loss =  0.011413726024329662\n",
            "Batch Training Loss =  0.042774099856615067\n",
            "Batch Training Loss =  0.014191264286637306\n",
            "Batch Training Loss =  0.011828883551061153\n",
            "Batch Training Loss =  0.02389460802078247\n",
            "Batch Training Loss =  0.009344643913209438\n",
            "Batch Training Loss =  0.014469548128545284\n",
            "Batch Training Loss =  0.00991786178201437\n",
            "Batch Training Loss =  0.010951642878353596\n",
            "Batch Training Loss =  0.014046174474060535\n",
            "Batch Training Loss =  0.011942828074097633\n",
            "Batch Training Loss =  0.03797944635152817\n",
            "Batch Training Loss =  0.014780240133404732\n",
            "Batch Training Loss =  0.01618090644478798\n",
            "Batch Training Loss =  0.015340174548327923\n",
            "Batch Training Loss =  0.011191117577254772\n",
            "Batch Training Loss =  0.02526523359119892\n",
            "Validation Loss in this epoch is 0.406\n",
            "This is  45 th epoch\n",
            "Batch Training Loss =  0.01818431168794632\n",
            "Batch Training Loss =  0.013846120797097683\n",
            "Batch Training Loss =  0.010314171202480793\n",
            "Batch Training Loss =  0.00970503967255354\n",
            "Batch Training Loss =  0.01248926017433405\n",
            "Batch Training Loss =  0.011143434792757034\n",
            "Batch Training Loss =  0.00963291060179472\n",
            "Batch Training Loss =  0.023245133459568024\n",
            "Batch Training Loss =  0.020968055352568626\n",
            "Batch Training Loss =  0.009954849258065224\n",
            "Batch Training Loss =  0.010404061526060104\n",
            "Batch Training Loss =  0.011134759522974491\n",
            "Batch Training Loss =  0.010071123018860817\n",
            "Batch Training Loss =  0.015002500265836716\n",
            "Batch Training Loss =  0.018126169219613075\n",
            "Batch Training Loss =  0.006994887720793486\n",
            "Batch Training Loss =  0.012703098356723785\n",
            "Batch Training Loss =  0.011051131412386894\n",
            "Batch Training Loss =  0.012122389860451221\n",
            "Batch Training Loss =  0.009634960442781448\n",
            "Batch Training Loss =  0.013551274314522743\n",
            "Batch Training Loss =  0.01484057679772377\n",
            "Batch Training Loss =  0.010421358048915863\n",
            "Batch Training Loss =  0.010038938373327255\n",
            "Batch Training Loss =  0.01096062920987606\n",
            "Batch Training Loss =  0.040103472769260406\n",
            "Batch Training Loss =  0.02033103257417679\n",
            "Batch Training Loss =  0.01857517473399639\n",
            "Batch Training Loss =  0.014520185068249702\n",
            "Batch Training Loss =  0.015222122892737389\n",
            "Batch Training Loss =  0.043177586048841476\n",
            "Batch Training Loss =  0.025866108015179634\n",
            "Validation Loss in this epoch is 0.414\n",
            "This is  46 th epoch\n",
            "Batch Training Loss =  0.011254541575908661\n",
            "Batch Training Loss =  0.019443994387984276\n",
            "Batch Training Loss =  0.011995404958724976\n",
            "Batch Training Loss =  0.012159842066466808\n",
            "Batch Training Loss =  0.03310121223330498\n",
            "Batch Training Loss =  0.009728960692882538\n",
            "Batch Training Loss =  0.014081486500799656\n",
            "Batch Training Loss =  0.014234431087970734\n",
            "Batch Training Loss =  0.01168905757367611\n",
            "Batch Training Loss =  0.017055975273251534\n",
            "Batch Training Loss =  0.009583988226950169\n",
            "Batch Training Loss =  0.017704343423247337\n",
            "Batch Training Loss =  0.03568006679415703\n",
            "Batch Training Loss =  0.02309059165418148\n",
            "Batch Training Loss =  0.011944346129894257\n",
            "Batch Training Loss =  0.013903539627790451\n",
            "Batch Training Loss =  0.013360368087887764\n",
            "Batch Training Loss =  0.01731017976999283\n",
            "Batch Training Loss =  0.01578596979379654\n",
            "Batch Training Loss =  0.010098135098814964\n",
            "Batch Training Loss =  0.011512236669659615\n",
            "Batch Training Loss =  0.011400001123547554\n",
            "Batch Training Loss =  0.014276614412665367\n",
            "Batch Training Loss =  0.04152689501643181\n",
            "Batch Training Loss =  0.021285319700837135\n",
            "Batch Training Loss =  0.02207285352051258\n",
            "Batch Training Loss =  0.013796997256577015\n",
            "Batch Training Loss =  0.01476354245096445\n",
            "Batch Training Loss =  0.02658175490796566\n",
            "Batch Training Loss =  0.01572462171316147\n",
            "Batch Training Loss =  0.014466005377471447\n",
            "Batch Training Loss =  0.0104179373010993\n",
            "Validation Loss in this epoch is 0.412\n",
            "This is  47 th epoch\n",
            "Batch Training Loss =  0.012792758643627167\n",
            "Batch Training Loss =  0.012005049735307693\n",
            "Batch Training Loss =  0.011838389560580254\n",
            "Batch Training Loss =  0.02033165656030178\n",
            "Batch Training Loss =  0.008561464957892895\n",
            "Batch Training Loss =  0.012764899991452694\n",
            "Batch Training Loss =  0.012521190568804741\n",
            "Batch Training Loss =  0.010606659576296806\n",
            "Batch Training Loss =  0.01787647232413292\n",
            "Batch Training Loss =  0.013583196327090263\n",
            "Batch Training Loss =  0.01502115000039339\n",
            "Batch Training Loss =  0.010303291492164135\n",
            "Batch Training Loss =  0.008747987449169159\n",
            "Batch Training Loss =  0.022074855864048004\n",
            "Batch Training Loss =  0.013112986460328102\n",
            "Batch Training Loss =  0.01069122925400734\n",
            "Batch Training Loss =  0.010695422068238258\n",
            "Batch Training Loss =  0.009403834119439125\n",
            "Batch Training Loss =  0.013097655028104782\n",
            "Batch Training Loss =  0.012862415052950382\n",
            "Batch Training Loss =  0.037532612681388855\n",
            "Batch Training Loss =  0.043281830847263336\n",
            "Batch Training Loss =  0.012201186269521713\n",
            "Batch Training Loss =  0.023564916104078293\n",
            "Batch Training Loss =  0.014816347509622574\n",
            "Batch Training Loss =  0.011730900034308434\n",
            "Batch Training Loss =  0.016509823501110077\n",
            "Batch Training Loss =  0.011709672398865223\n",
            "Batch Training Loss =  0.015606160275638103\n",
            "Batch Training Loss =  0.013009969145059586\n",
            "Batch Training Loss =  0.05333331972360611\n",
            "Batch Training Loss =  0.03171033784747124\n",
            "Validation Loss in this epoch is 0.400\n",
            "This is  48 th epoch\n",
            "Batch Training Loss =  0.020303256809711456\n",
            "Batch Training Loss =  0.026315543800592422\n",
            "Batch Training Loss =  0.026316897943615913\n",
            "Batch Training Loss =  0.011630666442215443\n",
            "Batch Training Loss =  0.011450819671154022\n",
            "Batch Training Loss =  0.012128612026572227\n",
            "Batch Training Loss =  0.013778496533632278\n",
            "Batch Training Loss =  0.013711009174585342\n",
            "Batch Training Loss =  0.012752174399793148\n",
            "Batch Training Loss =  0.008276436477899551\n",
            "Batch Training Loss =  0.012146150693297386\n",
            "Batch Training Loss =  0.01232664193958044\n",
            "Batch Training Loss =  0.013739539310336113\n",
            "Batch Training Loss =  0.013825841248035431\n",
            "Batch Training Loss =  0.01485874317586422\n",
            "Batch Training Loss =  0.009501622058451176\n",
            "Batch Training Loss =  0.00926164910197258\n",
            "Batch Training Loss =  0.016777444630861282\n",
            "Batch Training Loss =  0.010196574963629246\n",
            "Batch Training Loss =  0.03713013976812363\n",
            "Batch Training Loss =  0.011221595108509064\n",
            "Batch Training Loss =  0.015103344805538654\n",
            "Batch Training Loss =  0.014213677495718002\n",
            "Batch Training Loss =  0.012964226305484772\n",
            "Batch Training Loss =  0.013963720761239529\n",
            "Batch Training Loss =  0.03768017888069153\n",
            "Batch Training Loss =  0.02633015438914299\n",
            "Batch Training Loss =  0.012689344584941864\n",
            "Batch Training Loss =  0.013990265317261219\n",
            "Batch Training Loss =  0.021987073123455048\n",
            "Batch Training Loss =  0.010625331662595272\n",
            "Batch Training Loss =  0.010397198610007763\n",
            "Validation Loss in this epoch is 0.397\n",
            "This is  49 th epoch\n",
            "Batch Training Loss =  0.010493841022253036\n",
            "Batch Training Loss =  0.010614240542054176\n",
            "Batch Training Loss =  0.012260454706847668\n",
            "Batch Training Loss =  0.013739300891757011\n",
            "Batch Training Loss =  0.013949747197329998\n",
            "Batch Training Loss =  0.042438264936208725\n",
            "Batch Training Loss =  0.012116861529648304\n",
            "Batch Training Loss =  0.021487269550561905\n",
            "Batch Training Loss =  0.013544026762247086\n",
            "Batch Training Loss =  0.010294531472027302\n",
            "Batch Training Loss =  0.01141749694943428\n",
            "Batch Training Loss =  0.009936761111021042\n",
            "Batch Training Loss =  0.012089777737855911\n",
            "Batch Training Loss =  0.012391366995871067\n",
            "Batch Training Loss =  0.012987104244530201\n",
            "Batch Training Loss =  0.0172412171959877\n",
            "Batch Training Loss =  0.008299881592392921\n",
            "Batch Training Loss =  0.010238793678581715\n",
            "Batch Training Loss =  0.017049234360456467\n",
            "Batch Training Loss =  0.012541571632027626\n",
            "Batch Training Loss =  0.007928634062409401\n",
            "Batch Training Loss =  0.05496848747134209\n",
            "Batch Training Loss =  0.04607919231057167\n",
            "Batch Training Loss =  0.027893804013729095\n",
            "Batch Training Loss =  0.014529595151543617\n",
            "Batch Training Loss =  0.015271969139575958\n",
            "Batch Training Loss =  0.018780596554279327\n",
            "Batch Training Loss =  0.03777854144573212\n",
            "Batch Training Loss =  0.014663456939160824\n",
            "Batch Training Loss =  0.02608780935406685\n",
            "Batch Training Loss =  0.01787237450480461\n",
            "Batch Training Loss =  0.022576777264475822\n",
            "Validation Loss in this epoch is 0.408\n",
            "This is  50 th epoch\n",
            "Batch Training Loss =  0.018510891124606133\n",
            "Batch Training Loss =  0.008581900969147682\n",
            "Batch Training Loss =  0.025388196110725403\n",
            "Batch Training Loss =  0.01102357730269432\n",
            "Batch Training Loss =  0.012286200188100338\n",
            "Batch Training Loss =  0.020478034391999245\n",
            "Batch Training Loss =  0.016081811860203743\n",
            "Batch Training Loss =  0.012322258204221725\n",
            "Batch Training Loss =  0.013193073682487011\n",
            "Batch Training Loss =  0.038563672453165054\n",
            "Batch Training Loss =  0.013539254665374756\n",
            "Batch Training Loss =  0.01316858921200037\n",
            "Batch Training Loss =  0.016711082309484482\n",
            "Batch Training Loss =  0.010822512209415436\n",
            "Batch Training Loss =  0.01437639631330967\n",
            "Batch Training Loss =  0.01333985198289156\n",
            "Batch Training Loss =  0.010342874564230442\n",
            "Batch Training Loss =  0.01570509374141693\n",
            "Batch Training Loss =  0.031659141182899475\n",
            "Batch Training Loss =  0.031763795763254166\n",
            "Batch Training Loss =  0.020205873996019363\n",
            "Batch Training Loss =  0.012475205585360527\n",
            "Batch Training Loss =  0.016014546155929565\n",
            "Batch Training Loss =  0.012493329122662544\n",
            "Batch Training Loss =  0.012636622413992882\n",
            "Batch Training Loss =  0.011575005948543549\n",
            "Batch Training Loss =  0.010620160959661007\n",
            "Batch Training Loss =  0.013419494964182377\n",
            "Batch Training Loss =  0.010868910700082779\n",
            "Batch Training Loss =  0.01125417836010456\n",
            "Batch Training Loss =  0.014383235014975071\n",
            "Batch Training Loss =  0.008756686002016068\n",
            "Validation Loss in this epoch is 0.406\n",
            "This is  51 th epoch\n",
            "Batch Training Loss =  0.013375493697822094\n",
            "Batch Training Loss =  0.009108069352805614\n",
            "Batch Training Loss =  0.009183315560221672\n",
            "Batch Training Loss =  0.013746368698775768\n",
            "Batch Training Loss =  0.06883412599563599\n",
            "Batch Training Loss =  0.0274192001670599\n",
            "Batch Training Loss =  0.03980700299143791\n",
            "Batch Training Loss =  0.021857040002942085\n",
            "Batch Training Loss =  0.019004836678504944\n",
            "Batch Training Loss =  0.013070358894765377\n",
            "Batch Training Loss =  0.012581497430801392\n",
            "Batch Training Loss =  0.014349726028740406\n",
            "Batch Training Loss =  0.008296548388898373\n",
            "Batch Training Loss =  0.012947502546012402\n",
            "Batch Training Loss =  0.011587986722588539\n",
            "Batch Training Loss =  0.014284784905612469\n",
            "Batch Training Loss =  0.01221755426377058\n",
            "Batch Training Loss =  0.01656303182244301\n",
            "Batch Training Loss =  0.019113555550575256\n",
            "Batch Training Loss =  0.016929397359490395\n",
            "Batch Training Loss =  0.010533438064157963\n",
            "Batch Training Loss =  0.012567916885018349\n",
            "Batch Training Loss =  0.008677124045789242\n",
            "Batch Training Loss =  0.012494412250816822\n",
            "Batch Training Loss =  0.012731130234897137\n",
            "Batch Training Loss =  0.012512734159827232\n",
            "Batch Training Loss =  0.015585810877382755\n",
            "Batch Training Loss =  0.01591951586306095\n",
            "Batch Training Loss =  0.11397493630647659\n",
            "Batch Training Loss =  0.042315684258937836\n",
            "Batch Training Loss =  0.0280133243650198\n",
            "Batch Training Loss =  0.01868871971964836\n",
            "Validation Loss in this epoch is 0.408\n",
            "This is  52 th epoch\n",
            "Batch Training Loss =  0.012834825552999973\n",
            "Batch Training Loss =  0.013375440612435341\n",
            "Batch Training Loss =  0.01430083904415369\n",
            "Batch Training Loss =  0.009975748136639595\n",
            "Batch Training Loss =  0.039968207478523254\n",
            "Batch Training Loss =  0.010642069391906261\n",
            "Batch Training Loss =  0.021914944052696228\n",
            "Batch Training Loss =  0.01455865241587162\n",
            "Batch Training Loss =  0.018250294029712677\n",
            "Batch Training Loss =  0.016633717343211174\n",
            "Batch Training Loss =  0.010702583007514477\n",
            "Batch Training Loss =  0.013261542655527592\n",
            "Batch Training Loss =  0.010881870053708553\n",
            "Batch Training Loss =  0.018130365759134293\n",
            "Batch Training Loss =  0.0156561266630888\n",
            "Batch Training Loss =  0.011317509226500988\n",
            "Batch Training Loss =  0.014502120204269886\n",
            "Batch Training Loss =  0.013194018974900246\n",
            "Batch Training Loss =  0.012769251130521297\n",
            "Batch Training Loss =  0.012790730223059654\n",
            "Batch Training Loss =  0.011513706296682358\n",
            "Batch Training Loss =  0.012781220488250256\n",
            "Batch Training Loss =  0.008941061794757843\n",
            "Batch Training Loss =  0.015139235183596611\n",
            "Batch Training Loss =  0.014401031658053398\n",
            "Batch Training Loss =  0.00914410687983036\n",
            "Batch Training Loss =  0.07121308147907257\n",
            "Batch Training Loss =  0.011340291239321232\n",
            "Batch Training Loss =  0.03268372640013695\n",
            "Batch Training Loss =  0.012979840859770775\n",
            "Batch Training Loss =  0.011399766430258751\n",
            "Batch Training Loss =  0.017945973202586174\n",
            "Validation Loss in this epoch is 0.411\n",
            "This is  53 th epoch\n",
            "Batch Training Loss =  0.015198185108602047\n",
            "Batch Training Loss =  0.00892652291804552\n",
            "Batch Training Loss =  0.011896251700818539\n",
            "Batch Training Loss =  0.024220358580350876\n",
            "Batch Training Loss =  0.020609473809599876\n",
            "Batch Training Loss =  0.012035127729177475\n",
            "Batch Training Loss =  0.021520476788282394\n",
            "Batch Training Loss =  0.014974894933402538\n",
            "Batch Training Loss =  0.011884505860507488\n",
            "Batch Training Loss =  0.011784685775637627\n",
            "Batch Training Loss =  0.010494827292859554\n",
            "Batch Training Loss =  0.014639515429735184\n",
            "Batch Training Loss =  0.013565096072852612\n",
            "Batch Training Loss =  0.016688765957951546\n",
            "Batch Training Loss =  0.014538602903485298\n",
            "Batch Training Loss =  0.023496421054005623\n",
            "Batch Training Loss =  0.018311485648155212\n",
            "Batch Training Loss =  0.00981064885854721\n",
            "Batch Training Loss =  0.014793597161769867\n",
            "Batch Training Loss =  0.025816310197114944\n",
            "Batch Training Loss =  0.029898950830101967\n",
            "Batch Training Loss =  0.03134145215153694\n",
            "Batch Training Loss =  0.01714319735765457\n",
            "Batch Training Loss =  0.011341282166540623\n",
            "Batch Training Loss =  0.009025372564792633\n",
            "Batch Training Loss =  0.018766868859529495\n",
            "Batch Training Loss =  0.013640063814818859\n",
            "Batch Training Loss =  0.01593339815735817\n",
            "Batch Training Loss =  0.010152393952012062\n",
            "Batch Training Loss =  0.01238308660686016\n",
            "Batch Training Loss =  0.01182868704199791\n",
            "Batch Training Loss =  0.02272900752723217\n",
            "Validation Loss in this epoch is 0.428\n",
            "This is  54 th epoch\n",
            "Batch Training Loss =  0.013362283818423748\n",
            "Batch Training Loss =  0.008700218051671982\n",
            "Batch Training Loss =  0.010373272933065891\n",
            "Batch Training Loss =  0.011491011828184128\n",
            "Batch Training Loss =  0.011757436208426952\n",
            "Batch Training Loss =  0.055094312876462936\n",
            "Batch Training Loss =  0.031655747443437576\n",
            "Batch Training Loss =  0.01578337885439396\n",
            "Batch Training Loss =  0.010612661950290203\n",
            "Batch Training Loss =  0.04578308016061783\n",
            "Batch Training Loss =  0.01692523993551731\n",
            "Batch Training Loss =  0.0206560380756855\n",
            "Batch Training Loss =  0.015950001776218414\n",
            "Batch Training Loss =  0.016552455723285675\n",
            "Batch Training Loss =  0.01408541202545166\n",
            "Batch Training Loss =  0.015213285572826862\n",
            "Batch Training Loss =  0.010453402996063232\n",
            "Batch Training Loss =  0.015709614381194115\n",
            "Batch Training Loss =  0.015109637752175331\n",
            "Batch Training Loss =  0.020954253152012825\n",
            "Batch Training Loss =  0.010674398392438889\n",
            "Batch Training Loss =  0.007726440206170082\n",
            "Batch Training Loss =  0.07176436483860016\n",
            "Batch Training Loss =  0.023625554516911507\n",
            "Batch Training Loss =  0.026553228497505188\n",
            "Batch Training Loss =  0.01301996223628521\n",
            "Batch Training Loss =  0.0157039575278759\n",
            "Batch Training Loss =  0.014509117230772972\n",
            "Batch Training Loss =  0.015076308511197567\n",
            "Batch Training Loss =  0.020615937188267708\n",
            "Batch Training Loss =  0.014775078743696213\n",
            "Batch Training Loss =  0.012613301165401936\n",
            "Validation Loss in this epoch is 0.396\n",
            "This is  55 th epoch\n",
            "Batch Training Loss =  0.016858311370015144\n",
            "Batch Training Loss =  0.012308950535953045\n",
            "Batch Training Loss =  0.008090776391327381\n",
            "Batch Training Loss =  0.010411015711724758\n",
            "Batch Training Loss =  0.009058084338903427\n",
            "Batch Training Loss =  0.013499267399311066\n",
            "Batch Training Loss =  0.008683794178068638\n",
            "Batch Training Loss =  0.01209568977355957\n",
            "Batch Training Loss =  0.016704535111784935\n",
            "Batch Training Loss =  0.010487302206456661\n",
            "Batch Training Loss =  0.0336676724255085\n",
            "Batch Training Loss =  0.02021811157464981\n",
            "Batch Training Loss =  0.013526362366974354\n",
            "Batch Training Loss =  0.0339389331638813\n",
            "Batch Training Loss =  0.0148390494287014\n",
            "Batch Training Loss =  0.014273976907134056\n",
            "Batch Training Loss =  0.019543839618563652\n",
            "Batch Training Loss =  0.02620992809534073\n",
            "Batch Training Loss =  0.016770316287875175\n",
            "Batch Training Loss =  0.017231706529855728\n",
            "Batch Training Loss =  0.013804351910948753\n",
            "Batch Training Loss =  0.01483407337218523\n",
            "Batch Training Loss =  0.010499740950763226\n",
            "Batch Training Loss =  0.014593880623579025\n",
            "Batch Training Loss =  0.008838672190904617\n",
            "Batch Training Loss =  0.012482109479606152\n",
            "Batch Training Loss =  0.014315588399767876\n",
            "Batch Training Loss =  0.031755369156599045\n",
            "Batch Training Loss =  0.027458814904093742\n",
            "Batch Training Loss =  0.013989035040140152\n",
            "Batch Training Loss =  0.011968184262514114\n",
            "Batch Training Loss =  0.01235541794449091\n",
            "Validation Loss in this epoch is 0.399\n",
            "This is  56 th epoch\n",
            "Batch Training Loss =  0.007650607265532017\n",
            "Batch Training Loss =  0.009366936981678009\n",
            "Batch Training Loss =  0.010619740933179855\n",
            "Batch Training Loss =  0.010597608983516693\n",
            "Batch Training Loss =  0.009173751808702946\n",
            "Batch Training Loss =  0.01188705675303936\n",
            "Batch Training Loss =  0.01114501990377903\n",
            "Batch Training Loss =  0.01469237171113491\n",
            "Batch Training Loss =  0.011403316631913185\n",
            "Batch Training Loss =  0.01096922717988491\n",
            "Batch Training Loss =  0.009508732706308365\n",
            "Batch Training Loss =  0.015286166220903397\n",
            "Batch Training Loss =  0.01305155735462904\n",
            "Batch Training Loss =  0.012543530203402042\n",
            "Batch Training Loss =  0.015877529978752136\n",
            "Batch Training Loss =  0.014263263903558254\n",
            "Batch Training Loss =  0.05758140981197357\n",
            "Batch Training Loss =  0.06179158389568329\n",
            "Batch Training Loss =  0.14923380315303802\n",
            "Batch Training Loss =  0.37453988194465637\n",
            "Batch Training Loss =  0.5218543410301208\n",
            "Batch Training Loss =  1.9128669500350952\n",
            "Batch Training Loss =  0.8386089205741882\n",
            "Batch Training Loss =  0.38315868377685547\n",
            "Batch Training Loss =  0.3041243851184845\n",
            "Batch Training Loss =  0.1410747766494751\n",
            "Batch Training Loss =  0.32178547978401184\n",
            "Batch Training Loss =  0.2861838936805725\n",
            "Batch Training Loss =  0.15511013567447662\n",
            "Batch Training Loss =  0.11471561342477798\n",
            "Batch Training Loss =  0.1337699443101883\n",
            "Batch Training Loss =  0.2070760726928711\n",
            "Validation Loss in this epoch is 0.471\n",
            "This is  57 th epoch\n",
            "Batch Training Loss =  0.07013149559497833\n",
            "Batch Training Loss =  0.04118730127811432\n",
            "Batch Training Loss =  0.13834311068058014\n",
            "Batch Training Loss =  0.118740975856781\n",
            "Batch Training Loss =  0.14384828507900238\n",
            "Batch Training Loss =  0.06907550990581512\n",
            "Batch Training Loss =  0.14169710874557495\n",
            "Batch Training Loss =  0.08737345039844513\n",
            "Batch Training Loss =  0.04916015639901161\n",
            "Batch Training Loss =  0.1463627964258194\n",
            "Batch Training Loss =  0.10654392093420029\n",
            "Batch Training Loss =  0.0882376953959465\n",
            "Batch Training Loss =  0.11321182548999786\n",
            "Batch Training Loss =  0.04782174900174141\n",
            "Batch Training Loss =  0.03571029752492905\n",
            "Batch Training Loss =  0.08999712765216827\n",
            "Batch Training Loss =  0.14967074990272522\n",
            "Batch Training Loss =  0.263040691614151\n",
            "Batch Training Loss =  0.1355513632297516\n",
            "Batch Training Loss =  0.18461504578590393\n",
            "Batch Training Loss =  0.0878736600279808\n",
            "Batch Training Loss =  0.034205224364995956\n",
            "Batch Training Loss =  0.04579457640647888\n",
            "Batch Training Loss =  0.041932426393032074\n",
            "Batch Training Loss =  0.11514414101839066\n",
            "Batch Training Loss =  0.10793597996234894\n",
            "Batch Training Loss =  0.12465999275445938\n",
            "Batch Training Loss =  0.15466313064098358\n",
            "Batch Training Loss =  0.08485247939825058\n",
            "Batch Training Loss =  0.03257538378238678\n",
            "Batch Training Loss =  0.05259578675031662\n",
            "Batch Training Loss =  0.05511188134551048\n",
            "Validation Loss in this epoch is 0.467\n",
            "This is  58 th epoch\n",
            "Batch Training Loss =  0.05976559966802597\n",
            "Batch Training Loss =  0.036937326192855835\n",
            "Batch Training Loss =  0.02004394307732582\n",
            "Batch Training Loss =  0.0363200381398201\n",
            "Batch Training Loss =  0.03382766619324684\n",
            "Batch Training Loss =  0.027629274874925613\n",
            "Batch Training Loss =  0.02637939527630806\n",
            "Batch Training Loss =  0.028488269075751305\n",
            "Batch Training Loss =  0.026320280507206917\n",
            "Batch Training Loss =  0.032356102019548416\n",
            "Batch Training Loss =  0.03402985632419586\n",
            "Batch Training Loss =  0.01777677983045578\n",
            "Batch Training Loss =  0.05037473142147064\n",
            "Batch Training Loss =  0.026486465707421303\n",
            "Batch Training Loss =  0.09171699732542038\n",
            "Batch Training Loss =  0.10122717171907425\n",
            "Batch Training Loss =  0.16570018231868744\n",
            "Batch Training Loss =  0.08379118144512177\n",
            "Batch Training Loss =  0.12353770434856415\n",
            "Batch Training Loss =  0.07916788756847382\n",
            "Batch Training Loss =  0.04541679471731186\n",
            "Batch Training Loss =  0.052813176065683365\n",
            "Batch Training Loss =  0.045339226722717285\n",
            "Batch Training Loss =  0.03675488010048866\n",
            "Batch Training Loss =  0.07910923659801483\n",
            "Batch Training Loss =  0.04306960478425026\n",
            "Batch Training Loss =  0.020473597571253777\n",
            "Batch Training Loss =  0.08036672323942184\n",
            "Batch Training Loss =  0.05981006100773811\n",
            "Batch Training Loss =  0.021839391440153122\n",
            "Batch Training Loss =  0.022482803091406822\n",
            "Batch Training Loss =  0.028966937214136124\n",
            "Validation Loss in this epoch is 0.503\n",
            "This is  59 th epoch\n",
            "Batch Training Loss =  0.012570501305162907\n",
            "Batch Training Loss =  0.0191898625344038\n",
            "Batch Training Loss =  0.029565956443548203\n",
            "Batch Training Loss =  0.04506182670593262\n",
            "Batch Training Loss =  0.018970975652337074\n",
            "Batch Training Loss =  0.016813935711979866\n",
            "Batch Training Loss =  0.01375300157815218\n",
            "Batch Training Loss =  0.012126531451940536\n",
            "Batch Training Loss =  0.056331295520067215\n",
            "Batch Training Loss =  0.01672450453042984\n",
            "Batch Training Loss =  0.02213870733976364\n",
            "Batch Training Loss =  0.02285078540444374\n",
            "Batch Training Loss =  0.039842646569013596\n",
            "Batch Training Loss =  0.029976926743984222\n",
            "Batch Training Loss =  0.03125477954745293\n",
            "Batch Training Loss =  0.01833554543554783\n",
            "Batch Training Loss =  0.02441534586250782\n",
            "Batch Training Loss =  0.006177111994475126\n",
            "Batch Training Loss =  0.040475331246852875\n",
            "Batch Training Loss =  0.013745542615652084\n",
            "Batch Training Loss =  0.014682125300168991\n",
            "Batch Training Loss =  0.009837547317147255\n",
            "Batch Training Loss =  0.03340645879507065\n",
            "Batch Training Loss =  0.04275311902165413\n",
            "Batch Training Loss =  0.013584478758275509\n",
            "Batch Training Loss =  0.009730730205774307\n",
            "Batch Training Loss =  0.01491387840360403\n",
            "Batch Training Loss =  0.020512325689196587\n",
            "Batch Training Loss =  0.02921377681195736\n",
            "Batch Training Loss =  0.011990164406597614\n",
            "Batch Training Loss =  0.015116355381906033\n",
            "Batch Training Loss =  0.016889698803424835\n",
            "Validation Loss in this epoch is 0.447\n",
            "This is  60 th epoch\n",
            "Batch Training Loss =  0.009446167387068272\n",
            "Batch Training Loss =  0.010860312730073929\n",
            "Batch Training Loss =  0.009676813147962093\n",
            "Batch Training Loss =  0.01719050481915474\n",
            "Batch Training Loss =  0.006161144934594631\n",
            "Batch Training Loss =  0.007404705975204706\n",
            "Batch Training Loss =  0.010636555962264538\n",
            "Batch Training Loss =  0.013799615204334259\n",
            "Batch Training Loss =  0.014260165393352509\n",
            "Batch Training Loss =  0.01670495793223381\n",
            "Batch Training Loss =  0.008727225475013256\n",
            "Batch Training Loss =  0.012167057953774929\n",
            "Batch Training Loss =  0.008795817382633686\n",
            "Batch Training Loss =  0.009244278073310852\n",
            "Batch Training Loss =  0.01141353603452444\n",
            "Batch Training Loss =  0.08933749049901962\n",
            "Batch Training Loss =  0.05758722126483917\n",
            "Batch Training Loss =  0.04436782747507095\n",
            "Batch Training Loss =  0.026613276451826096\n",
            "Batch Training Loss =  0.02580195479094982\n",
            "Batch Training Loss =  0.029811205342411995\n",
            "Batch Training Loss =  0.023153502494096756\n",
            "Batch Training Loss =  0.014010947197675705\n",
            "Batch Training Loss =  0.018975699320435524\n",
            "Batch Training Loss =  0.01880941540002823\n",
            "Batch Training Loss =  0.031629592180252075\n",
            "Batch Training Loss =  0.036837369203567505\n",
            "Batch Training Loss =  0.015887843444943428\n",
            "Batch Training Loss =  0.018768005073070526\n",
            "Batch Training Loss =  0.01610322669148445\n",
            "Batch Training Loss =  0.01976259984076023\n",
            "Batch Training Loss =  0.02714732103049755\n",
            "Validation Loss in this epoch is 0.428\n",
            "This is  61 th epoch\n",
            "Batch Training Loss =  0.014483378268778324\n",
            "Batch Training Loss =  0.014357481151819229\n",
            "Batch Training Loss =  0.01274575013667345\n",
            "Batch Training Loss =  0.024514775723218918\n",
            "Batch Training Loss =  0.007413094863295555\n",
            "Batch Training Loss =  0.02570551075041294\n",
            "Batch Training Loss =  0.009443514980375767\n",
            "Batch Training Loss =  0.012493998743593693\n",
            "Batch Training Loss =  0.009164986200630665\n",
            "Batch Training Loss =  0.011432292871177197\n",
            "Batch Training Loss =  0.032961271703243256\n",
            "Batch Training Loss =  0.022252224385738373\n",
            "Batch Training Loss =  0.010252851992845535\n",
            "Batch Training Loss =  0.020170988515019417\n",
            "Batch Training Loss =  0.02883756160736084\n",
            "Batch Training Loss =  0.024113308638334274\n",
            "Batch Training Loss =  0.014583353884518147\n",
            "Batch Training Loss =  0.01201365515589714\n",
            "Batch Training Loss =  0.015049547888338566\n",
            "Batch Training Loss =  0.010814902372658253\n",
            "Batch Training Loss =  0.016069401055574417\n",
            "Batch Training Loss =  0.012023817747831345\n",
            "Batch Training Loss =  0.01130822766572237\n",
            "Batch Training Loss =  0.013813264667987823\n",
            "Batch Training Loss =  0.016973571851849556\n",
            "Batch Training Loss =  0.010335070081055164\n",
            "Batch Training Loss =  0.014606941491365433\n",
            "Batch Training Loss =  0.0077815912663936615\n",
            "Batch Training Loss =  0.01500712987035513\n",
            "Batch Training Loss =  0.02626325562596321\n",
            "Batch Training Loss =  0.029824281111359596\n",
            "Batch Training Loss =  0.01943495124578476\n",
            "Validation Loss in this epoch is 0.418\n",
            "This is  62 th epoch\n",
            "Batch Training Loss =  0.011841315776109695\n",
            "Batch Training Loss =  0.011034862138330936\n",
            "Batch Training Loss =  0.009825492277741432\n",
            "Batch Training Loss =  0.008895649574697018\n",
            "Batch Training Loss =  0.006920716725289822\n",
            "Batch Training Loss =  0.010948901996016502\n",
            "Batch Training Loss =  0.011385765857994556\n",
            "Batch Training Loss =  0.013791866600513458\n",
            "Batch Training Loss =  0.009049455635249615\n",
            "Batch Training Loss =  0.03551677614450455\n",
            "Batch Training Loss =  0.009763741865754128\n",
            "Batch Training Loss =  0.011655783280730247\n",
            "Batch Training Loss =  0.013241922482848167\n",
            "Batch Training Loss =  0.013863430358469486\n",
            "Batch Training Loss =  0.01258914079517126\n",
            "Batch Training Loss =  0.009844747371971607\n",
            "Batch Training Loss =  0.03762195259332657\n",
            "Batch Training Loss =  0.016106393188238144\n",
            "Batch Training Loss =  0.011063413694500923\n",
            "Batch Training Loss =  0.02936622127890587\n",
            "Batch Training Loss =  0.025991396978497505\n",
            "Batch Training Loss =  0.010053077712655067\n",
            "Batch Training Loss =  0.013294730335474014\n",
            "Batch Training Loss =  0.008274798281490803\n",
            "Batch Training Loss =  0.010919099673628807\n",
            "Batch Training Loss =  0.009987425059080124\n",
            "Batch Training Loss =  0.01132920477539301\n",
            "Batch Training Loss =  0.012143153697252274\n",
            "Batch Training Loss =  0.019286934286355972\n",
            "Batch Training Loss =  0.011450652033090591\n",
            "Batch Training Loss =  0.01552633848041296\n",
            "Batch Training Loss =  0.02483457326889038\n",
            "Validation Loss in this epoch is 0.438\n",
            "This is  63 th epoch\n",
            "Batch Training Loss =  0.025306491181254387\n",
            "Batch Training Loss =  0.012538621202111244\n",
            "Batch Training Loss =  0.011016273871064186\n",
            "Batch Training Loss =  0.008289150893688202\n",
            "Batch Training Loss =  0.008359250612556934\n",
            "Batch Training Loss =  0.02344648167490959\n",
            "Batch Training Loss =  0.01245118398219347\n",
            "Batch Training Loss =  0.014539886265993118\n",
            "Batch Training Loss =  0.008299480192363262\n",
            "Batch Training Loss =  0.008890191093087196\n",
            "Batch Training Loss =  0.01129843108355999\n",
            "Batch Training Loss =  0.01164685096591711\n",
            "Batch Training Loss =  0.010290353558957577\n",
            "Batch Training Loss =  0.010890049859881401\n",
            "Batch Training Loss =  0.007239794358611107\n",
            "Batch Training Loss =  0.009541132487356663\n",
            "Batch Training Loss =  0.044558778405189514\n",
            "Batch Training Loss =  0.03612513467669487\n",
            "Batch Training Loss =  0.017593014985322952\n",
            "Batch Training Loss =  0.016214750707149506\n",
            "Batch Training Loss =  0.009636560454964638\n",
            "Batch Training Loss =  0.011543529108166695\n",
            "Batch Training Loss =  0.01854787953197956\n",
            "Batch Training Loss =  0.04462564364075661\n",
            "Batch Training Loss =  0.035883720964193344\n",
            "Batch Training Loss =  0.015062403865158558\n",
            "Batch Training Loss =  0.016357574611902237\n",
            "Batch Training Loss =  0.010453253984451294\n",
            "Batch Training Loss =  0.009678434580564499\n",
            "Batch Training Loss =  0.012155532836914062\n",
            "Batch Training Loss =  0.012814903631806374\n",
            "Batch Training Loss =  0.01739414967596531\n",
            "Validation Loss in this epoch is 0.418\n",
            "This is  64 th epoch\n",
            "Batch Training Loss =  0.008890341967344284\n",
            "Batch Training Loss =  0.009865540079772472\n",
            "Batch Training Loss =  0.009606561623513699\n",
            "Batch Training Loss =  0.034385908395051956\n",
            "Batch Training Loss =  0.027467522770166397\n",
            "Batch Training Loss =  0.01170538179576397\n",
            "Batch Training Loss =  0.010360101237893105\n",
            "Batch Training Loss =  0.01480766199529171\n",
            "Batch Training Loss =  0.01887623779475689\n",
            "Batch Training Loss =  0.015650495886802673\n",
            "Batch Training Loss =  0.010500137694180012\n",
            "Batch Training Loss =  0.013413765467703342\n",
            "Batch Training Loss =  0.013610447756946087\n",
            "Batch Training Loss =  0.007639559451490641\n",
            "Batch Training Loss =  0.009099546819925308\n",
            "Batch Training Loss =  0.014163009822368622\n",
            "Batch Training Loss =  0.019527986645698547\n",
            "Batch Training Loss =  0.00951510388404131\n",
            "Batch Training Loss =  0.011129017919301987\n",
            "Batch Training Loss =  0.01371750608086586\n",
            "Batch Training Loss =  0.00823388434946537\n",
            "Batch Training Loss =  0.015226922929286957\n",
            "Batch Training Loss =  0.00953760091215372\n",
            "Batch Training Loss =  0.010158833116292953\n",
            "Batch Training Loss =  0.05452210083603859\n",
            "Batch Training Loss =  0.02873092144727707\n",
            "Batch Training Loss =  0.018121831119060516\n",
            "Batch Training Loss =  0.008996709249913692\n",
            "Batch Training Loss =  0.014743995852768421\n",
            "Batch Training Loss =  0.010183901526033878\n",
            "Batch Training Loss =  0.04257965460419655\n",
            "Batch Training Loss =  0.019488200545310974\n",
            "Validation Loss in this epoch is 0.421\n",
            "This is  65 th epoch\n",
            "Batch Training Loss =  0.013142351061105728\n",
            "Batch Training Loss =  0.013835889287292957\n",
            "Batch Training Loss =  0.017187530174851418\n",
            "Batch Training Loss =  0.013348814100027084\n",
            "Batch Training Loss =  0.00910338293761015\n",
            "Batch Training Loss =  0.01155454758554697\n",
            "Batch Training Loss =  0.011988379061222076\n",
            "Batch Training Loss =  0.012293931096792221\n",
            "Batch Training Loss =  0.015253466553986073\n",
            "Batch Training Loss =  0.007774221710860729\n",
            "Batch Training Loss =  0.013884839601814747\n",
            "Batch Training Loss =  0.007964950054883957\n",
            "Batch Training Loss =  0.010029149241745472\n",
            "Batch Training Loss =  0.011302899569272995\n",
            "Batch Training Loss =  0.007885752245783806\n",
            "Batch Training Loss =  0.011378658935427666\n",
            "Batch Training Loss =  0.0082630580291152\n",
            "Batch Training Loss =  0.009682673960924149\n",
            "Batch Training Loss =  0.011320212855935097\n",
            "Batch Training Loss =  0.010894293896853924\n",
            "Batch Training Loss =  0.015133064240217209\n",
            "Batch Training Loss =  0.010636475868523121\n",
            "Batch Training Loss =  0.04895959049463272\n",
            "Batch Training Loss =  0.028499620035290718\n",
            "Batch Training Loss =  0.021135246381163597\n",
            "Batch Training Loss =  0.01592257246375084\n",
            "Batch Training Loss =  0.011515714228153229\n",
            "Batch Training Loss =  0.017264628782868385\n",
            "Batch Training Loss =  0.011471174657344818\n",
            "Batch Training Loss =  0.010863653384149075\n",
            "Batch Training Loss =  0.0130388168618083\n",
            "Batch Training Loss =  0.0478772409260273\n",
            "Validation Loss in this epoch is 0.418\n",
            "This is  66 th epoch\n",
            "Batch Training Loss =  0.03889668360352516\n",
            "Batch Training Loss =  0.016910649836063385\n",
            "Batch Training Loss =  0.01255717035382986\n",
            "Batch Training Loss =  0.010202562436461449\n",
            "Batch Training Loss =  0.013671002350747585\n",
            "Batch Training Loss =  0.009320222772657871\n",
            "Batch Training Loss =  0.016006072983145714\n",
            "Batch Training Loss =  0.015403197146952152\n",
            "Batch Training Loss =  0.01180469524115324\n",
            "Batch Training Loss =  0.04772850126028061\n",
            "Batch Training Loss =  0.031411442905664444\n",
            "Batch Training Loss =  0.022751711308956146\n",
            "Batch Training Loss =  0.011293589137494564\n",
            "Batch Training Loss =  0.011612978763878345\n",
            "Batch Training Loss =  0.01988312043249607\n",
            "Batch Training Loss =  0.011256078258156776\n",
            "Batch Training Loss =  0.010083179920911789\n",
            "Batch Training Loss =  0.03365195170044899\n",
            "Batch Training Loss =  0.019135117530822754\n",
            "Batch Training Loss =  0.014363672584295273\n",
            "Batch Training Loss =  0.012897353619337082\n",
            "Batch Training Loss =  0.03471323102712631\n",
            "Batch Training Loss =  0.008793529123067856\n",
            "Batch Training Loss =  0.013613847084343433\n",
            "Batch Training Loss =  0.009706145152449608\n",
            "Batch Training Loss =  0.009633992798626423\n",
            "Batch Training Loss =  0.01235232688486576\n",
            "Batch Training Loss =  0.00958153884857893\n",
            "Batch Training Loss =  0.019435536116361618\n",
            "Batch Training Loss =  0.016259724274277687\n",
            "Batch Training Loss =  0.011149155907332897\n",
            "Batch Training Loss =  0.010157842189073563\n",
            "Validation Loss in this epoch is 0.402\n",
            "This is  67 th epoch\n",
            "Batch Training Loss =  0.007950962521135807\n",
            "Batch Training Loss =  0.011938379146158695\n",
            "Batch Training Loss =  0.009929144755005836\n",
            "Batch Training Loss =  0.00915137305855751\n",
            "Batch Training Loss =  0.008037027902901173\n",
            "Batch Training Loss =  0.00923256017267704\n",
            "Batch Training Loss =  0.009294298477470875\n",
            "Batch Training Loss =  0.013704520650207996\n",
            "Batch Training Loss =  0.010213213041424751\n",
            "Batch Training Loss =  0.016117174178361893\n",
            "Batch Training Loss =  0.010696961544454098\n",
            "Batch Training Loss =  0.01189444586634636\n",
            "Batch Training Loss =  0.010775556787848473\n",
            "Batch Training Loss =  0.011638686992228031\n",
            "Batch Training Loss =  0.015228522010147572\n",
            "Batch Training Loss =  0.010529163293540478\n",
            "Batch Training Loss =  0.00964504387229681\n",
            "Batch Training Loss =  0.013421965762972832\n",
            "Batch Training Loss =  0.016525819897651672\n",
            "Batch Training Loss =  0.01059677917510271\n",
            "Batch Training Loss =  0.05517752468585968\n",
            "Batch Training Loss =  0.025842059403657913\n",
            "Batch Training Loss =  0.013861726969480515\n",
            "Batch Training Loss =  0.014679450541734695\n",
            "Batch Training Loss =  0.011218334548175335\n",
            "Batch Training Loss =  0.02745426632463932\n",
            "Batch Training Loss =  0.012996891513466835\n",
            "Batch Training Loss =  0.03312629833817482\n",
            "Batch Training Loss =  0.015764145180583\n",
            "Batch Training Loss =  0.014500844292342663\n",
            "Batch Training Loss =  0.03334767743945122\n",
            "Batch Training Loss =  0.025219719856977463\n",
            "Validation Loss in this epoch is 0.421\n",
            "This is  68 th epoch\n",
            "Batch Training Loss =  0.024914495646953583\n",
            "Batch Training Loss =  0.022097116336226463\n",
            "Batch Training Loss =  0.010640481486916542\n",
            "Batch Training Loss =  0.014239744283258915\n",
            "Batch Training Loss =  0.01846516691148281\n",
            "Batch Training Loss =  0.01144198328256607\n",
            "Batch Training Loss =  0.02160896360874176\n",
            "Batch Training Loss =  0.019868146628141403\n",
            "Batch Training Loss =  0.024135272949934006\n",
            "Batch Training Loss =  0.009918682277202606\n",
            "Batch Training Loss =  0.016500575467944145\n",
            "Batch Training Loss =  0.010213959030807018\n",
            "Batch Training Loss =  0.012355288490653038\n",
            "Batch Training Loss =  0.008695241995155811\n",
            "Batch Training Loss =  0.014150697737932205\n",
            "Batch Training Loss =  0.010221992619335651\n",
            "Batch Training Loss =  0.008026889525353909\n",
            "Batch Training Loss =  0.016067130491137505\n",
            "Batch Training Loss =  0.012012613005936146\n",
            "Batch Training Loss =  0.029772095382213593\n",
            "Batch Training Loss =  0.01059631071984768\n",
            "Batch Training Loss =  0.008560805581510067\n",
            "Batch Training Loss =  0.010144616477191448\n",
            "Batch Training Loss =  0.016649186611175537\n",
            "Batch Training Loss =  0.012234430760145187\n",
            "Batch Training Loss =  0.011540497653186321\n",
            "Batch Training Loss =  0.012047737836837769\n",
            "Batch Training Loss =  0.013362174853682518\n",
            "Batch Training Loss =  0.010062682442367077\n",
            "Batch Training Loss =  0.012334618717432022\n",
            "Batch Training Loss =  0.017028870061039925\n",
            "Batch Training Loss =  0.009681099094450474\n",
            "Validation Loss in this epoch is 0.423\n",
            "This is  69 th epoch\n",
            "Batch Training Loss =  0.011034496128559113\n",
            "Batch Training Loss =  0.01043774001300335\n",
            "Batch Training Loss =  0.00809052586555481\n",
            "Batch Training Loss =  0.013516352511942387\n",
            "Batch Training Loss =  0.008262942545115948\n",
            "Batch Training Loss =  0.009116388857364655\n",
            "Batch Training Loss =  0.010743395425379276\n",
            "Batch Training Loss =  0.012000645510852337\n",
            "Batch Training Loss =  0.03504470735788345\n",
            "Batch Training Loss =  0.031260520219802856\n",
            "Batch Training Loss =  0.05077412724494934\n",
            "Batch Training Loss =  0.07066832482814789\n",
            "Batch Training Loss =  0.014521967619657516\n",
            "Batch Training Loss =  0.015515608713030815\n",
            "Batch Training Loss =  0.017765572294592857\n",
            "Batch Training Loss =  0.011756444349884987\n",
            "Batch Training Loss =  0.013682887889444828\n",
            "Batch Training Loss =  0.008790077641606331\n",
            "Batch Training Loss =  0.012041086331009865\n",
            "Batch Training Loss =  0.013531151227653027\n",
            "Batch Training Loss =  0.009178634732961655\n",
            "Batch Training Loss =  0.016209162771701813\n",
            "Batch Training Loss =  0.009494798257946968\n",
            "Batch Training Loss =  0.010985668748617172\n",
            "Batch Training Loss =  0.05373694747686386\n",
            "Batch Training Loss =  0.04004255682229996\n",
            "Batch Training Loss =  0.016427356749773026\n",
            "Batch Training Loss =  0.017448924481868744\n",
            "Batch Training Loss =  0.014312355779111385\n",
            "Batch Training Loss =  0.0227698665112257\n",
            "Batch Training Loss =  0.013383686542510986\n",
            "Batch Training Loss =  0.03307826444506645\n",
            "Validation Loss in this epoch is 0.411\n",
            "This is  70 th epoch\n",
            "Batch Training Loss =  0.018602019175887108\n",
            "Batch Training Loss =  0.023843618109822273\n",
            "Batch Training Loss =  0.009912955574691296\n",
            "Batch Training Loss =  0.010582664981484413\n",
            "Batch Training Loss =  0.010023832321166992\n",
            "Batch Training Loss =  0.014081098139286041\n",
            "Batch Training Loss =  0.016722004860639572\n",
            "Batch Training Loss =  0.0125270439311862\n",
            "Batch Training Loss =  0.010791238397359848\n",
            "Batch Training Loss =  0.011486462317407131\n",
            "Batch Training Loss =  0.01550549827516079\n",
            "Batch Training Loss =  0.01086150761693716\n",
            "Batch Training Loss =  0.00875706598162651\n",
            "Batch Training Loss =  0.011820436455309391\n",
            "Batch Training Loss =  0.01867331564426422\n",
            "Batch Training Loss =  0.04412124305963516\n",
            "Batch Training Loss =  0.020270850509405136\n",
            "Batch Training Loss =  0.01556322444230318\n",
            "Batch Training Loss =  0.014854486100375652\n",
            "Batch Training Loss =  0.02164786495268345\n",
            "Batch Training Loss =  0.022126860916614532\n",
            "Batch Training Loss =  0.01666952110826969\n",
            "Batch Training Loss =  0.012740748934447765\n",
            "Batch Training Loss =  0.015443405136466026\n",
            "Batch Training Loss =  0.011483948677778244\n",
            "Batch Training Loss =  0.01116561982780695\n",
            "Batch Training Loss =  0.009961049072444439\n",
            "Batch Training Loss =  0.012551304884254932\n",
            "Batch Training Loss =  0.011764366179704666\n",
            "Batch Training Loss =  0.014107145369052887\n",
            "Batch Training Loss =  0.011381772346794605\n",
            "Batch Training Loss =  0.020006921142339706\n",
            "Validation Loss in this epoch is 0.404\n",
            "This is  71 th epoch\n",
            "Batch Training Loss =  0.012158629484474659\n",
            "Batch Training Loss =  0.018011203035712242\n",
            "Batch Training Loss =  0.03796166554093361\n",
            "Batch Training Loss =  0.029443994164466858\n",
            "Batch Training Loss =  0.013358894735574722\n",
            "Batch Training Loss =  0.030055146664381027\n",
            "Batch Training Loss =  0.028248243033885956\n",
            "Batch Training Loss =  0.011723160743713379\n",
            "Batch Training Loss =  0.01403669361025095\n",
            "Batch Training Loss =  0.009910780005156994\n",
            "Batch Training Loss =  0.01094820350408554\n",
            "Batch Training Loss =  0.011356834322214127\n",
            "Batch Training Loss =  0.034559465944767\n",
            "Batch Training Loss =  0.015384155325591564\n",
            "Batch Training Loss =  0.01609492115676403\n",
            "Batch Training Loss =  0.01904190331697464\n",
            "Batch Training Loss =  0.010443556122481823\n",
            "Batch Training Loss =  0.015345431864261627\n",
            "Batch Training Loss =  0.011555313132703304\n",
            "Batch Training Loss =  0.013920823112130165\n",
            "Batch Training Loss =  0.012843267060816288\n",
            "Batch Training Loss =  0.01417466253042221\n",
            "Batch Training Loss =  0.013796533457934856\n",
            "Batch Training Loss =  0.019395478069782257\n",
            "Batch Training Loss =  0.01101046334952116\n",
            "Batch Training Loss =  0.025451479479670525\n",
            "Batch Training Loss =  0.019221292808651924\n",
            "Batch Training Loss =  0.010064437054097652\n",
            "Batch Training Loss =  0.011280126869678497\n",
            "Batch Training Loss =  0.011114751920104027\n",
            "Batch Training Loss =  0.00977547187358141\n",
            "Batch Training Loss =  0.014908036217093468\n",
            "Validation Loss in this epoch is 0.409\n",
            "This is  72 th epoch\n",
            "Batch Training Loss =  0.012438995763659477\n",
            "Batch Training Loss =  0.01655600219964981\n",
            "Batch Training Loss =  0.03735977038741112\n",
            "Batch Training Loss =  0.03835201635956764\n",
            "Batch Training Loss =  0.0731220468878746\n",
            "Batch Training Loss =  0.27975133061408997\n",
            "Batch Training Loss =  2.8848819732666016\n",
            "Batch Training Loss =  9.39926528930664\n",
            "Batch Training Loss =  0.8232475519180298\n",
            "Batch Training Loss =  0.4426480233669281\n",
            "Batch Training Loss =  0.45858660340309143\n",
            "Batch Training Loss =  0.39236873388290405\n",
            "Batch Training Loss =  0.9048158526420593\n",
            "Batch Training Loss =  0.4902522563934326\n",
            "Batch Training Loss =  0.4078303873538971\n",
            "Batch Training Loss =  0.44792670011520386\n",
            "Batch Training Loss =  0.5363525152206421\n",
            "Batch Training Loss =  0.3982012867927551\n",
            "Batch Training Loss =  0.5399723649024963\n",
            "Batch Training Loss =  0.4309808015823364\n",
            "Batch Training Loss =  0.4611715078353882\n",
            "Batch Training Loss =  0.7328610420227051\n",
            "Batch Training Loss =  0.6798487901687622\n",
            "Batch Training Loss =  0.44755762815475464\n",
            "Batch Training Loss =  0.45049867033958435\n",
            "Batch Training Loss =  0.5825861096382141\n",
            "Batch Training Loss =  0.5258049368858337\n",
            "Batch Training Loss =  0.4093568027019501\n",
            "Batch Training Loss =  0.4547426998615265\n",
            "Batch Training Loss =  0.365600049495697\n",
            "Batch Training Loss =  0.4253821074962616\n",
            "Batch Training Loss =  0.2716270387172699\n",
            "Validation Loss in this epoch is 0.499\n",
            "This is  73 th epoch\n",
            "Batch Training Loss =  0.36355453729629517\n",
            "Batch Training Loss =  0.2993279993534088\n",
            "Batch Training Loss =  0.2832406163215637\n",
            "Batch Training Loss =  0.22843988239765167\n",
            "Batch Training Loss =  0.303993284702301\n",
            "Batch Training Loss =  0.23266124725341797\n",
            "Batch Training Loss =  0.37211835384368896\n",
            "Batch Training Loss =  0.3616628646850586\n",
            "Batch Training Loss =  0.34058961272239685\n",
            "Batch Training Loss =  0.314090758562088\n",
            "Batch Training Loss =  0.2592712640762329\n",
            "Batch Training Loss =  0.22857064008712769\n",
            "Batch Training Loss =  0.27719825506210327\n",
            "Batch Training Loss =  0.19657573103904724\n",
            "Batch Training Loss =  0.17890897393226624\n",
            "Batch Training Loss =  0.13924828171730042\n",
            "Batch Training Loss =  0.2028089165687561\n",
            "Batch Training Loss =  0.13567250967025757\n",
            "Batch Training Loss =  0.24677589535713196\n",
            "Batch Training Loss =  0.21950577199459076\n",
            "Batch Training Loss =  0.26611611247062683\n",
            "Batch Training Loss =  0.10424771159887314\n",
            "Batch Training Loss =  0.16126924753189087\n",
            "Batch Training Loss =  0.15876545011997223\n",
            "Batch Training Loss =  0.22611303627490997\n",
            "Batch Training Loss =  0.1323232203722\n",
            "Batch Training Loss =  0.08781793713569641\n",
            "Batch Training Loss =  0.21657544374465942\n",
            "Batch Training Loss =  0.12954717874526978\n",
            "Batch Training Loss =  0.14002755284309387\n",
            "Batch Training Loss =  0.10302529484033585\n",
            "Batch Training Loss =  0.12216298282146454\n",
            "Validation Loss in this epoch is 0.380\n",
            "This is  74 th epoch\n",
            "Batch Training Loss =  0.09969258308410645\n",
            "Batch Training Loss =  0.07932490855455399\n",
            "Batch Training Loss =  0.06144427880644798\n",
            "Batch Training Loss =  0.06322507560253143\n",
            "Batch Training Loss =  0.08458665013313293\n",
            "Batch Training Loss =  0.06598962098360062\n",
            "Batch Training Loss =  0.0643109530210495\n",
            "Batch Training Loss =  0.1169222816824913\n",
            "Batch Training Loss =  0.062460895627737045\n",
            "Batch Training Loss =  0.047934986650943756\n",
            "Batch Training Loss =  0.14537543058395386\n",
            "Batch Training Loss =  0.0667252391576767\n",
            "Batch Training Loss =  0.04871716350317001\n",
            "Batch Training Loss =  0.13441163301467896\n",
            "Batch Training Loss =  0.05323224142193794\n",
            "Batch Training Loss =  0.06403406709432602\n",
            "Batch Training Loss =  0.07318472117185593\n",
            "Batch Training Loss =  0.05035818740725517\n",
            "Batch Training Loss =  0.06075192615389824\n",
            "Batch Training Loss =  0.046639636158943176\n",
            "Batch Training Loss =  0.08265403658151627\n",
            "Batch Training Loss =  0.031287502497434616\n",
            "Batch Training Loss =  0.09339676052331924\n",
            "Batch Training Loss =  0.043776389211416245\n",
            "Batch Training Loss =  0.05150660127401352\n",
            "Batch Training Loss =  0.057588472962379456\n",
            "Batch Training Loss =  0.06916828453540802\n",
            "Batch Training Loss =  0.060552336275577545\n",
            "Batch Training Loss =  0.043322205543518066\n",
            "Batch Training Loss =  0.03617554530501366\n",
            "Batch Training Loss =  0.03759138286113739\n",
            "Batch Training Loss =  0.054899729788303375\n",
            "Validation Loss in this epoch is 0.399\n",
            "This is  75 th epoch\n",
            "Batch Training Loss =  0.025825247168540955\n",
            "Batch Training Loss =  0.11445463448762894\n",
            "Batch Training Loss =  0.06223876029253006\n",
            "Batch Training Loss =  0.04596366733312607\n",
            "Batch Training Loss =  0.023243119940161705\n",
            "Batch Training Loss =  0.026147566735744476\n",
            "Batch Training Loss =  0.03196093812584877\n",
            "Batch Training Loss =  0.042537055909633636\n",
            "Batch Training Loss =  0.07486879080533981\n",
            "Batch Training Loss =  0.02783946320414543\n",
            "Batch Training Loss =  0.03176964819431305\n",
            "Batch Training Loss =  0.030561326071619987\n",
            "Batch Training Loss =  0.03918581083416939\n",
            "Batch Training Loss =  0.02545144408941269\n",
            "Batch Training Loss =  0.03193877637386322\n",
            "Batch Training Loss =  0.02623528055846691\n",
            "Batch Training Loss =  0.025889191776514053\n",
            "Batch Training Loss =  0.02351662889122963\n",
            "Batch Training Loss =  0.03339428827166557\n",
            "Batch Training Loss =  0.07196305692195892\n",
            "Batch Training Loss =  0.047597602009773254\n",
            "Batch Training Loss =  0.04204517975449562\n",
            "Batch Training Loss =  0.025365572422742844\n",
            "Batch Training Loss =  0.021677082404494286\n",
            "Batch Training Loss =  0.027145247906446457\n",
            "Batch Training Loss =  0.01753654144704342\n",
            "Batch Training Loss =  0.023424318060278893\n",
            "Batch Training Loss =  0.03835011646151543\n",
            "Batch Training Loss =  0.04142546281218529\n",
            "Batch Training Loss =  0.0291280597448349\n",
            "Batch Training Loss =  0.03402024880051613\n",
            "Batch Training Loss =  0.053642481565475464\n",
            "Validation Loss in this epoch is 0.421\n",
            "This is  76 th epoch\n",
            "Batch Training Loss =  0.01774640567600727\n",
            "Batch Training Loss =  0.01967674307525158\n",
            "Batch Training Loss =  0.02464180253446102\n",
            "Batch Training Loss =  0.01761106215417385\n",
            "Batch Training Loss =  0.015256054699420929\n",
            "Batch Training Loss =  0.022277621552348137\n",
            "Batch Training Loss =  0.019072914496064186\n",
            "Batch Training Loss =  0.02464412711560726\n",
            "Batch Training Loss =  0.09700395911931992\n",
            "Batch Training Loss =  0.04744679480791092\n",
            "Batch Training Loss =  0.03634335473179817\n",
            "Batch Training Loss =  0.01968487910926342\n",
            "Batch Training Loss =  0.022970300167798996\n",
            "Batch Training Loss =  0.043400589376688004\n",
            "Batch Training Loss =  0.038323890417814255\n",
            "Batch Training Loss =  0.03556009382009506\n",
            "Batch Training Loss =  0.01690720021724701\n",
            "Batch Training Loss =  0.04718942940235138\n",
            "Batch Training Loss =  0.024967627599835396\n",
            "Batch Training Loss =  0.014765998348593712\n",
            "Batch Training Loss =  0.03180422633886337\n",
            "Batch Training Loss =  0.01719549112021923\n",
            "Batch Training Loss =  0.028063008561730385\n",
            "Batch Training Loss =  0.023010868579149246\n",
            "Batch Training Loss =  0.018495792523026466\n",
            "Batch Training Loss =  0.018856827169656754\n",
            "Batch Training Loss =  0.016879292204976082\n",
            "Batch Training Loss =  0.030050812289118767\n",
            "Batch Training Loss =  0.023068958893418312\n",
            "Batch Training Loss =  0.020336980000138283\n",
            "Batch Training Loss =  0.018246056511998177\n",
            "Batch Training Loss =  0.01730591244995594\n",
            "Validation Loss in this epoch is 0.405\n",
            "This is  77 th epoch\n",
            "Batch Training Loss =  0.015461103990674019\n",
            "Batch Training Loss =  0.017437346279621124\n",
            "Batch Training Loss =  0.016808142885565758\n",
            "Batch Training Loss =  0.05547632277011871\n",
            "Batch Training Loss =  0.029830124229192734\n",
            "Batch Training Loss =  0.0125723322853446\n",
            "Batch Training Loss =  0.025443488731980324\n",
            "Batch Training Loss =  0.02802753634750843\n",
            "Batch Training Loss =  0.01555008627474308\n",
            "Batch Training Loss =  0.013929841108620167\n",
            "Batch Training Loss =  0.015191780403256416\n",
            "Batch Training Loss =  0.02189384028315544\n",
            "Batch Training Loss =  0.014281931333243847\n",
            "Batch Training Loss =  0.03678080439567566\n",
            "Batch Training Loss =  0.02124076895415783\n",
            "Batch Training Loss =  0.021916523575782776\n",
            "Batch Training Loss =  0.020148990675807\n",
            "Batch Training Loss =  0.01999380625784397\n",
            "Batch Training Loss =  0.016847863793373108\n",
            "Batch Training Loss =  0.013977420516312122\n",
            "Batch Training Loss =  0.01544319000095129\n",
            "Batch Training Loss =  0.01659444533288479\n",
            "Batch Training Loss =  0.013445816934108734\n",
            "Batch Training Loss =  0.016270605847239494\n",
            "Batch Training Loss =  0.010665249079465866\n",
            "Batch Training Loss =  0.017213180661201477\n",
            "Batch Training Loss =  0.061803534626960754\n",
            "Batch Training Loss =  0.04133863002061844\n",
            "Batch Training Loss =  0.03191879764199257\n",
            "Batch Training Loss =  0.028274839743971825\n",
            "Batch Training Loss =  0.013670125976204872\n",
            "Batch Training Loss =  0.021832415834069252\n",
            "Validation Loss in this epoch is 0.409\n",
            "This is  78 th epoch\n",
            "Batch Training Loss =  0.031555138528347015\n",
            "Batch Training Loss =  0.024796301499009132\n",
            "Batch Training Loss =  0.018474629148840904\n",
            "Batch Training Loss =  0.015118913725018501\n",
            "Batch Training Loss =  0.012376908212900162\n",
            "Batch Training Loss =  0.02185281552374363\n",
            "Batch Training Loss =  0.013028564862906933\n",
            "Batch Training Loss =  0.014947469346225262\n",
            "Batch Training Loss =  0.013251865282654762\n",
            "Batch Training Loss =  0.014440411701798439\n",
            "Batch Training Loss =  0.012069711461663246\n",
            "Batch Training Loss =  0.00936174113303423\n",
            "Batch Training Loss =  0.012592417187988758\n",
            "Batch Training Loss =  0.012349423952400684\n",
            "Batch Training Loss =  0.01045802142471075\n",
            "Batch Training Loss =  0.011715855449438095\n",
            "Batch Training Loss =  0.017093371599912643\n",
            "Batch Training Loss =  0.012407131493091583\n",
            "Batch Training Loss =  0.012400095351040363\n",
            "Batch Training Loss =  0.051294151693582535\n",
            "Batch Training Loss =  0.03624933958053589\n",
            "Batch Training Loss =  0.013219989836215973\n",
            "Batch Training Loss =  0.016575349494814873\n",
            "Batch Training Loss =  0.016504308208823204\n",
            "Batch Training Loss =  0.02054702676832676\n",
            "Batch Training Loss =  0.020806942135095596\n",
            "Batch Training Loss =  0.012657837942242622\n",
            "Batch Training Loss =  0.018795063719153404\n",
            "Batch Training Loss =  0.014594728127121925\n",
            "Batch Training Loss =  0.0147963622584939\n",
            "Batch Training Loss =  0.051487941294908524\n",
            "Batch Training Loss =  0.04128392040729523\n",
            "Validation Loss in this epoch is 0.429\n",
            "This is  79 th epoch\n",
            "Batch Training Loss =  0.03720217943191528\n",
            "Batch Training Loss =  0.01593579351902008\n",
            "Batch Training Loss =  0.014364467933773994\n",
            "Batch Training Loss =  0.01820005476474762\n",
            "Batch Training Loss =  0.010542461648583412\n",
            "Batch Training Loss =  0.011623043566942215\n",
            "Batch Training Loss =  0.023836329579353333\n",
            "Batch Training Loss =  0.012403596192598343\n",
            "Batch Training Loss =  0.01568019948899746\n",
            "Batch Training Loss =  0.00995418056845665\n",
            "Batch Training Loss =  0.015945343300700188\n",
            "Batch Training Loss =  0.035276301205158234\n",
            "Batch Training Loss =  0.01979306899011135\n",
            "Batch Training Loss =  0.01297951489686966\n",
            "Batch Training Loss =  0.01303522102534771\n",
            "Batch Training Loss =  0.013058502227067947\n",
            "Batch Training Loss =  0.0132106589153409\n",
            "Batch Training Loss =  0.011648417450487614\n",
            "Batch Training Loss =  0.01302960142493248\n",
            "Batch Training Loss =  0.039083048701286316\n",
            "Batch Training Loss =  0.024802321568131447\n",
            "Batch Training Loss =  0.021645214408636093\n",
            "Batch Training Loss =  0.0144195556640625\n",
            "Batch Training Loss =  0.022773290053009987\n",
            "Batch Training Loss =  0.0237292293459177\n",
            "Batch Training Loss =  0.014744087122380733\n",
            "Batch Training Loss =  0.015217742882668972\n",
            "Batch Training Loss =  0.011426427401602268\n",
            "Batch Training Loss =  0.018618373200297356\n",
            "Batch Training Loss =  0.009948386810719967\n",
            "Batch Training Loss =  0.01636774279177189\n",
            "Batch Training Loss =  0.014182350598275661\n",
            "Validation Loss in this epoch is 0.414\n",
            "This is  80 th epoch\n",
            "Batch Training Loss =  0.016232969239354134\n",
            "Batch Training Loss =  0.010174347087740898\n",
            "Batch Training Loss =  0.014087262563407421\n",
            "Batch Training Loss =  0.011952762492001057\n",
            "Batch Training Loss =  0.009538073092699051\n",
            "Batch Training Loss =  0.016078103333711624\n",
            "Batch Training Loss =  0.0125736678019166\n",
            "Batch Training Loss =  0.029233785346150398\n",
            "Batch Training Loss =  0.015209397301077843\n",
            "Batch Training Loss =  0.018176306039094925\n",
            "Batch Training Loss =  0.02806120179593563\n",
            "Batch Training Loss =  0.011984494514763355\n",
            "Batch Training Loss =  0.017866220325231552\n",
            "Batch Training Loss =  0.01263398863375187\n",
            "Batch Training Loss =  0.01267658919095993\n",
            "Batch Training Loss =  0.011918545700609684\n",
            "Batch Training Loss =  0.020999524742364883\n",
            "Batch Training Loss =  0.008984824642539024\n",
            "Batch Training Loss =  0.009057673625648022\n",
            "Batch Training Loss =  0.014146368950605392\n",
            "Batch Training Loss =  0.010406621731817722\n",
            "Batch Training Loss =  0.011488714255392551\n",
            "Batch Training Loss =  0.011864909902215004\n",
            "Batch Training Loss =  0.011441459879279137\n",
            "Batch Training Loss =  0.014284111559391022\n",
            "Batch Training Loss =  0.009833713993430138\n",
            "Batch Training Loss =  0.01281882543116808\n",
            "Batch Training Loss =  0.05223073065280914\n",
            "Batch Training Loss =  0.03849472105503082\n",
            "Batch Training Loss =  0.020216450095176697\n",
            "Batch Training Loss =  0.010597704909741879\n",
            "Batch Training Loss =  0.009175733663141727\n",
            "Validation Loss in this epoch is 0.419\n",
            "This is  81 th epoch\n",
            "Batch Training Loss =  0.010948529466986656\n",
            "Batch Training Loss =  0.012018309906125069\n",
            "Batch Training Loss =  0.01021579746156931\n",
            "Batch Training Loss =  0.009949933737516403\n",
            "Batch Training Loss =  0.016160689294338226\n",
            "Batch Training Loss =  0.008357728831470013\n",
            "Batch Training Loss =  0.09819160401821136\n",
            "Batch Training Loss =  0.1632775515317917\n",
            "Batch Training Loss =  0.3287442922592163\n",
            "Batch Training Loss =  0.5676343441009521\n",
            "Batch Training Loss =  0.24614205956459045\n",
            "Batch Training Loss =  0.09676317125558853\n",
            "Batch Training Loss =  0.21976421773433685\n",
            "Batch Training Loss =  0.07683378458023071\n",
            "Batch Training Loss =  0.053847625851631165\n",
            "Batch Training Loss =  0.0534733384847641\n",
            "Batch Training Loss =  0.02803158015012741\n",
            "Batch Training Loss =  0.05555211007595062\n",
            "Batch Training Loss =  0.04451144114136696\n",
            "Batch Training Loss =  0.03068831004202366\n",
            "Batch Training Loss =  0.04253479093313217\n",
            "Batch Training Loss =  0.04274693876504898\n",
            "Batch Training Loss =  0.03360503166913986\n",
            "Batch Training Loss =  0.06235151365399361\n",
            "Batch Training Loss =  0.027522306889295578\n",
            "Batch Training Loss =  0.06563308089971542\n",
            "Batch Training Loss =  0.040874697268009186\n",
            "Batch Training Loss =  0.06192032992839813\n",
            "Batch Training Loss =  0.04738180339336395\n",
            "Batch Training Loss =  0.03090999275445938\n",
            "Batch Training Loss =  0.048135608434677124\n",
            "Batch Training Loss =  0.014935514889657497\n",
            "Validation Loss in this epoch is 0.447\n",
            "This is  82 th epoch\n",
            "Batch Training Loss =  0.018497707322239876\n",
            "Batch Training Loss =  0.028057338669896126\n",
            "Batch Training Loss =  0.018372898921370506\n",
            "Batch Training Loss =  0.01498128566890955\n",
            "Batch Training Loss =  0.02436814084649086\n",
            "Batch Training Loss =  0.02226710505783558\n",
            "Batch Training Loss =  0.014111382886767387\n",
            "Batch Training Loss =  0.019355587661266327\n",
            "Batch Training Loss =  0.014125015586614609\n",
            "Batch Training Loss =  0.017587583512067795\n",
            "Batch Training Loss =  0.0781823992729187\n",
            "Batch Training Loss =  0.01973699778318405\n",
            "Batch Training Loss =  0.03779129683971405\n",
            "Batch Training Loss =  0.015004783868789673\n",
            "Batch Training Loss =  0.02215522527694702\n",
            "Batch Training Loss =  0.022823743522167206\n",
            "Batch Training Loss =  0.024077916517853737\n",
            "Batch Training Loss =  0.03698901832103729\n",
            "Batch Training Loss =  0.018803710117936134\n",
            "Batch Training Loss =  0.018861815333366394\n",
            "Batch Training Loss =  0.01955673284828663\n",
            "Batch Training Loss =  0.03272772952914238\n",
            "Batch Training Loss =  0.013354782946407795\n",
            "Batch Training Loss =  0.02538856491446495\n",
            "Batch Training Loss =  0.021247364580631256\n",
            "Batch Training Loss =  0.0075791971758008\n",
            "Batch Training Loss =  0.01566695235669613\n",
            "Batch Training Loss =  0.02018565498292446\n",
            "Batch Training Loss =  0.013356137089431286\n",
            "Batch Training Loss =  0.00999120157212019\n",
            "Batch Training Loss =  0.008578812703490257\n",
            "Batch Training Loss =  0.019032906740903854\n",
            "Validation Loss in this epoch is 0.414\n",
            "This is  83 th epoch\n",
            "Batch Training Loss =  0.014738216064870358\n",
            "Batch Training Loss =  0.01106969453394413\n",
            "Batch Training Loss =  0.013098245486617088\n",
            "Batch Training Loss =  0.015857521444559097\n",
            "Batch Training Loss =  0.011024189181625843\n",
            "Batch Training Loss =  0.008991857059299946\n",
            "Batch Training Loss =  0.008021448738873005\n",
            "Batch Training Loss =  0.015764864161610603\n",
            "Batch Training Loss =  0.01288510486483574\n",
            "Batch Training Loss =  0.014480847865343094\n",
            "Batch Training Loss =  0.012588156387209892\n",
            "Batch Training Loss =  0.011620726436376572\n",
            "Batch Training Loss =  0.01055010873824358\n",
            "Batch Training Loss =  0.009261463768780231\n",
            "Batch Training Loss =  0.013625889085233212\n",
            "Batch Training Loss =  0.01051093079149723\n",
            "Batch Training Loss =  0.011531933210790157\n",
            "Batch Training Loss =  0.011910059489309788\n",
            "Batch Training Loss =  0.008110488764941692\n",
            "Batch Training Loss =  0.009143517352640629\n",
            "Batch Training Loss =  0.015639645978808403\n",
            "Batch Training Loss =  0.012619958259165287\n",
            "Batch Training Loss =  0.014949074946343899\n",
            "Batch Training Loss =  0.06847212463617325\n",
            "Batch Training Loss =  0.03158610686659813\n",
            "Batch Training Loss =  0.02924342080950737\n",
            "Batch Training Loss =  0.018461894243955612\n",
            "Batch Training Loss =  0.022072236984968185\n",
            "Batch Training Loss =  0.020020581781864166\n",
            "Batch Training Loss =  0.052491478621959686\n",
            "Batch Training Loss =  0.017452744767069817\n",
            "Batch Training Loss =  0.023492947220802307\n",
            "Validation Loss in this epoch is 0.431\n",
            "This is  84 th epoch\n",
            "Batch Training Loss =  0.013863896019756794\n",
            "Batch Training Loss =  0.009860176593065262\n",
            "Batch Training Loss =  0.010586898773908615\n",
            "Batch Training Loss =  0.013021771796047688\n",
            "Batch Training Loss =  0.011138273403048515\n",
            "Batch Training Loss =  0.03808131814002991\n",
            "Batch Training Loss =  0.018163785338401794\n",
            "Batch Training Loss =  0.020383069291710854\n",
            "Batch Training Loss =  0.016235608607530594\n",
            "Batch Training Loss =  0.013236308470368385\n",
            "Batch Training Loss =  0.012435047887265682\n",
            "Batch Training Loss =  0.011959653347730637\n",
            "Batch Training Loss =  0.01408469770103693\n",
            "Batch Training Loss =  0.02122250571846962\n",
            "Batch Training Loss =  0.009509081952273846\n",
            "Batch Training Loss =  0.013017069548368454\n",
            "Batch Training Loss =  0.015092666260898113\n",
            "Batch Training Loss =  0.008443492464721203\n",
            "Batch Training Loss =  0.015986019745469093\n",
            "Batch Training Loss =  0.03468120098114014\n",
            "Batch Training Loss =  0.0445505827665329\n",
            "Batch Training Loss =  0.01127336174249649\n",
            "Batch Training Loss =  0.018438493832945824\n",
            "Batch Training Loss =  0.010364573448896408\n",
            "Batch Training Loss =  0.011921259574592113\n",
            "Batch Training Loss =  0.026581458747386932\n",
            "Batch Training Loss =  0.017179345712065697\n",
            "Batch Training Loss =  0.011559578590095043\n",
            "Batch Training Loss =  0.01184912584722042\n",
            "Batch Training Loss =  0.01626358926296234\n",
            "Batch Training Loss =  0.012490267865359783\n",
            "Batch Training Loss =  0.013374158181250095\n",
            "Validation Loss in this epoch is 0.419\n",
            "This is  85 th epoch\n",
            "Batch Training Loss =  0.01994045451283455\n",
            "Batch Training Loss =  0.01320597529411316\n",
            "Batch Training Loss =  0.013672186993062496\n",
            "Batch Training Loss =  0.00764103839173913\n",
            "Batch Training Loss =  0.011561065912246704\n",
            "Batch Training Loss =  0.009733341634273529\n",
            "Batch Training Loss =  0.014336065389215946\n",
            "Batch Training Loss =  0.011148379184305668\n",
            "Batch Training Loss =  0.009370947256684303\n",
            "Batch Training Loss =  0.01299225352704525\n",
            "Batch Training Loss =  0.007529166992753744\n",
            "Batch Training Loss =  0.012381205335259438\n",
            "Batch Training Loss =  0.03360564634203911\n",
            "Batch Training Loss =  0.024971475824713707\n",
            "Batch Training Loss =  0.015203489921987057\n",
            "Batch Training Loss =  0.010974083095788956\n",
            "Batch Training Loss =  0.015070515684783459\n",
            "Batch Training Loss =  0.014051282778382301\n",
            "Batch Training Loss =  0.012075708247721195\n",
            "Batch Training Loss =  0.011341291479766369\n",
            "Batch Training Loss =  0.012115688063204288\n",
            "Batch Training Loss =  0.008738304488360882\n",
            "Batch Training Loss =  0.02814658358693123\n",
            "Batch Training Loss =  0.022196967154741287\n",
            "Batch Training Loss =  0.014851170592010021\n",
            "Batch Training Loss =  0.022492721676826477\n",
            "Batch Training Loss =  0.007760623004287481\n",
            "Batch Training Loss =  0.012222163379192352\n",
            "Batch Training Loss =  0.012016450054943562\n",
            "Batch Training Loss =  0.012243718840181828\n",
            "Batch Training Loss =  0.010757509618997574\n",
            "Batch Training Loss =  0.009161148220300674\n",
            "Validation Loss in this epoch is 0.409\n",
            "This is  86 th epoch\n",
            "Batch Training Loss =  0.011044208891689777\n",
            "Batch Training Loss =  0.010958224534988403\n",
            "Batch Training Loss =  0.012095553800463676\n",
            "Batch Training Loss =  0.009411581791937351\n",
            "Batch Training Loss =  0.01317397877573967\n",
            "Batch Training Loss =  0.007987890392541885\n",
            "Batch Training Loss =  0.01179062481969595\n",
            "Batch Training Loss =  0.009697417728602886\n",
            "Batch Training Loss =  0.010265025310218334\n",
            "Batch Training Loss =  0.010956461541354656\n",
            "Batch Training Loss =  0.009695222601294518\n",
            "Batch Training Loss =  0.0446980856359005\n",
            "Batch Training Loss =  0.011742150411009789\n",
            "Batch Training Loss =  0.013441510498523712\n",
            "Batch Training Loss =  0.017257709056138992\n",
            "Batch Training Loss =  0.00889583583921194\n",
            "Batch Training Loss =  0.01132714468985796\n",
            "Batch Training Loss =  0.010268483310937881\n",
            "Batch Training Loss =  0.012149150483310223\n",
            "Batch Training Loss =  0.008264206349849701\n",
            "Batch Training Loss =  0.02415039762854576\n",
            "Batch Training Loss =  0.013294006697833538\n",
            "Batch Training Loss =  0.013952028006315231\n",
            "Batch Training Loss =  0.04403960704803467\n",
            "Batch Training Loss =  0.014536497183144093\n",
            "Batch Training Loss =  0.026576682925224304\n",
            "Batch Training Loss =  0.009944223798811436\n",
            "Batch Training Loss =  0.0091331722214818\n",
            "Batch Training Loss =  0.01157157402485609\n",
            "Batch Training Loss =  0.013396400026977062\n",
            "Batch Training Loss =  0.01267711166292429\n",
            "Batch Training Loss =  0.011705880053341389\n",
            "Validation Loss in this epoch is 0.412\n",
            "This is  87 th epoch\n",
            "Batch Training Loss =  0.01159282773733139\n",
            "Batch Training Loss =  0.10238476097583771\n",
            "Batch Training Loss =  0.05301647260785103\n",
            "Batch Training Loss =  0.04690798372030258\n",
            "Batch Training Loss =  0.03202347457408905\n",
            "Batch Training Loss =  0.019626107066869736\n",
            "Batch Training Loss =  0.013682740740478039\n",
            "Batch Training Loss =  0.022814102470874786\n",
            "Batch Training Loss =  0.029331140220165253\n",
            "Batch Training Loss =  0.01975870132446289\n",
            "Batch Training Loss =  0.013670350424945354\n",
            "Batch Training Loss =  0.011838095262646675\n",
            "Batch Training Loss =  0.014451691880822182\n",
            "Batch Training Loss =  0.010209586471319199\n",
            "Batch Training Loss =  0.013654880225658417\n",
            "Batch Training Loss =  0.013075322844088078\n",
            "Batch Training Loss =  0.012716185301542282\n",
            "Batch Training Loss =  0.020193370059132576\n",
            "Batch Training Loss =  0.009189153090119362\n",
            "Batch Training Loss =  0.011764328926801682\n",
            "Batch Training Loss =  0.01377998385578394\n",
            "Batch Training Loss =  0.01133919507265091\n",
            "Batch Training Loss =  0.014374030753970146\n",
            "Batch Training Loss =  0.011860213242471218\n",
            "Batch Training Loss =  0.012474145740270615\n",
            "Batch Training Loss =  0.07473543286323547\n",
            "Batch Training Loss =  0.04204421862959862\n",
            "Batch Training Loss =  0.03165140002965927\n",
            "Batch Training Loss =  0.01977432519197464\n",
            "Batch Training Loss =  0.021100608631968498\n",
            "Batch Training Loss =  0.040727872401475906\n",
            "Batch Training Loss =  0.020849037915468216\n",
            "Validation Loss in this epoch is 0.434\n",
            "This is  88 th epoch\n",
            "Batch Training Loss =  0.021874260157346725\n",
            "Batch Training Loss =  0.01904585398733616\n",
            "Batch Training Loss =  0.012712068855762482\n",
            "Batch Training Loss =  0.014496970921754837\n",
            "Batch Training Loss =  0.011612137779593468\n",
            "Batch Training Loss =  0.014716907404363155\n",
            "Batch Training Loss =  0.039402950555086136\n",
            "Batch Training Loss =  0.012226616032421589\n",
            "Batch Training Loss =  0.013089406304061413\n",
            "Batch Training Loss =  0.012194406241178513\n",
            "Batch Training Loss =  0.012355724349617958\n",
            "Batch Training Loss =  0.013420501723885536\n",
            "Batch Training Loss =  0.010392969474196434\n",
            "Batch Training Loss =  0.01048719510436058\n",
            "Batch Training Loss =  0.009835419245064259\n",
            "Batch Training Loss =  0.012148130685091019\n",
            "Batch Training Loss =  0.013186224736273289\n",
            "Batch Training Loss =  0.008725952357053757\n",
            "Batch Training Loss =  0.011242534033954144\n",
            "Batch Training Loss =  0.008376606740057468\n",
            "Batch Training Loss =  0.011827118694782257\n",
            "Batch Training Loss =  0.011316823773086071\n",
            "Batch Training Loss =  0.03286382555961609\n",
            "Batch Training Loss =  0.012257957831025124\n",
            "Batch Training Loss =  0.011390297673642635\n",
            "Batch Training Loss =  0.010320945642888546\n",
            "Batch Training Loss =  0.01459803618490696\n",
            "Batch Training Loss =  0.015618477016687393\n",
            "Batch Training Loss =  0.011392713524401188\n",
            "Batch Training Loss =  0.04464246705174446\n",
            "Batch Training Loss =  0.013829898089170456\n",
            "Batch Training Loss =  0.012199447490274906\n",
            "Validation Loss in this epoch is 0.410\n",
            "This is  89 th epoch\n",
            "Batch Training Loss =  0.009169832803308964\n",
            "Batch Training Loss =  0.010955662466585636\n",
            "Batch Training Loss =  0.027960792183876038\n",
            "Batch Training Loss =  0.013616783544421196\n",
            "Batch Training Loss =  0.02391737885773182\n",
            "Batch Training Loss =  0.01591147482395172\n",
            "Batch Training Loss =  0.0143042067065835\n",
            "Batch Training Loss =  0.009884756989777088\n",
            "Batch Training Loss =  0.008491527289152145\n",
            "Batch Training Loss =  0.012233085930347443\n",
            "Batch Training Loss =  0.02683023363351822\n",
            "Batch Training Loss =  0.026460573077201843\n",
            "Batch Training Loss =  0.010240117087960243\n",
            "Batch Training Loss =  0.011771011166274548\n",
            "Batch Training Loss =  0.009234137833118439\n",
            "Batch Training Loss =  0.014780743978917599\n",
            "Batch Training Loss =  0.038859084248542786\n",
            "Batch Training Loss =  0.03192766010761261\n",
            "Batch Training Loss =  0.019525196403265\n",
            "Batch Training Loss =  0.01084607932716608\n",
            "Batch Training Loss =  0.015531551092863083\n",
            "Batch Training Loss =  0.024311654269695282\n",
            "Batch Training Loss =  0.013553347438573837\n",
            "Batch Training Loss =  0.01589045114815235\n",
            "Batch Training Loss =  0.010432901792228222\n",
            "Batch Training Loss =  0.012050415389239788\n",
            "Batch Training Loss =  0.010951748117804527\n",
            "Batch Training Loss =  0.011592690832912922\n",
            "Batch Training Loss =  0.010117869824171066\n",
            "Batch Training Loss =  0.014809828251600266\n",
            "Batch Training Loss =  0.008920945227146149\n",
            "Batch Training Loss =  0.012666434049606323\n",
            "Validation Loss in this epoch is 0.411\n",
            "This is  90 th epoch\n",
            "Batch Training Loss =  0.011021401733160019\n",
            "Batch Training Loss =  0.013318326324224472\n",
            "Batch Training Loss =  0.03403543308377266\n",
            "Batch Training Loss =  0.020235221832990646\n",
            "Batch Training Loss =  0.017768219113349915\n",
            "Batch Training Loss =  0.00946059450507164\n",
            "Batch Training Loss =  0.010599273256957531\n",
            "Batch Training Loss =  0.007594536058604717\n",
            "Batch Training Loss =  0.026465795934200287\n",
            "Batch Training Loss =  0.009766887873411179\n",
            "Batch Training Loss =  0.011894799768924713\n",
            "Batch Training Loss =  0.013572623953223228\n",
            "Batch Training Loss =  0.014240686781704426\n",
            "Batch Training Loss =  0.012068872340023518\n",
            "Batch Training Loss =  0.013671261258423328\n",
            "Batch Training Loss =  0.01369868777692318\n",
            "Batch Training Loss =  0.009118347428739071\n",
            "Batch Training Loss =  0.01116978283971548\n",
            "Batch Training Loss =  0.016966218128800392\n",
            "Batch Training Loss =  0.010190422646701336\n",
            "Batch Training Loss =  0.010894324630498886\n",
            "Batch Training Loss =  0.008689219132065773\n",
            "Batch Training Loss =  0.01150913629680872\n",
            "Batch Training Loss =  0.041434600949287415\n",
            "Batch Training Loss =  0.03813618794083595\n",
            "Batch Training Loss =  0.026892727240920067\n",
            "Batch Training Loss =  0.013397583737969398\n",
            "Batch Training Loss =  0.018139267340302467\n",
            "Batch Training Loss =  0.015053683891892433\n",
            "Batch Training Loss =  0.009267359040677547\n",
            "Batch Training Loss =  0.02050916478037834\n",
            "Batch Training Loss =  0.01196400634944439\n",
            "Validation Loss in this epoch is 0.410\n",
            "This is  91 th epoch\n",
            "Batch Training Loss =  0.011681297793984413\n",
            "Batch Training Loss =  0.012004884891211987\n",
            "Batch Training Loss =  0.035008836537599564\n",
            "Batch Training Loss =  0.02184874378144741\n",
            "Batch Training Loss =  0.018600819632411003\n",
            "Batch Training Loss =  0.02313457988202572\n",
            "Batch Training Loss =  0.01004959736019373\n",
            "Batch Training Loss =  0.00997642707079649\n",
            "Batch Training Loss =  0.009343371726572514\n",
            "Batch Training Loss =  0.011797858402132988\n",
            "Batch Training Loss =  0.024717288091778755\n",
            "Batch Training Loss =  0.013427022844552994\n",
            "Batch Training Loss =  0.011463245376944542\n",
            "Batch Training Loss =  0.012398443184792995\n",
            "Batch Training Loss =  0.014207902364432812\n",
            "Batch Training Loss =  0.014437003992497921\n",
            "Batch Training Loss =  0.00796458963304758\n",
            "Batch Training Loss =  0.012756641954183578\n",
            "Batch Training Loss =  0.009366011247038841\n",
            "Batch Training Loss =  0.03143267333507538\n",
            "Batch Training Loss =  0.011956172995269299\n",
            "Batch Training Loss =  0.011131025850772858\n",
            "Batch Training Loss =  0.012558014132082462\n",
            "Batch Training Loss =  0.010884113609790802\n",
            "Batch Training Loss =  0.013624122366309166\n",
            "Batch Training Loss =  0.011042325757443905\n",
            "Batch Training Loss =  0.008322197012603283\n",
            "Batch Training Loss =  0.016039062291383743\n",
            "Batch Training Loss =  0.009393484331667423\n",
            "Batch Training Loss =  0.020393243059515953\n",
            "Batch Training Loss =  0.024752670899033546\n",
            "Batch Training Loss =  0.018413683399558067\n",
            "Validation Loss in this epoch is 0.407\n",
            "This is  92 th epoch\n",
            "Batch Training Loss =  0.010436867363750935\n",
            "Batch Training Loss =  0.017994865775108337\n",
            "Batch Training Loss =  0.01043923944234848\n",
            "Batch Training Loss =  0.00843420997262001\n",
            "Batch Training Loss =  0.006420586258172989\n",
            "Batch Training Loss =  0.011099901981651783\n",
            "Batch Training Loss =  0.011275026015937328\n",
            "Batch Training Loss =  0.009493480436503887\n",
            "Batch Training Loss =  0.01087167114019394\n",
            "Batch Training Loss =  0.021458810195326805\n",
            "Batch Training Loss =  0.020499883219599724\n",
            "Batch Training Loss =  0.013476811349391937\n",
            "Batch Training Loss =  0.011216764338314533\n",
            "Batch Training Loss =  0.014553963206708431\n",
            "Batch Training Loss =  0.02289954200387001\n",
            "Batch Training Loss =  0.021204641088843346\n",
            "Batch Training Loss =  0.01264110580086708\n",
            "Batch Training Loss =  0.010276011191308498\n",
            "Batch Training Loss =  0.007798769976943731\n",
            "Batch Training Loss =  0.013183138333261013\n",
            "Batch Training Loss =  0.013533464632928371\n",
            "Batch Training Loss =  0.009762028232216835\n",
            "Batch Training Loss =  0.012726832181215286\n",
            "Batch Training Loss =  0.03875692933797836\n",
            "Batch Training Loss =  0.021548444405198097\n",
            "Batch Training Loss =  0.010824921540915966\n",
            "Batch Training Loss =  0.015402443706989288\n",
            "Batch Training Loss =  0.011639906093478203\n",
            "Batch Training Loss =  0.014969920739531517\n",
            "Batch Training Loss =  0.010770704597234726\n",
            "Batch Training Loss =  0.01123877428472042\n",
            "Batch Training Loss =  0.012352810241281986\n",
            "Validation Loss in this epoch is 0.407\n",
            "This is  93 th epoch\n",
            "Batch Training Loss =  0.01257286686450243\n",
            "Batch Training Loss =  0.011306338012218475\n",
            "Batch Training Loss =  0.01306054275482893\n",
            "Batch Training Loss =  0.029334312304854393\n",
            "Batch Training Loss =  0.017576102167367935\n",
            "Batch Training Loss =  0.011564705520868301\n",
            "Batch Training Loss =  0.01290524285286665\n",
            "Batch Training Loss =  0.021207170560956\n",
            "Batch Training Loss =  0.01201225258409977\n",
            "Batch Training Loss =  0.007602101191878319\n",
            "Batch Training Loss =  0.013607796281576157\n",
            "Batch Training Loss =  0.010823591612279415\n",
            "Batch Training Loss =  0.01193249225616455\n",
            "Batch Training Loss =  0.03153897821903229\n",
            "Batch Training Loss =  0.013422558084130287\n",
            "Batch Training Loss =  0.019692372530698776\n",
            "Batch Training Loss =  0.015243802219629288\n",
            "Batch Training Loss =  0.011249475181102753\n",
            "Batch Training Loss =  0.012860073707997799\n",
            "Batch Training Loss =  0.013294577598571777\n",
            "Batch Training Loss =  0.025730283930897713\n",
            "Batch Training Loss =  0.034128423780202866\n",
            "Batch Training Loss =  0.02183370850980282\n",
            "Batch Training Loss =  0.0075652278028428555\n",
            "Batch Training Loss =  0.008580673485994339\n",
            "Batch Training Loss =  0.018636351451277733\n",
            "Batch Training Loss =  0.015154603868722916\n",
            "Batch Training Loss =  0.00874421652406454\n",
            "Batch Training Loss =  0.011160674504935741\n",
            "Batch Training Loss =  0.014965406619012356\n",
            "Batch Training Loss =  0.012538351118564606\n",
            "Batch Training Loss =  0.011780690401792526\n",
            "Validation Loss in this epoch is 0.406\n",
            "This is  94 th epoch\n",
            "Batch Training Loss =  0.00935868825763464\n",
            "Batch Training Loss =  0.008381977677345276\n",
            "Batch Training Loss =  0.01204906776547432\n",
            "Batch Training Loss =  0.01321828830987215\n",
            "Batch Training Loss =  0.029826821759343147\n",
            "Batch Training Loss =  0.018086887896060944\n",
            "Batch Training Loss =  0.012008829042315483\n",
            "Batch Training Loss =  0.01114779431372881\n",
            "Batch Training Loss =  0.014310543425381184\n",
            "Batch Training Loss =  0.012067045085132122\n",
            "Batch Training Loss =  0.01290930062532425\n",
            "Batch Training Loss =  0.012189694680273533\n",
            "Batch Training Loss =  0.013571729883551598\n",
            "Batch Training Loss =  0.008184836246073246\n",
            "Batch Training Loss =  0.010832201689481735\n",
            "Batch Training Loss =  0.02428889460861683\n",
            "Batch Training Loss =  0.012792975641787052\n",
            "Batch Training Loss =  0.016091009601950645\n",
            "Batch Training Loss =  0.03232024237513542\n",
            "Batch Training Loss =  0.021838152781128883\n",
            "Batch Training Loss =  0.011333073489367962\n",
            "Batch Training Loss =  0.014305943623185158\n",
            "Batch Training Loss =  0.01438203826546669\n",
            "Batch Training Loss =  0.01799933798611164\n",
            "Batch Training Loss =  0.010850383900105953\n",
            "Batch Training Loss =  0.0115400031208992\n",
            "Batch Training Loss =  0.012483973056077957\n",
            "Batch Training Loss =  0.014036007225513458\n",
            "Batch Training Loss =  0.0154899712651968\n",
            "Batch Training Loss =  0.011487466283142567\n",
            "Batch Training Loss =  0.009876767173409462\n",
            "Batch Training Loss =  0.012275227345526218\n",
            "Validation Loss in this epoch is 0.391\n",
            "This is  95 th epoch\n",
            "Batch Training Loss =  0.015904150903224945\n",
            "Batch Training Loss =  0.051492661237716675\n",
            "Batch Training Loss =  0.04028988257050514\n",
            "Batch Training Loss =  0.013710219413042068\n",
            "Batch Training Loss =  0.01259544212371111\n",
            "Batch Training Loss =  0.014715131372213364\n",
            "Batch Training Loss =  0.014038747176527977\n",
            "Batch Training Loss =  0.0111019816249609\n",
            "Batch Training Loss =  0.007616574876010418\n",
            "Batch Training Loss =  0.012060858309268951\n",
            "Batch Training Loss =  0.00947767123579979\n",
            "Batch Training Loss =  0.01220729574561119\n",
            "Batch Training Loss =  0.013886465691030025\n",
            "Batch Training Loss =  0.012236467562615871\n",
            "Batch Training Loss =  0.013715649023652077\n",
            "Batch Training Loss =  0.011708215810358524\n",
            "Batch Training Loss =  0.009456031955778599\n",
            "Batch Training Loss =  0.008831102401018143\n",
            "Batch Training Loss =  0.010419676080346107\n",
            "Batch Training Loss =  0.010629825294017792\n",
            "Batch Training Loss =  0.014655190519988537\n",
            "Batch Training Loss =  0.039059557020664215\n",
            "Batch Training Loss =  0.03532331809401512\n",
            "Batch Training Loss =  0.0167473703622818\n",
            "Batch Training Loss =  0.01431180164217949\n",
            "Batch Training Loss =  0.00979879405349493\n",
            "Batch Training Loss =  0.0182149950414896\n",
            "Batch Training Loss =  0.012951238080859184\n",
            "Batch Training Loss =  0.009964540600776672\n",
            "Batch Training Loss =  0.016498247161507607\n",
            "Batch Training Loss =  0.02356606163084507\n",
            "Batch Training Loss =  0.011531977914273739\n",
            "Validation Loss in this epoch is 0.392\n",
            "This is  96 th epoch\n",
            "Batch Training Loss =  0.020955633372068405\n",
            "Batch Training Loss =  0.006667316425591707\n",
            "Batch Training Loss =  0.009525281377136707\n",
            "Batch Training Loss =  0.009945506229996681\n",
            "Batch Training Loss =  0.037942998111248016\n",
            "Batch Training Loss =  0.024612171575427055\n",
            "Batch Training Loss =  0.01877754181623459\n",
            "Batch Training Loss =  0.01959572173655033\n",
            "Batch Training Loss =  0.009357796981930733\n",
            "Batch Training Loss =  0.013790416531264782\n",
            "Batch Training Loss =  0.012084641493856907\n",
            "Batch Training Loss =  0.010067387484014034\n",
            "Batch Training Loss =  0.0210551954805851\n",
            "Batch Training Loss =  0.018193276599049568\n",
            "Batch Training Loss =  0.0131275225430727\n",
            "Batch Training Loss =  0.012921581044793129\n",
            "Batch Training Loss =  0.014476334676146507\n",
            "Batch Training Loss =  0.010935690253973007\n",
            "Batch Training Loss =  0.013340010307729244\n",
            "Batch Training Loss =  0.012287856079638004\n",
            "Batch Training Loss =  0.018330330029129982\n",
            "Batch Training Loss =  0.016032986342906952\n",
            "Batch Training Loss =  0.04530460387468338\n",
            "Batch Training Loss =  0.017994869500398636\n",
            "Batch Training Loss =  0.024580668658018112\n",
            "Batch Training Loss =  0.017122376710176468\n",
            "Batch Training Loss =  0.017334245145320892\n",
            "Batch Training Loss =  0.013032013550400734\n",
            "Batch Training Loss =  0.00933865923434496\n",
            "Batch Training Loss =  0.015575157478451729\n",
            "Batch Training Loss =  0.017713332548737526\n",
            "Batch Training Loss =  0.014829647727310658\n",
            "Validation Loss in this epoch is 0.381\n",
            "This is  97 th epoch\n",
            "Batch Training Loss =  0.009252762421965599\n",
            "Batch Training Loss =  0.013237614184617996\n",
            "Batch Training Loss =  0.008712041191756725\n",
            "Batch Training Loss =  0.028770243749022484\n",
            "Batch Training Loss =  0.01876951940357685\n",
            "Batch Training Loss =  0.027664175257086754\n",
            "Batch Training Loss =  0.012944178655743599\n",
            "Batch Training Loss =  0.010725497268140316\n",
            "Batch Training Loss =  0.021200044080615044\n",
            "Batch Training Loss =  0.013426577672362328\n",
            "Batch Training Loss =  0.009813383221626282\n",
            "Batch Training Loss =  0.011948726139962673\n",
            "Batch Training Loss =  0.008337673731148243\n",
            "Batch Training Loss =  0.014152029529213905\n",
            "Batch Training Loss =  0.013064313679933548\n",
            "Batch Training Loss =  0.04572305083274841\n",
            "Batch Training Loss =  0.025304147973656654\n",
            "Batch Training Loss =  0.016268258914351463\n",
            "Batch Training Loss =  0.01129740010946989\n",
            "Batch Training Loss =  0.01859518513083458\n",
            "Batch Training Loss =  0.015422279946506023\n",
            "Batch Training Loss =  0.011814235709607601\n",
            "Batch Training Loss =  0.011468485929071903\n",
            "Batch Training Loss =  0.016601281240582466\n",
            "Batch Training Loss =  0.012947705574333668\n",
            "Batch Training Loss =  0.016914231702685356\n",
            "Batch Training Loss =  0.010627645999193192\n",
            "Batch Training Loss =  0.019896341487765312\n",
            "Batch Training Loss =  0.011868116445839405\n",
            "Batch Training Loss =  0.011519058607518673\n",
            "Batch Training Loss =  0.009116621688008308\n",
            "Batch Training Loss =  0.011431582272052765\n",
            "Validation Loss in this epoch is 0.410\n",
            "This is  98 th epoch\n",
            "Batch Training Loss =  0.006593427620828152\n",
            "Batch Training Loss =  0.008876343257725239\n",
            "Batch Training Loss =  0.16845273971557617\n",
            "Batch Training Loss =  0.41612017154693604\n",
            "Batch Training Loss =  2.813816547393799\n",
            "Batch Training Loss =  2.3374111652374268\n",
            "Batch Training Loss =  0.4255341589450836\n",
            "Batch Training Loss =  0.4538913369178772\n",
            "Batch Training Loss =  0.4605697989463806\n",
            "Batch Training Loss =  0.4803546965122223\n",
            "Batch Training Loss =  0.34355485439300537\n",
            "Batch Training Loss =  0.4146759510040283\n",
            "Batch Training Loss =  0.36393436789512634\n",
            "Batch Training Loss =  0.2883031964302063\n",
            "Batch Training Loss =  0.28102976083755493\n",
            "Batch Training Loss =  0.3883008062839508\n",
            "Batch Training Loss =  0.28622233867645264\n",
            "Batch Training Loss =  0.29938262701034546\n",
            "Batch Training Loss =  0.25683706998825073\n",
            "Batch Training Loss =  0.2587089240550995\n",
            "Batch Training Loss =  0.20457546412944794\n",
            "Batch Training Loss =  0.15676218271255493\n",
            "Batch Training Loss =  0.17754420638084412\n",
            "Batch Training Loss =  0.13618044555187225\n",
            "Batch Training Loss =  0.19792167842388153\n",
            "Batch Training Loss =  0.13288091123104095\n",
            "Batch Training Loss =  0.10977521538734436\n",
            "Batch Training Loss =  0.1302018165588379\n",
            "Batch Training Loss =  0.21669751405715942\n",
            "Batch Training Loss =  0.12779554724693298\n",
            "Batch Training Loss =  0.09192966669797897\n",
            "Batch Training Loss =  0.13621334731578827\n",
            "Validation Loss in this epoch is 0.378\n",
            "This is  99 th epoch\n",
            "Batch Training Loss =  0.08206754922866821\n",
            "Batch Training Loss =  0.0639011561870575\n",
            "Batch Training Loss =  0.08141718804836273\n",
            "Batch Training Loss =  0.04167947918176651\n",
            "Batch Training Loss =  0.034775201231241226\n",
            "Batch Training Loss =  0.056361034512519836\n",
            "Batch Training Loss =  0.1389949470758438\n",
            "Batch Training Loss =  0.033311452716588974\n",
            "Batch Training Loss =  0.046180181205272675\n",
            "Batch Training Loss =  0.033964306116104126\n",
            "Batch Training Loss =  0.09120409935712814\n",
            "Batch Training Loss =  0.07956136018037796\n",
            "Batch Training Loss =  0.04196218401193619\n",
            "Batch Training Loss =  0.10973858088254929\n",
            "Batch Training Loss =  0.04675307124853134\n",
            "Batch Training Loss =  0.032288212329149246\n",
            "Batch Training Loss =  0.033307068049907684\n",
            "Batch Training Loss =  0.04766497015953064\n",
            "Batch Training Loss =  0.030784644186496735\n",
            "Batch Training Loss =  0.02434026449918747\n",
            "Batch Training Loss =  0.059883736073970795\n",
            "Batch Training Loss =  0.05399550870060921\n",
            "Batch Training Loss =  0.04485233128070831\n",
            "Batch Training Loss =  0.0283823125064373\n",
            "Batch Training Loss =  0.026183325797319412\n",
            "Batch Training Loss =  0.04072088375687599\n",
            "Batch Training Loss =  0.04288787767291069\n",
            "Batch Training Loss =  0.02607477270066738\n",
            "Batch Training Loss =  0.027993105351924896\n",
            "Batch Training Loss =  0.022388571873307228\n",
            "Batch Training Loss =  0.03642355650663376\n",
            "Batch Training Loss =  0.08902334421873093\n",
            "Validation Loss in this epoch is 0.384\n",
            "This is  100 th epoch\n",
            "Batch Training Loss =  0.031572118401527405\n",
            "Batch Training Loss =  0.01834288239479065\n",
            "Batch Training Loss =  0.012809383682906628\n",
            "Batch Training Loss =  0.014087698422372341\n",
            "Batch Training Loss =  0.017081143334507942\n",
            "Batch Training Loss =  0.01928945817053318\n",
            "Batch Training Loss =  0.0546441413462162\n",
            "Batch Training Loss =  0.037789348512887955\n",
            "Batch Training Loss =  0.01637747325003147\n",
            "Batch Training Loss =  0.03175920248031616\n",
            "Batch Training Loss =  0.03062114678323269\n",
            "Batch Training Loss =  0.017075886949896812\n",
            "Batch Training Loss =  0.027035627514123917\n",
            "Batch Training Loss =  0.06201058253645897\n",
            "Batch Training Loss =  0.02255551517009735\n",
            "Batch Training Loss =  0.023521672934293747\n",
            "Batch Training Loss =  0.03916861489415169\n",
            "Batch Training Loss =  0.018138563260436058\n",
            "Batch Training Loss =  0.01966584287583828\n",
            "Batch Training Loss =  0.027686087414622307\n",
            "Batch Training Loss =  0.022576842457056046\n",
            "Batch Training Loss =  0.04129525646567345\n",
            "Batch Training Loss =  0.01849464513361454\n",
            "Batch Training Loss =  0.020964240655303\n",
            "Batch Training Loss =  0.01481559406965971\n",
            "Batch Training Loss =  0.0251191146671772\n",
            "Batch Training Loss =  0.0570354163646698\n",
            "Batch Training Loss =  0.014976383186876774\n",
            "Batch Training Loss =  0.020562157034873962\n",
            "Batch Training Loss =  0.02762582153081894\n",
            "Batch Training Loss =  0.020713675767183304\n",
            "Batch Training Loss =  0.024302855134010315\n",
            "Validation Loss in this epoch is 0.413\n"
          ]
        }
      ],
      "source": [
        "# TODO [3 points]: Perform cross-validation to get train/val accuracy\n",
        "# for all hyper-parameter settings in the list below.\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "weight_decays = [0., 0.01]\n",
        "batch_size = 50\n",
        "n_epochs = 100\n",
        "n_folds = 5\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        val_accs = []  # store validation accuracy for each fold\n",
        "        train_accs = []  # store training accuracy for each fold\n",
        "\n",
        "        # TODO: iterate over folds, remember to use \"shuffle=True\", as datapoints are not shuffled\n",
        "\n",
        "        myKFold = KFold(n_splits = n_folds, shuffle = True, random_state = 0)\n",
        "\n",
        "        # TODO: Split data into train and validation\n",
        "        for trainIndex, valIndex in myKFold.split(X):\n",
        "\n",
        "                # TODO: Create data loaders to pass to training loop\n",
        "                XTrain, XVal = X[trainIndex], X[valIndex]\n",
        "                yTrain, yVal = y[trainIndex], y[valIndex]\n",
        "\n",
        "\n",
        "                trainLoader = DataLoader(TensorDataset(XTrain, yTrain), batch_size = batch_size,shuffle = True)\n",
        "                valLoader = DataLoader(TensorDataset(XVal, yVal), batch_size = len(XVal))\n",
        "\n",
        "                # TODO: Initialize model, criterion (Cross entropy loss), and optimizer (SGD with various hyperparameters)\n",
        "                model = MyMLP()\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = torch.optim.SGD(model.parameters(), lr = lr, weight_decay=wd)\n",
        "\n",
        "                # Call your training function\n",
        "                train(model, trainLoader, valLoader, n_epochs, optimizer, criterion, verbose=True)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # TODO: Use the trained model to estimate train/val accuracy\n",
        "                    # (Hint: our model outputs logits, argmax is good to get the class prediction corresponding to max logit)\n",
        "                    yPredTrain = model(XTrain).argmax(dim = 1)\n",
        "                    yPredVal = model(XVal).argmax(dim = 1)\n",
        "\n",
        "                    train_acc = accuracy_score(yPredTrain, yTrain)\n",
        "                    train_accs.append(train_acc)\n",
        "\n",
        "                    val_acc = accuracy_score(yPredVal, yVal)\n",
        "                    val_accs.append(val_acc)\n",
        "\n",
        "        # For each hyper-parameter, I'm storing the parameter values and the mean and standard error of accuracy in a list in \"results\".\n",
        "        train_std, train_mean = torch.std_mean(torch.tensor(train_accs))\n",
        "        val_std, val_mean = torch.std_mean(torch.tensor(val_accs))\n",
        "        rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds\n",
        "        train_se, val_se = train_std / rootn, val_std / rootn\n",
        "        # Storing learning rate, weight decay value, train mean accuracy, standard error, val mean accuracy, standard error\n",
        "        results.append((lr, wd, train_mean.item(), train_se.item(), val_mean.item(), val_se.item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eNw9Fimx-KX"
      },
      "source": [
        "## Show result [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "L_RBvwQ4x-KX",
        "outputId": "a81441e6-117f-4e05-960f-e3715338dfcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training results\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007071067811865475,\n        \"min\": 0.0,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.814 +/- 0.004\",\n          \"0.811 +/- 0.006\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.999 +/- 0.000\",\n          \"1.000 +/- 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.998 +/- 0.001\",\n          \"1.000 +/- 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "pivot_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f759f6f0-f0c2-4547-b958-be183e97d542\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.811 +/- 0.006</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>0.814 +/- 0.004</td>\n",
              "      <td>0.999 +/- 0.000</td>\n",
              "      <td>0.998 +/- 0.001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f759f6f0-f0c2-4547-b958-be183e97d542')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f759f6f0-f0c2-4547-b958-be183e97d542 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f759f6f0-f0c2-4547-b958-be183e97d542');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5d6a18a9-9bb7-410f-84f7-151a4181831c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5d6a18a9-9bb7-410f-84f7-151a4181831c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5d6a18a9-9bb7-410f-84f7-151a4181831c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_828835c9-1858-4173-a620-eb8d068b248c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_828835c9-1858-4173-a620-eb8d068b248c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           0.811 +/- 0.006  1.000 +/- 0.000  1.000 +/- 0.000\n",
              "0.01           0.814 +/- 0.004  0.999 +/- 0.000  0.998 +/- 0.001"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation results\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007071067811865475,\n        \"min\": 0.0,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.771 +/- 0.010\",\n          \"0.768 +/- 0.008\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.809 +/- 0.009\",\n          \"0.818 +/- 0.006\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.824 +/- 0.004\",\n          \"0.825 +/- 0.008\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "pivot_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-764799d8-27e4-47b5-afa3-05fa4e4a6ec0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.768 +/- 0.008</td>\n",
              "      <td>0.818 +/- 0.006</td>\n",
              "      <td>0.825 +/- 0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>0.771 +/- 0.010</td>\n",
              "      <td>0.809 +/- 0.009</td>\n",
              "      <td>0.824 +/- 0.004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-764799d8-27e4-47b5-afa3-05fa4e4a6ec0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-764799d8-27e4-47b5-afa3-05fa4e4a6ec0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-764799d8-27e4-47b5-afa3-05fa4e4a6ec0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b3343067-4a9e-464f-8b70-a39f64515950\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b3343067-4a9e-464f-8b70-a39f64515950')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b3343067-4a9e-464f-8b70-a39f64515950 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_eb334f68-283e-4f22-968e-08ae34e8fe14\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_eb334f68-283e-4f22-968e-08ae34e8fe14 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           0.768 +/- 0.008  0.818 +/- 0.006  0.825 +/- 0.008\n",
              "0.01           0.771 +/- 0.010  0.809 +/- 0.009  0.824 +/- 0.004"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO [3 points]. Print the final result (should be no need to modify code)\n",
        "# You should be able to see a best train acc > 95% , and a best val acc > 80%\n",
        "\n",
        "# Create a DataFrame from the list of tuples, with labeled columns\n",
        "column_names = ['learning_rate', 'weight_decay', 'train_mean', 'train_se','val_mean', 'val_se']\n",
        "df = pd.DataFrame(results, columns=column_names)\n",
        "\n",
        "# Make pretty printable strings, with standard error bars\n",
        "df['train_output'] = df.apply(lambda row: f\"{row['train_mean']:.3f} +/- {row['train_se']:.3f}\", axis=1)\n",
        "df['val_output'] = df.apply(lambda row: f\"{row['val_mean']:.3f} +/- {row['val_se']:.3f}\", axis=1)\n",
        "\n",
        "print('Training results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='train_output')\n",
        "display(pivot_df)\n",
        "\n",
        "print('Validation results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='val_output')\n",
        "display(pivot_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu7nxboCx-KX"
      },
      "source": [
        "## Extra credit\n",
        "\n",
        "There are some nice opportunities for extra credit, though I will be fairly stingy with the points, so you should only try it if you're interested in learning more.\n",
        "Some examples of things you could try for 1 extra point.\n",
        "- Use t-SNE or UMAP to visualize a 2-d embedding of all the points, and see if the real and fake images are separable in the 2-d space.\n",
        "- Use a more complex vision backbone like a pretrained ResNet to first embed the images, then train your MLP. You'll have to be careful to transform the images before input into a ResNet, as they usually expect a specific resolution. You can use torchvision transforms library for this. Does this increase accuracy? I don't know, but I speculate it won't help much - these embeddings are trained for classification accuracy, so they have no reason to preserve differences that are useful for finding fakes.\n",
        "- Train a more complex vision backbone, instead of using the MLP. Again, a ResNet or a small vision transformer would be interesting. I think this would be the most typical and effective approach.\n",
        "- Being an expert at hyper-parameter tuning is a skill that will benefit you greatly. Try a more fancy way to do this, like https://docs.wandb.ai, and see how well you can do on this assigment if you also vary other hyper-parameters (architecture, n_epochs, maybe early stopping, more learning rate/weight decay settings, regularizers, etc.)\n",
        "\n",
        "Of course you can train your deepfake detector on my fakes, but how well will it do on ones from some other system? This is the fundamental research question in that field - how to build robust detectors that will work well even on new image generators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVUMBmSALXan"
      },
      "source": [
        "## Extra Credit 1 - Visualizing using t-SNE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "xaSvp028L-zo",
        "outputId": "95c81bc9-c995-459b-a37d-c89be63440ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shapes before flattening:\n",
            "X: torch.Size([2000, 3, 32, 32])\n",
            "y: torch.Size([2000])\n",
            "X shape after flattening: torch.Size([2000, 3072])\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Reload the dataset as variable names may have been utilized elsewhere\n",
        "# Referenced this website - https://builtin.com/data-science/tsne-python\n",
        "\n",
        "X, y = torch.load('/content/drive/MyDrive/CS224-FunadamentalsOfMachineLearning/HW2-DeepFakeCatDetector/hw2_data.pt')\n",
        "\n",
        "print('Data shapes before flattening:')\n",
        "print('X:', X.shape)  # 2000, 3, 32, 32, 2000 images, channel, height width\n",
        "print('y:', y.shape)  # 2000 binary labels 0 is real, 1 is fake\n",
        "\n",
        "# Let's print some examples from each class\n",
        "grid = vutils.make_grid(X[y==0][:8], nrow=4, padding=2, normalize=True)\n",
        "fig, axs = plt.subplots(2, 1, figsize=(8, 8))\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('REAL Cat images')\n",
        "axs[0].imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "grid = vutils.make_grid(X[y==1][:8], nrow=4, padding=2, normalize=True)\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('FAKE Cat images')\n",
        "axs[1].imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "X = X.flatten(start_dim=1)  # From now on, we work with the flattened vector\n",
        "print(f\"X shape after flattening: {X.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axa2cNINJQo9",
        "outputId": "a7bc0b63-f527-4d5a-ad33-db427f24e15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the dataframe: (2000, 3074)\n"
          ]
        }
      ],
      "source": [
        "# Convert data into a pandas dataframe\n",
        "\n",
        "feat_cols = [ 'pixel' + str(i) for i in range(X.shape[1]) ]\n",
        "\n",
        "df = pd.DataFrame(X, columns = feat_cols)\n",
        "df['y'] = y\n",
        "df['label'] = df['y'].apply(lambda i: str(i))\n",
        "\n",
        "print('Size of the dataframe: {}'.format(df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "NyTpTLpDw-ay",
        "outputId": "5b4df4cd-52a7-4115-d3ab-669599423d0b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-93e4cb32-dc56-4f7c-bb9b-cb499c5a7bde\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel3066</th>\n",
              "      <th>pixel3067</th>\n",
              "      <th>pixel3068</th>\n",
              "      <th>pixel3069</th>\n",
              "      <th>pixel3070</th>\n",
              "      <th>pixel3071</th>\n",
              "      <th>y</th>\n",
              "      <th>label</th>\n",
              "      <th>tsne-2d-first</th>\n",
              "      <th>tsne-2d-second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.019608</td>\n",
              "      <td>-0.137255</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>-0.168627</td>\n",
              "      <td>-0.168627</td>\n",
              "      <td>0.105882</td>\n",
              "      <td>0.372549</td>\n",
              "      <td>0.372549</td>\n",
              "      <td>0.160784</td>\n",
              "      <td>-0.168627</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.411765</td>\n",
              "      <td>-0.380392</td>\n",
              "      <td>-0.364706</td>\n",
              "      <td>-0.356863</td>\n",
              "      <td>-0.341176</td>\n",
              "      <td>-0.325490</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.737267</td>\n",
              "      <td>3.158624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.137255</td>\n",
              "      <td>0.113726</td>\n",
              "      <td>0.184314</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>-0.349020</td>\n",
              "      <td>-0.568627</td>\n",
              "      <td>-0.623529</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>-0.592157</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.380392</td>\n",
              "      <td>-0.443137</td>\n",
              "      <td>-0.474510</td>\n",
              "      <td>-0.529412</td>\n",
              "      <td>-0.529412</td>\n",
              "      <td>-0.513726</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.125408</td>\n",
              "      <td>4.464776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.976471</td>\n",
              "      <td>0.952941</td>\n",
              "      <td>0.960784</td>\n",
              "      <td>0.968627</td>\n",
              "      <td>0.968627</td>\n",
              "      <td>0.952941</td>\n",
              "      <td>0.874510</td>\n",
              "      <td>0.945098</td>\n",
              "      <td>0.968627</td>\n",
              "      <td>0.968627</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.239216</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.050980</td>\n",
              "      <td>-0.113725</td>\n",
              "      <td>-0.145098</td>\n",
              "      <td>-0.309804</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.944373</td>\n",
              "      <td>2.424734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.027451</td>\n",
              "      <td>-0.027451</td>\n",
              "      <td>-0.090196</td>\n",
              "      <td>-0.027451</td>\n",
              "      <td>0.113726</td>\n",
              "      <td>0.247059</td>\n",
              "      <td>0.341177</td>\n",
              "      <td>0.364706</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.443137</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.960784</td>\n",
              "      <td>-0.866667</td>\n",
              "      <td>-0.631373</td>\n",
              "      <td>-0.458824</td>\n",
              "      <td>-0.427451</td>\n",
              "      <td>-0.427451</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.125390</td>\n",
              "      <td>2.433873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.945098</td>\n",
              "      <td>-0.945098</td>\n",
              "      <td>-0.960784</td>\n",
              "      <td>-0.945098</td>\n",
              "      <td>-0.913725</td>\n",
              "      <td>-0.913725</td>\n",
              "      <td>-0.866667</td>\n",
              "      <td>-0.803922</td>\n",
              "      <td>-0.858824</td>\n",
              "      <td>-0.929412</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.239216</td>\n",
              "      <td>-0.356863</td>\n",
              "      <td>-0.356863</td>\n",
              "      <td>-0.403922</td>\n",
              "      <td>-0.364706</td>\n",
              "      <td>-0.066667</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.494876</td>\n",
              "      <td>3.700543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.498039</td>\n",
              "      <td>-0.403922</td>\n",
              "      <td>-0.443137</td>\n",
              "      <td>-0.796078</td>\n",
              "      <td>-0.843137</td>\n",
              "      <td>-0.843137</td>\n",
              "      <td>-0.827451</td>\n",
              "      <td>-0.796078</td>\n",
              "      <td>-0.654902</td>\n",
              "      <td>...</td>\n",
              "      <td>0.388235</td>\n",
              "      <td>0.223529</td>\n",
              "      <td>-0.074510</td>\n",
              "      <td>-0.129412</td>\n",
              "      <td>0.121569</td>\n",
              "      <td>0.247059</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-4.299292</td>\n",
              "      <td>-1.552993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>-0.176471</td>\n",
              "      <td>0.105882</td>\n",
              "      <td>0.498039</td>\n",
              "      <td>0.184314</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.223529</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>-0.356863</td>\n",
              "      <td>-0.482353</td>\n",
              "      <td>-0.247059</td>\n",
              "      <td>...</td>\n",
              "      <td>0.937255</td>\n",
              "      <td>-0.192157</td>\n",
              "      <td>-0.686275</td>\n",
              "      <td>-0.749020</td>\n",
              "      <td>-0.858824</td>\n",
              "      <td>-0.764706</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.290282</td>\n",
              "      <td>0.892527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>-0.733333</td>\n",
              "      <td>-0.670588</td>\n",
              "      <td>-0.647059</td>\n",
              "      <td>-0.662745</td>\n",
              "      <td>-0.654902</td>\n",
              "      <td>-0.482353</td>\n",
              "      <td>-0.458824</td>\n",
              "      <td>-0.639216</td>\n",
              "      <td>-0.686275</td>\n",
              "      <td>-0.725490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.450980</td>\n",
              "      <td>-0.521569</td>\n",
              "      <td>-0.560784</td>\n",
              "      <td>-0.482353</td>\n",
              "      <td>-0.537255</td>\n",
              "      <td>-0.482353</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-4.479886</td>\n",
              "      <td>0.206603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>0.247059</td>\n",
              "      <td>0.254902</td>\n",
              "      <td>0.317647</td>\n",
              "      <td>0.239216</td>\n",
              "      <td>0.105882</td>\n",
              "      <td>0.403922</td>\n",
              "      <td>0.458824</td>\n",
              "      <td>0.403922</td>\n",
              "      <td>0.364706</td>\n",
              "      <td>0.396078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.435294</td>\n",
              "      <td>0.152941</td>\n",
              "      <td>-0.066667</td>\n",
              "      <td>-0.113725</td>\n",
              "      <td>-0.090196</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.020858</td>\n",
              "      <td>-1.507088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>-0.678431</td>\n",
              "      <td>-0.654902</td>\n",
              "      <td>-0.631373</td>\n",
              "      <td>-0.513726</td>\n",
              "      <td>-0.058824</td>\n",
              "      <td>-0.129412</td>\n",
              "      <td>-0.749020</td>\n",
              "      <td>-0.631373</td>\n",
              "      <td>-0.474510</td>\n",
              "      <td>-0.513726</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.796078</td>\n",
              "      <td>-0.662745</td>\n",
              "      <td>-0.866667</td>\n",
              "      <td>-0.584314</td>\n",
              "      <td>-0.490196</td>\n",
              "      <td>-0.435294</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.241427</td>\n",
              "      <td>-0.105877</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows  3076 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93e4cb32-dc56-4f7c-bb9b-cb499c5a7bde')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-93e4cb32-dc56-4f7c-bb9b-cb499c5a7bde button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-93e4cb32-dc56-4f7c-bb9b-cb499c5a7bde');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ffab5b4d-ffe4-4b40-a8b7-62708c4e6517\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ffab5b4d-ffe4-4b40-a8b7-62708c4e6517')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ffab5b4d-ffe4-4b40-a8b7-62708c4e6517 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9c3c1c7e-65c1-4a81-b8f3-3fcc99313070\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9c3c1c7e-65c1-4a81-b8f3-3fcc99313070 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        pixel0    pixel1    pixel2    pixel3    pixel4    pixel5    pixel6  \\\n",
              "0    -0.019608 -0.137255 -0.200000 -0.168627 -0.168627  0.105882  0.372549   \n",
              "1    -0.137255  0.113726  0.184314  0.043137 -0.349020 -0.568627 -0.623529   \n",
              "2     0.976471  0.952941  0.960784  0.968627  0.968627  0.952941  0.874510   \n",
              "3     0.027451 -0.027451 -0.090196 -0.027451  0.113726  0.247059  0.341177   \n",
              "4    -0.945098 -0.945098 -0.960784 -0.945098 -0.913725 -0.913725 -0.866667   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1995  0.866667  0.498039 -0.403922 -0.443137 -0.796078 -0.843137 -0.843137   \n",
              "1996 -0.176471  0.105882  0.498039  0.184314  0.058824  0.223529  0.058824   \n",
              "1997 -0.733333 -0.670588 -0.647059 -0.662745 -0.654902 -0.482353 -0.458824   \n",
              "1998  0.247059  0.254902  0.317647  0.239216  0.105882  0.403922  0.458824   \n",
              "1999 -0.678431 -0.654902 -0.631373 -0.513726 -0.058824 -0.129412 -0.749020   \n",
              "\n",
              "        pixel7    pixel8    pixel9  ...  pixel3066  pixel3067  pixel3068  \\\n",
              "0     0.372549  0.160784 -0.168627  ...  -0.411765  -0.380392  -0.364706   \n",
              "1    -0.600000 -0.600000 -0.592157  ...  -0.380392  -0.443137  -0.474510   \n",
              "2     0.945098  0.968627  0.968627  ...  -0.239216  -0.082353  -0.050980   \n",
              "3     0.364706  0.411765  0.443137  ...  -0.960784  -0.866667  -0.631373   \n",
              "4    -0.803922 -0.858824 -0.929412  ...  -0.239216  -0.356863  -0.356863   \n",
              "...        ...       ...       ...  ...        ...        ...        ...   \n",
              "1995 -0.827451 -0.796078 -0.654902  ...   0.388235   0.223529  -0.074510   \n",
              "1996 -0.356863 -0.482353 -0.247059  ...   0.937255  -0.192157  -0.686275   \n",
              "1997 -0.639216 -0.686275 -0.725490  ...  -0.450980  -0.521569  -0.560784   \n",
              "1998  0.403922  0.364706  0.396078  ...   0.529412   0.435294   0.152941   \n",
              "1999 -0.631373 -0.474510 -0.513726  ...  -0.796078  -0.662745  -0.866667   \n",
              "\n",
              "      pixel3069  pixel3070  pixel3071  y  label  tsne-2d-first  tsne-2d-second  \n",
              "0     -0.356863  -0.341176  -0.325490  0      0      -3.737267        3.158624  \n",
              "1     -0.529412  -0.529412  -0.513726  0      0      -1.125408        4.464776  \n",
              "2     -0.113725  -0.145098  -0.309804  0      0      -2.944373        2.424734  \n",
              "3     -0.458824  -0.427451  -0.427451  0      0      -3.125390        2.433873  \n",
              "4     -0.403922  -0.364706  -0.066667  0      0       0.494876        3.700543  \n",
              "...         ...        ...        ... ..    ...            ...             ...  \n",
              "1995  -0.129412   0.121569   0.247059  1      1      -4.299292       -1.552993  \n",
              "1996  -0.749020  -0.858824  -0.764706  1      1       1.290282        0.892527  \n",
              "1997  -0.482353  -0.537255  -0.482353  1      1      -4.479886        0.206603  \n",
              "1998  -0.066667  -0.113725  -0.090196  1      1       2.020858       -1.507088  \n",
              "1999  -0.584314  -0.490196  -0.435294  1      1      -0.241427       -0.105877  \n",
              "\n",
              "[2000 rows x 3076 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n948X4FdKHfs",
        "outputId": "695dd5eb-2e81-4c63-f8b3-6496ea4018af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[t-SNE] Computing 121 nearest neighbors...\n",
            "[t-SNE] Indexed 2000 samples in 0.002s...\n",
            "[t-SNE] Computed neighbors for 2000 samples in 0.805s...\n",
            "[t-SNE] Computed conditional probabilities for sample 1000 / 2000\n",
            "[t-SNE] Computed conditional probabilities for sample 2000 / 2000\n",
            "[t-SNE] Mean sigma: 7.352482\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 73.339935\n",
            "[t-SNE] KL divergence after 300 iterations: 2.498494\n",
            "t-SNE done! Time elapsed: 8.877891302108765 seconds\n"
          ]
        }
      ],
      "source": [
        "# Since we have only 2000 datapoints we can try to use them all for the T-Distributed Stochastic Neighbour Embedding (t-SNE)\n",
        "# Each sample has 3074 dimensions (RGB - 3, each one has 32 * 32 pixels)\n",
        "from sklearn.manifold import TSNE\n",
        "import time\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "# n_components = 2 as we want only 2 features for a 2-d plot.\n",
        "tsne = TSNE(n_components = 2, verbose = 1, perplexity = 40, n_iter = 300)\n",
        "tsne_results = tsne.fit_transform(df[feat_cols].values)\n",
        "\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fiB2DcENmAb",
        "outputId": "eae80b78-83a5-460a-c462-ba7195215101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 2)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tsne_results.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "567HGQfAKHjH",
        "outputId": "16efffd0-7f10-4929-8462-4587ec9b5de2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='tsne-2d-first', ylabel='tsne-2d-second'>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We have now reduced 3072 dimensional data to a 2-d embedding for all 2000 images\n",
        "# Scatter plot with color for real and fake images\n",
        "\n",
        "# Use seaborn for scatterplot\n",
        "import seaborn\n",
        "\n",
        "# Add tsne resulting dimensions to the dataframe as new columns\n",
        "df['tsne-2d-first'] = tsne_results[:,0]\n",
        "df['tsne-2d-second'] = tsne_results[:,1]\n",
        "\n",
        "# Create a seaborn plot\n",
        "plt.figure(figsize = (8,5))\n",
        "seaborn.scatterplot(\n",
        "    x = \"tsne-2d-first\", y = \"tsne-2d-second\",\n",
        "    hue = \"y\",\n",
        "    palette = seaborn.color_palette(\"hls\", 2), # two colors for 2 classes - real and fake\n",
        "    data = df,\n",
        "    legend = \"full\",\n",
        "    alpha = 0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfTU0XEWOsYS"
      },
      "source": [
        "***t-SNE Observation***\n",
        "\n",
        "As we can see from the above plot, y = 0 class which are the real cat images are kind of concentrated to the left and right points and spread out like triangles toward the center. And as we move towards the center the density decreases.\n",
        "\n",
        "And y = 1 class, which are the fake images are mostly in the center. Though there are some towards the corners too, but with a much lower density."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-e86EcvLf9U"
      },
      "source": [
        "## Extra Credit 2 - Using ResNet (18) to embed images and then train using my MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFneyOWbL-MY",
        "outputId": "1a9319f2-7e17-4db0-c767-2ecafbd5d68c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|| 44.7M/44.7M [00:00<00:00, 83.1MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# I am using ResNet 18\n",
        "# Referred https://pytorch.org/hub/pytorch_vision_resnet/ and\n",
        "# https://pytorch.org/vision/stable/transforms.html\n",
        "\n",
        "resNetModel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "\n",
        "# Use eval mode as we are not using the resNetModel for training\n",
        "resNetModel.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dvCPTs3RP-s",
        "outputId": "d5d1cb84-5f82-4efb-985d-2d30068d446f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Shape of X before transformation was  torch.Size([2000, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W),\n",
        "# where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using\n",
        "# mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
        "\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "X, y = torch.load('/content/drive/MyDrive/CS224-FunadamentalsOfMachineLearning/HW2-DeepFakeCatDetector/hw2_data.pt')\n",
        "\n",
        "def transformImgTensorForResnet(inputImageTensor):\n",
        "    transforms = v2.Compose([\n",
        "        v2.CenterCrop(size = 224),    # pad with zeroes around the center as image was 32 x 32 and now it is 224 x 224\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return transforms(inputImageTensor)\n",
        "\n",
        "print (\"Shape of X before transformation was \", X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TamYrW0tXbJr",
        "outputId": "fc002031-0149-4c6b-c5f0-5c15f161605f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X after transformation is  torch.Size([2000, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "# Transform the images as required by resnet\n",
        "\n",
        "X = transformImgTensorForResnet(X)\n",
        "print (\"Shape of X after transformation is \", X.shape)\n",
        "\n",
        "# RAM exceeding for embeddding, so trying to use GPU\n",
        "if torch.cuda.is_available():\n",
        "    X = X.to('cuda')\n",
        "    resNetModel.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_smwNyTRQAj"
      },
      "outputs": [],
      "source": [
        "# A function to generate embeddings using a given model\n",
        "\n",
        "def genEmbeddings(input_batch, model = resNetModel):\n",
        "    with torch.no_grad():\n",
        "        output = model(input_batch)\n",
        "    # print(output[0])\n",
        "    # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "    # probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    # # print(probabilities)\n",
        "    # print(probabilities.shape)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA-KrK23X8lY"
      },
      "outputs": [],
      "source": [
        "# Generate the embeddings using ResNet 18\n",
        "X = genEmbeddings(X, resNetModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17cjNrC6gCNG",
        "outputId": "b3f9d9de-5639-4152-eb44-0c19c9f995b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2000, 1000])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv2msqnLw8ee"
      },
      "outputs": [],
      "source": [
        "# Transfer embeddings back to CPU\n",
        "X = X.detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnFcNqXXvll6",
        "outputId": "844afb31-e8d5-4812-d425-e5b9f302ae49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch Training Loss =  0.6746610999107361\n",
            "Validation Loss in this epoch is 0.634\n",
            "This is  54 th epoch\n",
            "Batch Training Loss =  0.6403040289878845\n",
            "Batch Training Loss =  0.42420709133148193\n",
            "Batch Training Loss =  0.7560130953788757\n",
            "Batch Training Loss =  0.7024860978126526\n",
            "Batch Training Loss =  0.652995228767395\n",
            "Batch Training Loss =  0.3724589943885803\n",
            "Batch Training Loss =  0.36517617106437683\n",
            "Batch Training Loss =  0.3979174494743347\n",
            "Batch Training Loss =  0.5190121531486511\n",
            "Batch Training Loss =  0.5483348965644836\n",
            "Batch Training Loss =  0.4080445468425751\n",
            "Batch Training Loss =  0.432442843914032\n",
            "Batch Training Loss =  0.43919411301612854\n",
            "Batch Training Loss =  0.37716931104660034\n",
            "Batch Training Loss =  0.4033195376396179\n",
            "Batch Training Loss =  0.3891856372356415\n",
            "Batch Training Loss =  0.42453107237815857\n",
            "Batch Training Loss =  0.540072500705719\n",
            "Batch Training Loss =  0.5575858354568481\n",
            "Batch Training Loss =  0.3147521913051605\n",
            "Batch Training Loss =  0.25036701560020447\n",
            "Batch Training Loss =  0.31843769550323486\n",
            "Batch Training Loss =  0.38647204637527466\n",
            "Batch Training Loss =  0.4567207992076874\n",
            "Batch Training Loss =  0.6295900344848633\n",
            "Batch Training Loss =  0.35572266578674316\n",
            "Batch Training Loss =  0.4821581244468689\n",
            "Batch Training Loss =  0.4106653332710266\n",
            "Batch Training Loss =  0.418766051530838\n",
            "Batch Training Loss =  0.3753112852573395\n",
            "Batch Training Loss =  0.3450755178928375\n",
            "Batch Training Loss =  0.30060234665870667\n",
            "Validation Loss in this epoch is 0.406\n",
            "This is  55 th epoch\n",
            "Batch Training Loss =  0.32480257749557495\n",
            "Batch Training Loss =  0.6336024403572083\n",
            "Batch Training Loss =  0.7334182858467102\n",
            "Batch Training Loss =  0.5017142295837402\n",
            "Batch Training Loss =  0.33451738953590393\n",
            "Batch Training Loss =  0.6661267876625061\n",
            "Batch Training Loss =  0.5721753835678101\n",
            "Batch Training Loss =  0.49338218569755554\n",
            "Batch Training Loss =  0.3152357041835785\n",
            "Batch Training Loss =  0.6327403783798218\n",
            "Batch Training Loss =  0.6552464365959167\n",
            "Batch Training Loss =  0.30653467774391174\n",
            "Batch Training Loss =  0.3009518086910248\n",
            "Batch Training Loss =  0.459705114364624\n",
            "Batch Training Loss =  0.4981400668621063\n",
            "Batch Training Loss =  0.5328043699264526\n",
            "Batch Training Loss =  0.31934353709220886\n",
            "Batch Training Loss =  0.5524260997772217\n",
            "Batch Training Loss =  0.4127737879753113\n",
            "Batch Training Loss =  0.44295743107795715\n",
            "Batch Training Loss =  0.6221261024475098\n",
            "Batch Training Loss =  0.4475231468677521\n",
            "Batch Training Loss =  0.45427536964416504\n",
            "Batch Training Loss =  0.29521864652633667\n",
            "Batch Training Loss =  0.366056889295578\n",
            "Batch Training Loss =  0.33900487422943115\n",
            "Batch Training Loss =  0.39107897877693176\n",
            "Batch Training Loss =  0.6224973201751709\n",
            "Batch Training Loss =  0.8535654544830322\n",
            "Batch Training Loss =  1.039131760597229\n",
            "Batch Training Loss =  0.6823086738586426\n",
            "Batch Training Loss =  0.5242334604263306\n",
            "Validation Loss in this epoch is 0.427\n",
            "This is  56 th epoch\n",
            "Batch Training Loss =  0.5214435458183289\n",
            "Batch Training Loss =  0.616379976272583\n",
            "Batch Training Loss =  0.3583879768848419\n",
            "Batch Training Loss =  0.3307150900363922\n",
            "Batch Training Loss =  0.37628164887428284\n",
            "Batch Training Loss =  0.48697030544281006\n",
            "Batch Training Loss =  0.45057758688926697\n",
            "Batch Training Loss =  0.7465381026268005\n",
            "Batch Training Loss =  0.30240267515182495\n",
            "Batch Training Loss =  0.4350054860115051\n",
            "Batch Training Loss =  0.5546199679374695\n",
            "Batch Training Loss =  0.8170078992843628\n",
            "Batch Training Loss =  0.757480800151825\n",
            "Batch Training Loss =  0.8410856127738953\n",
            "Batch Training Loss =  0.6952930688858032\n",
            "Batch Training Loss =  0.9907287359237671\n",
            "Batch Training Loss =  0.7213545441627502\n",
            "Batch Training Loss =  0.5844826698303223\n",
            "Batch Training Loss =  0.690328061580658\n",
            "Batch Training Loss =  0.38133296370506287\n",
            "Batch Training Loss =  0.4558117985725403\n",
            "Batch Training Loss =  0.43200820684432983\n",
            "Batch Training Loss =  0.6409032344818115\n",
            "Batch Training Loss =  0.6863667368888855\n",
            "Batch Training Loss =  0.4619405269622803\n",
            "Batch Training Loss =  0.48127418756484985\n",
            "Batch Training Loss =  0.3583967983722687\n",
            "Batch Training Loss =  0.44477570056915283\n",
            "Batch Training Loss =  0.3430105447769165\n",
            "Batch Training Loss =  0.25925150513648987\n",
            "Batch Training Loss =  0.5203067660331726\n",
            "Batch Training Loss =  0.6726378798484802\n",
            "Validation Loss in this epoch is 0.555\n",
            "This is  57 th epoch\n",
            "Batch Training Loss =  0.48502472043037415\n",
            "Batch Training Loss =  0.3765590786933899\n",
            "Batch Training Loss =  0.443168044090271\n",
            "Batch Training Loss =  0.29602667689323425\n",
            "Batch Training Loss =  0.3493962585926056\n",
            "Batch Training Loss =  0.36426272988319397\n",
            "Batch Training Loss =  0.36816588044166565\n",
            "Batch Training Loss =  0.6649093627929688\n",
            "Batch Training Loss =  0.8112770915031433\n",
            "Batch Training Loss =  0.5767587423324585\n",
            "Batch Training Loss =  0.5321159362792969\n",
            "Batch Training Loss =  0.3386276960372925\n",
            "Batch Training Loss =  0.2985630929470062\n",
            "Batch Training Loss =  0.4330824017524719\n",
            "Batch Training Loss =  0.6698154211044312\n",
            "Batch Training Loss =  0.72075355052948\n",
            "Batch Training Loss =  0.6176850199699402\n",
            "Batch Training Loss =  0.4836675226688385\n",
            "Batch Training Loss =  0.5342939496040344\n",
            "Batch Training Loss =  0.510303795337677\n",
            "Batch Training Loss =  0.3896532952785492\n",
            "Batch Training Loss =  0.36982864141464233\n",
            "Batch Training Loss =  0.40548887848854065\n",
            "Batch Training Loss =  0.4381937086582184\n",
            "Batch Training Loss =  0.4084233343601227\n",
            "Batch Training Loss =  0.36229029297828674\n",
            "Batch Training Loss =  0.3292236030101776\n",
            "Batch Training Loss =  0.4144628643989563\n",
            "Batch Training Loss =  0.41902095079421997\n",
            "Batch Training Loss =  0.4559350609779358\n",
            "Batch Training Loss =  0.5871577858924866\n",
            "Batch Training Loss =  0.4431445002555847\n",
            "Validation Loss in this epoch is 0.480\n",
            "This is  58 th epoch\n",
            "Batch Training Loss =  0.48137354850769043\n",
            "Batch Training Loss =  0.3990221321582794\n",
            "Batch Training Loss =  0.5092455148696899\n",
            "Batch Training Loss =  0.40219399333000183\n",
            "Batch Training Loss =  0.41036200523376465\n",
            "Batch Training Loss =  0.6027311086654663\n",
            "Batch Training Loss =  0.39121609926223755\n",
            "Batch Training Loss =  0.5543559789657593\n",
            "Batch Training Loss =  0.8058893084526062\n",
            "Batch Training Loss =  0.7046328783035278\n",
            "Batch Training Loss =  0.5975943803787231\n",
            "Batch Training Loss =  0.3387826979160309\n",
            "Batch Training Loss =  0.5283100008964539\n",
            "Batch Training Loss =  0.7969939708709717\n",
            "Batch Training Loss =  0.560910165309906\n",
            "Batch Training Loss =  0.32098090648651123\n",
            "Batch Training Loss =  0.3834589123725891\n",
            "Batch Training Loss =  0.38369491696357727\n",
            "Batch Training Loss =  0.5018072128295898\n",
            "Batch Training Loss =  0.5729213356971741\n",
            "Batch Training Loss =  0.44075295329093933\n",
            "Batch Training Loss =  0.5587964057922363\n",
            "Batch Training Loss =  0.3460727035999298\n",
            "Batch Training Loss =  0.4000357389450073\n",
            "Batch Training Loss =  0.3672243356704712\n",
            "Batch Training Loss =  0.4928262233734131\n",
            "Batch Training Loss =  0.6590957641601562\n",
            "Batch Training Loss =  0.4759918451309204\n",
            "Batch Training Loss =  0.6801730394363403\n",
            "Batch Training Loss =  0.3612111210823059\n",
            "Batch Training Loss =  0.4838026762008667\n",
            "Batch Training Loss =  0.6367675065994263\n",
            "Validation Loss in this epoch is 0.425\n",
            "This is  59 th epoch\n",
            "Batch Training Loss =  0.30301961302757263\n",
            "Batch Training Loss =  0.43423956632614136\n",
            "Batch Training Loss =  0.3871750235557556\n",
            "Batch Training Loss =  0.439826101064682\n",
            "Batch Training Loss =  0.513727068901062\n",
            "Batch Training Loss =  0.3906298875808716\n",
            "Batch Training Loss =  0.3679448962211609\n",
            "Batch Training Loss =  0.6524003744125366\n",
            "Batch Training Loss =  0.7713226079940796\n",
            "Batch Training Loss =  0.7170842885971069\n",
            "Batch Training Loss =  0.5996876358985901\n",
            "Batch Training Loss =  0.47273510694503784\n",
            "Batch Training Loss =  0.6673517823219299\n",
            "Batch Training Loss =  0.49563461542129517\n",
            "Batch Training Loss =  0.35319197177886963\n",
            "Batch Training Loss =  0.3507425785064697\n",
            "Batch Training Loss =  0.33715930581092834\n",
            "Batch Training Loss =  0.27070534229278564\n",
            "Batch Training Loss =  0.4350197911262512\n",
            "Batch Training Loss =  0.5331549644470215\n",
            "Batch Training Loss =  0.4328191876411438\n",
            "Batch Training Loss =  0.3959129750728607\n",
            "Batch Training Loss =  0.5011376738548279\n",
            "Batch Training Loss =  0.3872874975204468\n",
            "Batch Training Loss =  0.3714364767074585\n",
            "Batch Training Loss =  0.471332848072052\n",
            "Batch Training Loss =  0.3029715418815613\n",
            "Batch Training Loss =  0.34303560853004456\n",
            "Batch Training Loss =  0.39197415113449097\n",
            "Batch Training Loss =  0.5328269004821777\n",
            "Batch Training Loss =  0.7900547981262207\n",
            "Batch Training Loss =  0.46893131732940674\n",
            "Validation Loss in this epoch is 0.605\n",
            "This is  60 th epoch\n",
            "Batch Training Loss =  0.6459807753562927\n",
            "Batch Training Loss =  0.369649201631546\n",
            "Batch Training Loss =  0.3217639923095703\n",
            "Batch Training Loss =  0.35711419582366943\n",
            "Batch Training Loss =  0.32954394817352295\n",
            "Batch Training Loss =  0.3474093973636627\n",
            "Batch Training Loss =  0.7357473969459534\n",
            "Batch Training Loss =  0.7210390567779541\n",
            "Batch Training Loss =  0.7496693134307861\n",
            "Batch Training Loss =  0.7233250141143799\n",
            "Batch Training Loss =  0.4771170914173126\n",
            "Batch Training Loss =  0.3765900731086731\n",
            "Batch Training Loss =  0.3575442433357239\n",
            "Batch Training Loss =  0.3084867298603058\n",
            "Batch Training Loss =  0.46638035774230957\n",
            "Batch Training Loss =  0.7156699895858765\n",
            "Batch Training Loss =  0.4049127697944641\n",
            "Batch Training Loss =  0.34963512420654297\n",
            "Batch Training Loss =  0.47271955013275146\n",
            "Batch Training Loss =  0.5894948840141296\n",
            "Batch Training Loss =  0.37550708651542664\n",
            "Batch Training Loss =  0.3994341194629669\n",
            "Batch Training Loss =  0.31433290243148804\n",
            "Batch Training Loss =  0.37651264667510986\n",
            "Batch Training Loss =  0.3796733021736145\n",
            "Batch Training Loss =  0.3350350558757782\n",
            "Batch Training Loss =  0.3454311490058899\n",
            "Batch Training Loss =  0.3383557200431824\n",
            "Batch Training Loss =  0.3069492280483246\n",
            "Batch Training Loss =  0.544646680355072\n",
            "Batch Training Loss =  0.8207128047943115\n",
            "Batch Training Loss =  0.6171892285346985\n",
            "Validation Loss in this epoch is 0.541\n",
            "This is  61 th epoch\n",
            "Batch Training Loss =  0.554781436920166\n",
            "Batch Training Loss =  0.4461395740509033\n",
            "Batch Training Loss =  0.6658140420913696\n",
            "Batch Training Loss =  0.37921786308288574\n",
            "Batch Training Loss =  0.2738650441169739\n",
            "Batch Training Loss =  0.7823454141616821\n",
            "Batch Training Loss =  0.7408981323242188\n",
            "Batch Training Loss =  0.6357569098472595\n",
            "Batch Training Loss =  0.7855281233787537\n",
            "Batch Training Loss =  0.393077552318573\n",
            "Batch Training Loss =  0.42033007740974426\n",
            "Batch Training Loss =  0.5066839456558228\n",
            "Batch Training Loss =  0.47963273525238037\n",
            "Batch Training Loss =  0.40300795435905457\n",
            "Batch Training Loss =  0.48870739340782166\n",
            "Batch Training Loss =  0.31918060779571533\n",
            "Batch Training Loss =  0.4956098794937134\n",
            "Batch Training Loss =  0.43529823422431946\n",
            "Batch Training Loss =  0.5911282896995544\n",
            "Batch Training Loss =  0.575916051864624\n",
            "Batch Training Loss =  0.3762068450450897\n",
            "Batch Training Loss =  0.463724821805954\n",
            "Batch Training Loss =  0.44990676641464233\n",
            "Batch Training Loss =  0.3160835802555084\n",
            "Batch Training Loss =  0.35723409056663513\n",
            "Batch Training Loss =  0.3622259199619293\n",
            "Batch Training Loss =  0.31480807065963745\n",
            "Batch Training Loss =  0.3006643056869507\n",
            "Batch Training Loss =  0.3391747772693634\n",
            "Batch Training Loss =  0.4118630290031433\n",
            "Batch Training Loss =  0.7670982480049133\n",
            "Batch Training Loss =  0.642871081829071\n",
            "Validation Loss in this epoch is 0.476\n",
            "This is  62 th epoch\n",
            "Batch Training Loss =  0.543008029460907\n",
            "Batch Training Loss =  0.3886733651161194\n",
            "Batch Training Loss =  0.6936308145523071\n",
            "Batch Training Loss =  0.3183322548866272\n",
            "Batch Training Loss =  0.3197792172431946\n",
            "Batch Training Loss =  0.34772244095802307\n",
            "Batch Training Loss =  0.38444823026657104\n",
            "Batch Training Loss =  0.5234564542770386\n",
            "Batch Training Loss =  0.6325811147689819\n",
            "Batch Training Loss =  0.5140261650085449\n",
            "Batch Training Loss =  0.3716060519218445\n",
            "Batch Training Loss =  0.4862367510795593\n",
            "Batch Training Loss =  0.39934584498405457\n",
            "Batch Training Loss =  0.4507390856742859\n",
            "Batch Training Loss =  0.4982539415359497\n",
            "Batch Training Loss =  0.5279901027679443\n",
            "Batch Training Loss =  0.3694172203540802\n",
            "Batch Training Loss =  0.44894999265670776\n",
            "Batch Training Loss =  0.4355873167514801\n",
            "Batch Training Loss =  0.426324725151062\n",
            "Batch Training Loss =  0.3953303396701813\n",
            "Batch Training Loss =  0.6182040572166443\n",
            "Batch Training Loss =  0.6654251217842102\n",
            "Batch Training Loss =  0.42565056681632996\n",
            "Batch Training Loss =  0.5010506510734558\n",
            "Batch Training Loss =  0.6691915392875671\n",
            "Batch Training Loss =  0.29966071248054504\n",
            "Batch Training Loss =  0.3508096933364868\n",
            "Batch Training Loss =  0.42352402210235596\n",
            "Batch Training Loss =  0.4317624568939209\n",
            "Batch Training Loss =  0.38867294788360596\n",
            "Batch Training Loss =  0.34591811895370483\n",
            "Validation Loss in this epoch is 0.443\n",
            "This is  63 th epoch\n",
            "Batch Training Loss =  0.5078889727592468\n",
            "Batch Training Loss =  0.40521523356437683\n",
            "Batch Training Loss =  0.38762202858924866\n",
            "Batch Training Loss =  0.37875476479530334\n",
            "Batch Training Loss =  0.5246303677558899\n",
            "Batch Training Loss =  0.30155661702156067\n",
            "Batch Training Loss =  0.3785816431045532\n",
            "Batch Training Loss =  0.3662263751029968\n",
            "Batch Training Loss =  0.6538589000701904\n",
            "Batch Training Loss =  0.6804628968238831\n",
            "Batch Training Loss =  0.3885889947414398\n",
            "Batch Training Loss =  0.29658186435699463\n",
            "Batch Training Loss =  0.2477160096168518\n",
            "Batch Training Loss =  0.23661202192306519\n",
            "Batch Training Loss =  0.3492943048477173\n",
            "Batch Training Loss =  0.3740912973880768\n",
            "Batch Training Loss =  0.4260390102863312\n",
            "Batch Training Loss =  0.4730916917324066\n",
            "Batch Training Loss =  0.8033316135406494\n",
            "Batch Training Loss =  0.39975833892822266\n",
            "Batch Training Loss =  0.43652060627937317\n",
            "Batch Training Loss =  0.5056874752044678\n",
            "Batch Training Loss =  0.4927845001220703\n",
            "Batch Training Loss =  0.40384629368782043\n",
            "Batch Training Loss =  0.38273343443870544\n",
            "Batch Training Loss =  0.406889945268631\n",
            "Batch Training Loss =  0.652138888835907\n",
            "Batch Training Loss =  0.7576010227203369\n",
            "Batch Training Loss =  0.7030839323997498\n",
            "Batch Training Loss =  0.4545394778251648\n",
            "Batch Training Loss =  0.35854798555374146\n",
            "Batch Training Loss =  0.44288718700408936\n",
            "Validation Loss in this epoch is 0.423\n",
            "This is  64 th epoch\n",
            "Batch Training Loss =  0.5141611695289612\n",
            "Batch Training Loss =  0.7803930640220642\n",
            "Batch Training Loss =  0.6226468682289124\n",
            "Batch Training Loss =  0.36506539583206177\n",
            "Batch Training Loss =  0.396850049495697\n",
            "Batch Training Loss =  0.5686953663825989\n",
            "Batch Training Loss =  0.6100963354110718\n",
            "Batch Training Loss =  0.4134382903575897\n",
            "Batch Training Loss =  0.39601650834083557\n",
            "Batch Training Loss =  0.29662808775901794\n",
            "Batch Training Loss =  0.22517676651477814\n",
            "Batch Training Loss =  0.3000481426715851\n",
            "Batch Training Loss =  0.4791218936443329\n",
            "Batch Training Loss =  0.43362316489219666\n",
            "Batch Training Loss =  0.3373226523399353\n",
            "Batch Training Loss =  0.3021445572376251\n",
            "Batch Training Loss =  0.3348892629146576\n",
            "Batch Training Loss =  0.30279216170310974\n",
            "Batch Training Loss =  0.47185805439949036\n",
            "Batch Training Loss =  0.6949466466903687\n",
            "Batch Training Loss =  0.4106161594390869\n",
            "Batch Training Loss =  0.5592381358146667\n",
            "Batch Training Loss =  0.7433997392654419\n",
            "Batch Training Loss =  0.38778048753738403\n",
            "Batch Training Loss =  0.4310898184776306\n",
            "Batch Training Loss =  0.6086024641990662\n",
            "Batch Training Loss =  0.4048498570919037\n",
            "Batch Training Loss =  0.30755898356437683\n",
            "Batch Training Loss =  0.5240386724472046\n",
            "Batch Training Loss =  0.413874089717865\n",
            "Batch Training Loss =  0.6301672458648682\n",
            "Batch Training Loss =  0.3548904359340668\n",
            "Validation Loss in this epoch is 0.568\n",
            "This is  65 th epoch\n",
            "Batch Training Loss =  0.6306201815605164\n",
            "Batch Training Loss =  0.7933908104896545\n",
            "Batch Training Loss =  0.6806054711341858\n",
            "Batch Training Loss =  0.5737378597259521\n",
            "Batch Training Loss =  0.5279594659805298\n",
            "Batch Training Loss =  0.7836453318595886\n",
            "Batch Training Loss =  0.5640511512756348\n",
            "Batch Training Loss =  0.45507895946502686\n",
            "Batch Training Loss =  0.37230023741722107\n",
            "Batch Training Loss =  0.29610320925712585\n",
            "Batch Training Loss =  0.34239867329597473\n",
            "Batch Training Loss =  0.3345734477043152\n",
            "Batch Training Loss =  0.6294558644294739\n",
            "Batch Training Loss =  0.6037558913230896\n",
            "Batch Training Loss =  0.511690080165863\n",
            "Batch Training Loss =  0.3177081346511841\n",
            "Batch Training Loss =  0.39983174204826355\n",
            "Batch Training Loss =  0.4863514006137848\n",
            "Batch Training Loss =  0.269644558429718\n",
            "Batch Training Loss =  0.43157637119293213\n",
            "Batch Training Loss =  0.5391817688941956\n",
            "Batch Training Loss =  0.44755348563194275\n",
            "Batch Training Loss =  0.5402241349220276\n",
            "Batch Training Loss =  0.4174553155899048\n",
            "Batch Training Loss =  0.5610692501068115\n",
            "Batch Training Loss =  0.5651953816413879\n",
            "Batch Training Loss =  0.4039078950881958\n",
            "Batch Training Loss =  0.2926191985607147\n",
            "Batch Training Loss =  0.35870593786239624\n",
            "Batch Training Loss =  0.3080238699913025\n",
            "Batch Training Loss =  0.5017486810684204\n",
            "Batch Training Loss =  0.3807782232761383\n",
            "Validation Loss in this epoch is 0.404\n",
            "This is  66 th epoch\n",
            "Batch Training Loss =  0.5215936899185181\n",
            "Batch Training Loss =  0.3957594633102417\n",
            "Batch Training Loss =  0.48201870918273926\n",
            "Batch Training Loss =  0.6190975308418274\n",
            "Batch Training Loss =  0.621144711971283\n",
            "Batch Training Loss =  0.7676396369934082\n",
            "Batch Training Loss =  0.37016838788986206\n",
            "Batch Training Loss =  0.26754117012023926\n",
            "Batch Training Loss =  0.28442391753196716\n",
            "Batch Training Loss =  0.41902533173561096\n",
            "Batch Training Loss =  0.6667553186416626\n",
            "Batch Training Loss =  0.720643937587738\n",
            "Batch Training Loss =  0.7313788533210754\n",
            "Batch Training Loss =  0.5504474639892578\n",
            "Batch Training Loss =  0.3332012891769409\n",
            "Batch Training Loss =  0.3524249792098999\n",
            "Batch Training Loss =  0.47811630368232727\n",
            "Batch Training Loss =  0.4027867019176483\n",
            "Batch Training Loss =  0.5400123000144958\n",
            "Batch Training Loss =  0.3291114568710327\n",
            "Batch Training Loss =  0.4429987668991089\n",
            "Batch Training Loss =  0.4946517050266266\n",
            "Batch Training Loss =  0.38842347264289856\n",
            "Batch Training Loss =  0.35341259837150574\n",
            "Batch Training Loss =  0.5308783650398254\n",
            "Batch Training Loss =  0.46814635396003723\n",
            "Batch Training Loss =  0.36969131231307983\n",
            "Batch Training Loss =  0.5049259662628174\n",
            "Batch Training Loss =  0.29625099897384644\n",
            "Batch Training Loss =  0.4451502561569214\n",
            "Batch Training Loss =  0.3421713709831238\n",
            "Batch Training Loss =  0.3851463198661804\n",
            "Validation Loss in this epoch is 0.402\n",
            "This is  67 th epoch\n",
            "Batch Training Loss =  0.373783141374588\n",
            "Batch Training Loss =  0.3672051727771759\n",
            "Batch Training Loss =  0.497455358505249\n",
            "Batch Training Loss =  0.6856048703193665\n",
            "Batch Training Loss =  0.41348621249198914\n",
            "Batch Training Loss =  0.4400060176849365\n",
            "Batch Training Loss =  0.7139930725097656\n",
            "Batch Training Loss =  0.3819845914840698\n",
            "Batch Training Loss =  0.5799946188926697\n",
            "Batch Training Loss =  0.47773540019989014\n",
            "Batch Training Loss =  0.47108539938926697\n",
            "Batch Training Loss =  0.31730127334594727\n",
            "Batch Training Loss =  0.4661649763584137\n",
            "Batch Training Loss =  0.3780997097492218\n",
            "Batch Training Loss =  0.40147635340690613\n",
            "Batch Training Loss =  0.590127170085907\n",
            "Batch Training Loss =  0.5353317260742188\n",
            "Batch Training Loss =  0.5654113292694092\n",
            "Batch Training Loss =  0.3902369737625122\n",
            "Batch Training Loss =  0.27181974053382874\n",
            "Batch Training Loss =  0.39532747864723206\n",
            "Batch Training Loss =  0.41890648007392883\n",
            "Batch Training Loss =  0.2761616110801697\n",
            "Batch Training Loss =  0.48808902502059937\n",
            "Batch Training Loss =  0.6414148807525635\n",
            "Batch Training Loss =  0.429132878780365\n",
            "Batch Training Loss =  0.6488896012306213\n",
            "Batch Training Loss =  0.6267873644828796\n",
            "Batch Training Loss =  0.6419062614440918\n",
            "Batch Training Loss =  0.34571048617362976\n",
            "Batch Training Loss =  0.5185615420341492\n",
            "Batch Training Loss =  0.6184709072113037\n",
            "Validation Loss in this epoch is 0.400\n",
            "This is  68 th epoch\n",
            "Batch Training Loss =  0.40160685777664185\n",
            "Batch Training Loss =  0.42566385865211487\n",
            "Batch Training Loss =  0.37134498357772827\n",
            "Batch Training Loss =  0.46579602360725403\n",
            "Batch Training Loss =  0.41446763277053833\n",
            "Batch Training Loss =  0.5640100240707397\n",
            "Batch Training Loss =  0.5084232091903687\n",
            "Batch Training Loss =  0.44820159673690796\n",
            "Batch Training Loss =  0.4777714014053345\n",
            "Batch Training Loss =  0.30065903067588806\n",
            "Batch Training Loss =  0.3889651596546173\n",
            "Batch Training Loss =  0.41437390446662903\n",
            "Batch Training Loss =  0.7820943593978882\n",
            "Batch Training Loss =  0.7532092332839966\n",
            "Batch Training Loss =  0.6467519402503967\n",
            "Batch Training Loss =  0.487863153219223\n",
            "Batch Training Loss =  0.4519737958908081\n",
            "Batch Training Loss =  0.5645681023597717\n",
            "Batch Training Loss =  0.3036442697048187\n",
            "Batch Training Loss =  0.4179270565509796\n",
            "Batch Training Loss =  0.5212454795837402\n",
            "Batch Training Loss =  0.3132701516151428\n",
            "Batch Training Loss =  0.597952127456665\n",
            "Batch Training Loss =  0.7285787463188171\n",
            "Batch Training Loss =  0.36286431550979614\n",
            "Batch Training Loss =  0.5143267512321472\n",
            "Batch Training Loss =  0.43901312351226807\n",
            "Batch Training Loss =  0.5186859369277954\n",
            "Batch Training Loss =  0.41348183155059814\n",
            "Batch Training Loss =  0.3391937017440796\n",
            "Batch Training Loss =  0.2976696491241455\n",
            "Batch Training Loss =  0.42193055152893066\n",
            "Validation Loss in this epoch is 0.444\n",
            "This is  69 th epoch\n",
            "Batch Training Loss =  0.3638647496700287\n",
            "Batch Training Loss =  0.5269776582717896\n",
            "Batch Training Loss =  0.5947635173797607\n",
            "Batch Training Loss =  0.7445074319839478\n",
            "Batch Training Loss =  0.6169811487197876\n",
            "Batch Training Loss =  0.345510870218277\n",
            "Batch Training Loss =  0.7231970429420471\n",
            "Batch Training Loss =  0.7410907745361328\n",
            "Batch Training Loss =  0.5660139322280884\n",
            "Batch Training Loss =  0.4638504385948181\n",
            "Batch Training Loss =  0.42448484897613525\n",
            "Batch Training Loss =  0.3693084418773651\n",
            "Batch Training Loss =  0.45900675654411316\n",
            "Batch Training Loss =  0.27640169858932495\n",
            "Batch Training Loss =  0.34019115567207336\n",
            "Batch Training Loss =  0.33435219526290894\n",
            "Batch Training Loss =  0.44378653168678284\n",
            "Batch Training Loss =  0.44796866178512573\n",
            "Batch Training Loss =  0.6602426767349243\n",
            "Batch Training Loss =  0.594861626625061\n",
            "Batch Training Loss =  0.35215526819229126\n",
            "Batch Training Loss =  0.5791938304901123\n",
            "Batch Training Loss =  0.47929900884628296\n",
            "Batch Training Loss =  0.46938157081604004\n",
            "Batch Training Loss =  0.3412642776966095\n",
            "Batch Training Loss =  0.396618127822876\n",
            "Batch Training Loss =  0.32500770688056946\n",
            "Batch Training Loss =  0.7417985796928406\n",
            "Batch Training Loss =  0.6068000197410583\n",
            "Batch Training Loss =  0.420279860496521\n",
            "Batch Training Loss =  0.25452855229377747\n",
            "Batch Training Loss =  0.4394887685775757\n",
            "Validation Loss in this epoch is 0.460\n",
            "This is  70 th epoch\n",
            "Batch Training Loss =  0.4077644646167755\n",
            "Batch Training Loss =  0.35886693000793457\n",
            "Batch Training Loss =  0.3821074664592743\n",
            "Batch Training Loss =  0.39627331495285034\n",
            "Batch Training Loss =  0.468735009431839\n",
            "Batch Training Loss =  0.4260247051715851\n",
            "Batch Training Loss =  0.3996572494506836\n",
            "Batch Training Loss =  0.5779498815536499\n",
            "Batch Training Loss =  0.2604251205921173\n",
            "Batch Training Loss =  0.31738564372062683\n",
            "Batch Training Loss =  0.5057581067085266\n",
            "Batch Training Loss =  0.6752712726593018\n",
            "Batch Training Loss =  0.8154879212379456\n",
            "Batch Training Loss =  0.5154514908790588\n",
            "Batch Training Loss =  0.49744728207588196\n",
            "Batch Training Loss =  0.2959085702896118\n",
            "Batch Training Loss =  0.47192999720573425\n",
            "Batch Training Loss =  0.3011029362678528\n",
            "Batch Training Loss =  0.505405068397522\n",
            "Batch Training Loss =  0.2584313154220581\n",
            "Batch Training Loss =  0.40077152848243713\n",
            "Batch Training Loss =  0.3705676198005676\n",
            "Batch Training Loss =  0.33337485790252686\n",
            "Batch Training Loss =  0.4328407943248749\n",
            "Batch Training Loss =  0.3985298275947571\n",
            "Batch Training Loss =  0.3093224763870239\n",
            "Batch Training Loss =  0.3583667278289795\n",
            "Batch Training Loss =  0.5246525406837463\n",
            "Batch Training Loss =  0.3088184595108032\n",
            "Batch Training Loss =  0.5067608952522278\n",
            "Batch Training Loss =  0.7784732580184937\n",
            "Batch Training Loss =  0.39864325523376465\n",
            "Validation Loss in this epoch is 0.413\n",
            "This is  71 th epoch\n",
            "Batch Training Loss =  0.29341965913772583\n",
            "Batch Training Loss =  0.7454653978347778\n",
            "Batch Training Loss =  0.6987671852111816\n",
            "Batch Training Loss =  0.823662519454956\n",
            "Batch Training Loss =  0.72878098487854\n",
            "Batch Training Loss =  0.8586848378181458\n",
            "Batch Training Loss =  0.8424025774002075\n",
            "Batch Training Loss =  0.8199124336242676\n",
            "Batch Training Loss =  0.4545101523399353\n",
            "Batch Training Loss =  0.3972989618778229\n",
            "Batch Training Loss =  0.5191411972045898\n",
            "Batch Training Loss =  0.5273035168647766\n",
            "Batch Training Loss =  0.601342499256134\n",
            "Batch Training Loss =  0.4537086486816406\n",
            "Batch Training Loss =  0.6399950981140137\n",
            "Batch Training Loss =  0.6353991031646729\n",
            "Batch Training Loss =  0.5610150694847107\n",
            "Batch Training Loss =  0.43181028962135315\n",
            "Batch Training Loss =  0.39197400212287903\n",
            "Batch Training Loss =  0.408669114112854\n",
            "Batch Training Loss =  0.38494637608528137\n",
            "Batch Training Loss =  0.38496899604797363\n",
            "Batch Training Loss =  0.5582515001296997\n",
            "Batch Training Loss =  0.6702835559844971\n",
            "Batch Training Loss =  0.7355532050132751\n",
            "Batch Training Loss =  0.5256047248840332\n",
            "Batch Training Loss =  0.38217324018478394\n",
            "Batch Training Loss =  0.4324527084827423\n",
            "Batch Training Loss =  0.3071455657482147\n",
            "Batch Training Loss =  0.30389851331710815\n",
            "Batch Training Loss =  0.3640871047973633\n",
            "Batch Training Loss =  0.39780762791633606\n",
            "Validation Loss in this epoch is 0.420\n",
            "This is  72 th epoch\n",
            "Batch Training Loss =  0.3572034537792206\n",
            "Batch Training Loss =  0.39160239696502686\n",
            "Batch Training Loss =  0.4159903824329376\n",
            "Batch Training Loss =  0.3667792081832886\n",
            "Batch Training Loss =  0.47375279664993286\n",
            "Batch Training Loss =  0.5082690119743347\n",
            "Batch Training Loss =  0.3228124678134918\n",
            "Batch Training Loss =  0.4334603250026703\n",
            "Batch Training Loss =  0.37147095799446106\n",
            "Batch Training Loss =  0.36095497012138367\n",
            "Batch Training Loss =  0.37727877497673035\n",
            "Batch Training Loss =  0.46471327543258667\n",
            "Batch Training Loss =  0.31760239601135254\n",
            "Batch Training Loss =  0.22957909107208252\n",
            "Batch Training Loss =  0.3290422558784485\n",
            "Batch Training Loss =  0.4450208246707916\n",
            "Batch Training Loss =  0.6603153347969055\n",
            "Batch Training Loss =  0.6352006793022156\n",
            "Batch Training Loss =  0.7990100979804993\n",
            "Batch Training Loss =  0.46751952171325684\n",
            "Batch Training Loss =  0.31953489780426025\n",
            "Batch Training Loss =  0.4971993565559387\n",
            "Batch Training Loss =  0.4060195982456207\n",
            "Batch Training Loss =  0.434965580701828\n",
            "Batch Training Loss =  0.2368529886007309\n",
            "Batch Training Loss =  0.40276598930358887\n",
            "Batch Training Loss =  0.5692719221115112\n",
            "Batch Training Loss =  0.29997503757476807\n",
            "Batch Training Loss =  0.4048303961753845\n",
            "Batch Training Loss =  0.8860021233558655\n",
            "Batch Training Loss =  0.8572894334793091\n",
            "Batch Training Loss =  0.9546936750411987\n",
            "Validation Loss in this epoch is 0.753\n",
            "This is  73 th epoch\n",
            "Batch Training Loss =  0.6887294054031372\n",
            "Batch Training Loss =  0.9050544500350952\n",
            "Batch Training Loss =  0.7468716502189636\n",
            "Batch Training Loss =  0.5540112853050232\n",
            "Batch Training Loss =  0.5683952569961548\n",
            "Batch Training Loss =  0.5694265961647034\n",
            "Batch Training Loss =  0.40370818972587585\n",
            "Batch Training Loss =  0.36613723635673523\n",
            "Batch Training Loss =  0.6012613773345947\n",
            "Batch Training Loss =  0.6379907131195068\n",
            "Batch Training Loss =  0.5373371839523315\n",
            "Batch Training Loss =  0.44933047890663147\n",
            "Batch Training Loss =  0.4618464708328247\n",
            "Batch Training Loss =  0.3055180609226227\n",
            "Batch Training Loss =  0.3164791464805603\n",
            "Batch Training Loss =  0.31725648045539856\n",
            "Batch Training Loss =  0.7204360961914062\n",
            "Batch Training Loss =  0.621523916721344\n",
            "Batch Training Loss =  0.5458717942237854\n",
            "Batch Training Loss =  0.43723064661026\n",
            "Batch Training Loss =  0.321816623210907\n",
            "Batch Training Loss =  0.26753073930740356\n",
            "Batch Training Loss =  0.5526283383369446\n",
            "Batch Training Loss =  0.6702403426170349\n",
            "Batch Training Loss =  0.4367068409919739\n",
            "Batch Training Loss =  0.4575328826904297\n",
            "Batch Training Loss =  0.3401346504688263\n",
            "Batch Training Loss =  0.45278921723365784\n",
            "Batch Training Loss =  0.46369290351867676\n",
            "Batch Training Loss =  0.42267829179763794\n",
            "Batch Training Loss =  0.32999151945114136\n",
            "Batch Training Loss =  0.3015056550502777\n",
            "Validation Loss in this epoch is 0.505\n",
            "This is  74 th epoch\n",
            "Batch Training Loss =  0.5063111782073975\n",
            "Batch Training Loss =  0.5879881978034973\n",
            "Batch Training Loss =  0.2747495174407959\n",
            "Batch Training Loss =  0.28218042850494385\n",
            "Batch Training Loss =  0.37298256158828735\n",
            "Batch Training Loss =  0.3194606900215149\n",
            "Batch Training Loss =  0.31088149547576904\n",
            "Batch Training Loss =  0.4680362343788147\n",
            "Batch Training Loss =  0.556672990322113\n",
            "Batch Training Loss =  0.2602139413356781\n",
            "Batch Training Loss =  0.42215603590011597\n",
            "Batch Training Loss =  0.43103834986686707\n",
            "Batch Training Loss =  0.42130452394485474\n",
            "Batch Training Loss =  0.5439382195472717\n",
            "Batch Training Loss =  0.6130447387695312\n",
            "Batch Training Loss =  0.5243495106697083\n",
            "Batch Training Loss =  0.4144373834133148\n",
            "Batch Training Loss =  0.465363085269928\n",
            "Batch Training Loss =  0.37750527262687683\n",
            "Batch Training Loss =  0.3670918941497803\n",
            "Batch Training Loss =  0.35033828020095825\n",
            "Batch Training Loss =  0.292688250541687\n",
            "Batch Training Loss =  0.337342232465744\n",
            "Batch Training Loss =  0.3122638463973999\n",
            "Batch Training Loss =  0.7915586233139038\n",
            "Batch Training Loss =  0.8816806077957153\n",
            "Batch Training Loss =  0.7924681901931763\n",
            "Batch Training Loss =  0.6095931529998779\n",
            "Batch Training Loss =  0.4566809833049774\n",
            "Batch Training Loss =  0.31921494007110596\n",
            "Batch Training Loss =  0.3797573745250702\n",
            "Batch Training Loss =  0.2794214189052582\n",
            "Validation Loss in this epoch is 0.680\n",
            "This is  75 th epoch\n",
            "Batch Training Loss =  0.7049508094787598\n",
            "Batch Training Loss =  0.7578334808349609\n",
            "Batch Training Loss =  0.8743353486061096\n",
            "Batch Training Loss =  0.7976968884468079\n",
            "Batch Training Loss =  0.6463088989257812\n",
            "Batch Training Loss =  0.546004056930542\n",
            "Batch Training Loss =  0.3852662146091461\n",
            "Batch Training Loss =  0.4721030294895172\n",
            "Batch Training Loss =  0.692806601524353\n",
            "Batch Training Loss =  0.4090315103530884\n",
            "Batch Training Loss =  0.42638397216796875\n",
            "Batch Training Loss =  0.4142904579639435\n",
            "Batch Training Loss =  0.563644289970398\n",
            "Batch Training Loss =  0.5851913094520569\n",
            "Batch Training Loss =  0.429210901260376\n",
            "Batch Training Loss =  0.469399631023407\n",
            "Batch Training Loss =  0.34000900387763977\n",
            "Batch Training Loss =  0.42178893089294434\n",
            "Batch Training Loss =  0.28682172298431396\n",
            "Batch Training Loss =  0.5179910063743591\n",
            "Batch Training Loss =  0.3945326507091522\n",
            "Batch Training Loss =  0.5297253131866455\n",
            "Batch Training Loss =  0.37212225794792175\n",
            "Batch Training Loss =  0.4091687798500061\n",
            "Batch Training Loss =  0.33169490098953247\n",
            "Batch Training Loss =  0.3292471170425415\n",
            "Batch Training Loss =  0.2627331614494324\n",
            "Batch Training Loss =  0.5786587595939636\n",
            "Batch Training Loss =  0.6995394825935364\n",
            "Batch Training Loss =  0.7665995955467224\n",
            "Batch Training Loss =  0.3925796151161194\n",
            "Batch Training Loss =  0.34759846329689026\n",
            "Validation Loss in this epoch is 0.401\n",
            "This is  76 th epoch\n",
            "Batch Training Loss =  0.45896396040916443\n",
            "Batch Training Loss =  0.4521082639694214\n",
            "Batch Training Loss =  0.48225799202919006\n",
            "Batch Training Loss =  0.701321005821228\n",
            "Batch Training Loss =  0.3565416634082794\n",
            "Batch Training Loss =  0.385932058095932\n",
            "Batch Training Loss =  0.3189011514186859\n",
            "Batch Training Loss =  0.3485634922981262\n",
            "Batch Training Loss =  0.7679076194763184\n",
            "Batch Training Loss =  0.7152498364448547\n",
            "Batch Training Loss =  0.5463055372238159\n",
            "Batch Training Loss =  0.48639702796936035\n",
            "Batch Training Loss =  0.3527892827987671\n",
            "Batch Training Loss =  0.37308457493782043\n",
            "Batch Training Loss =  0.35321304202079773\n",
            "Batch Training Loss =  0.39899688959121704\n",
            "Batch Training Loss =  0.42458823323249817\n",
            "Batch Training Loss =  0.2697017788887024\n",
            "Batch Training Loss =  0.3191922903060913\n",
            "Batch Training Loss =  0.4284520745277405\n",
            "Batch Training Loss =  0.4855962097644806\n",
            "Batch Training Loss =  0.7298126220703125\n",
            "Batch Training Loss =  0.6018494963645935\n",
            "Batch Training Loss =  0.5368146896362305\n",
            "Batch Training Loss =  0.4419446587562561\n",
            "Batch Training Loss =  0.43309128284454346\n",
            "Batch Training Loss =  0.4924279451370239\n",
            "Batch Training Loss =  0.38298651576042175\n",
            "Batch Training Loss =  0.22338351607322693\n",
            "Batch Training Loss =  0.33655107021331787\n",
            "Batch Training Loss =  0.36757224798202515\n",
            "Batch Training Loss =  0.39759117364883423\n",
            "Validation Loss in this epoch is 0.400\n",
            "This is  77 th epoch\n",
            "Batch Training Loss =  0.2809472978115082\n",
            "Batch Training Loss =  0.2704419791698456\n",
            "Batch Training Loss =  0.396122545003891\n",
            "Batch Training Loss =  0.49090635776519775\n",
            "Batch Training Loss =  0.44918835163116455\n",
            "Batch Training Loss =  0.6794490218162537\n",
            "Batch Training Loss =  0.6179937124252319\n",
            "Batch Training Loss =  0.3060341775417328\n",
            "Batch Training Loss =  0.3133848011493683\n",
            "Batch Training Loss =  0.34091323614120483\n",
            "Batch Training Loss =  0.4551188349723816\n",
            "Batch Training Loss =  0.4438420534133911\n",
            "Batch Training Loss =  0.5981929302215576\n",
            "Batch Training Loss =  0.49521613121032715\n",
            "Batch Training Loss =  0.3306575119495392\n",
            "Batch Training Loss =  0.3379238545894623\n",
            "Batch Training Loss =  0.47711431980133057\n",
            "Batch Training Loss =  0.5283940434455872\n",
            "Batch Training Loss =  0.6169622540473938\n",
            "Batch Training Loss =  0.36977308988571167\n",
            "Batch Training Loss =  0.5103652477264404\n",
            "Batch Training Loss =  0.5832729935646057\n",
            "Batch Training Loss =  0.6268031597137451\n",
            "Batch Training Loss =  0.6152132153511047\n",
            "Batch Training Loss =  0.40791264176368713\n",
            "Batch Training Loss =  0.44477593898773193\n",
            "Batch Training Loss =  0.49983352422714233\n",
            "Batch Training Loss =  0.4932132363319397\n",
            "Batch Training Loss =  0.5533035397529602\n",
            "Batch Training Loss =  0.39214298129081726\n",
            "Batch Training Loss =  0.4703904390335083\n",
            "Batch Training Loss =  0.5198219418525696\n",
            "Validation Loss in this epoch is 0.394\n",
            "This is  78 th epoch\n",
            "Batch Training Loss =  0.33389976620674133\n",
            "Batch Training Loss =  0.2925174832344055\n",
            "Batch Training Loss =  0.5038197040557861\n",
            "Batch Training Loss =  0.6152175068855286\n",
            "Batch Training Loss =  0.45592761039733887\n",
            "Batch Training Loss =  0.3077723979949951\n",
            "Batch Training Loss =  0.37534481287002563\n",
            "Batch Training Loss =  0.3854832947254181\n",
            "Batch Training Loss =  0.4015781879425049\n",
            "Batch Training Loss =  0.27676770091056824\n",
            "Batch Training Loss =  0.42201557755470276\n",
            "Batch Training Loss =  0.4523351788520813\n",
            "Batch Training Loss =  0.33423319458961487\n",
            "Batch Training Loss =  0.31383979320526123\n",
            "Batch Training Loss =  0.44856613874435425\n",
            "Batch Training Loss =  0.3460273742675781\n",
            "Batch Training Loss =  0.4755069315433502\n",
            "Batch Training Loss =  0.6037776470184326\n",
            "Batch Training Loss =  0.2855246067047119\n",
            "Batch Training Loss =  0.4844364821910858\n",
            "Batch Training Loss =  0.657653272151947\n",
            "Batch Training Loss =  0.42146748304367065\n",
            "Batch Training Loss =  0.3601921796798706\n",
            "Batch Training Loss =  0.5164378881454468\n",
            "Batch Training Loss =  0.5914201736450195\n",
            "Batch Training Loss =  0.6605510711669922\n",
            "Batch Training Loss =  0.519258439540863\n",
            "Batch Training Loss =  0.2610158324241638\n",
            "Batch Training Loss =  0.3784470856189728\n",
            "Batch Training Loss =  0.402235746383667\n",
            "Batch Training Loss =  0.5705499649047852\n",
            "Batch Training Loss =  0.8545971512794495\n",
            "Validation Loss in this epoch is 0.556\n",
            "This is  79 th epoch\n",
            "Batch Training Loss =  0.5184198617935181\n",
            "Batch Training Loss =  0.4758763015270233\n",
            "Batch Training Loss =  0.5224832892417908\n",
            "Batch Training Loss =  0.33587783575057983\n",
            "Batch Training Loss =  0.3472225069999695\n",
            "Batch Training Loss =  0.3920861780643463\n",
            "Batch Training Loss =  0.3690205514431\n",
            "Batch Training Loss =  0.3212631940841675\n",
            "Batch Training Loss =  0.3880167305469513\n",
            "Batch Training Loss =  0.3471376895904541\n",
            "Batch Training Loss =  0.2865848243236542\n",
            "Batch Training Loss =  0.3278385400772095\n",
            "Batch Training Loss =  0.2387874722480774\n",
            "Batch Training Loss =  0.3960338830947876\n",
            "Batch Training Loss =  0.566795289516449\n",
            "Batch Training Loss =  0.6662222146987915\n",
            "Batch Training Loss =  0.6782996654510498\n",
            "Batch Training Loss =  0.29094693064689636\n",
            "Batch Training Loss =  0.3209850788116455\n",
            "Batch Training Loss =  0.24260777235031128\n",
            "Batch Training Loss =  0.4229908883571625\n",
            "Batch Training Loss =  0.4783330261707306\n",
            "Batch Training Loss =  0.434805691242218\n",
            "Batch Training Loss =  0.377875417470932\n",
            "Batch Training Loss =  0.38407212495803833\n",
            "Batch Training Loss =  0.43143513798713684\n",
            "Batch Training Loss =  0.34577590227127075\n",
            "Batch Training Loss =  0.3452197313308716\n",
            "Batch Training Loss =  0.4849628210067749\n",
            "Batch Training Loss =  0.8395580053329468\n",
            "Batch Training Loss =  0.40452730655670166\n",
            "Batch Training Loss =  0.9474571943283081\n",
            "Validation Loss in this epoch is 0.852\n",
            "This is  80 th epoch\n",
            "Batch Training Loss =  0.9982166290283203\n",
            "Batch Training Loss =  0.9493661522865295\n",
            "Batch Training Loss =  0.7774382829666138\n",
            "Batch Training Loss =  0.8358383774757385\n",
            "Batch Training Loss =  0.7861617803573608\n",
            "Batch Training Loss =  0.7529260516166687\n",
            "Batch Training Loss =  0.7739573121070862\n",
            "Batch Training Loss =  0.7148300409317017\n",
            "Batch Training Loss =  0.6911173462867737\n",
            "Batch Training Loss =  0.5040470361709595\n",
            "Batch Training Loss =  0.46171921491622925\n",
            "Batch Training Loss =  0.5480177402496338\n",
            "Batch Training Loss =  0.6226243376731873\n",
            "Batch Training Loss =  0.6602115631103516\n",
            "Batch Training Loss =  0.3168864846229553\n",
            "Batch Training Loss =  0.38358473777770996\n",
            "Batch Training Loss =  0.3789811432361603\n",
            "Batch Training Loss =  0.4708569049835205\n",
            "Batch Training Loss =  0.4234909117221832\n",
            "Batch Training Loss =  0.5959059000015259\n",
            "Batch Training Loss =  0.35358738899230957\n",
            "Batch Training Loss =  0.3799663484096527\n",
            "Batch Training Loss =  0.28877589106559753\n",
            "Batch Training Loss =  0.47185540199279785\n",
            "Batch Training Loss =  0.5870697498321533\n",
            "Batch Training Loss =  0.39730304479599\n",
            "Batch Training Loss =  0.40992188453674316\n",
            "Batch Training Loss =  0.4853518307209015\n",
            "Batch Training Loss =  0.4355734884738922\n",
            "Batch Training Loss =  0.3601861298084259\n",
            "Batch Training Loss =  0.30439648032188416\n",
            "Batch Training Loss =  0.3225269615650177\n",
            "Validation Loss in this epoch is 0.441\n",
            "This is  81 th epoch\n",
            "Batch Training Loss =  0.49635544419288635\n",
            "Batch Training Loss =  0.485472708940506\n",
            "Batch Training Loss =  0.5689015984535217\n",
            "Batch Training Loss =  0.6409326195716858\n",
            "Batch Training Loss =  0.5729475021362305\n",
            "Batch Training Loss =  0.37195146083831787\n",
            "Batch Training Loss =  0.31669872999191284\n",
            "Batch Training Loss =  0.34157872200012207\n",
            "Batch Training Loss =  0.3254837095737457\n",
            "Batch Training Loss =  0.3420877754688263\n",
            "Batch Training Loss =  0.43400222063064575\n",
            "Batch Training Loss =  0.4456469714641571\n",
            "Batch Training Loss =  0.43251511454582214\n",
            "Batch Training Loss =  0.5209810137748718\n",
            "Batch Training Loss =  0.7991170287132263\n",
            "Batch Training Loss =  0.6081944108009338\n",
            "Batch Training Loss =  0.41738778352737427\n",
            "Batch Training Loss =  0.3932420313358307\n",
            "Batch Training Loss =  0.33077776432037354\n",
            "Batch Training Loss =  0.8660703897476196\n",
            "Batch Training Loss =  0.8481253981590271\n",
            "Batch Training Loss =  0.7063129544258118\n",
            "Batch Training Loss =  0.6014505624771118\n",
            "Batch Training Loss =  0.4794883728027344\n",
            "Batch Training Loss =  0.48925259709358215\n",
            "Batch Training Loss =  0.5304301977157593\n",
            "Batch Training Loss =  0.5935012102127075\n",
            "Batch Training Loss =  0.6513004302978516\n",
            "Batch Training Loss =  0.45603951811790466\n",
            "Batch Training Loss =  0.3338763117790222\n",
            "Batch Training Loss =  0.3321736454963684\n",
            "Batch Training Loss =  0.28095805644989014\n",
            "Validation Loss in this epoch is 0.428\n",
            "This is  82 th epoch\n",
            "Batch Training Loss =  0.5172787308692932\n",
            "Batch Training Loss =  0.5763136744499207\n",
            "Batch Training Loss =  0.4135545492172241\n",
            "Batch Training Loss =  0.3730790317058563\n",
            "Batch Training Loss =  0.26263314485549927\n",
            "Batch Training Loss =  0.5899184942245483\n",
            "Batch Training Loss =  0.749491810798645\n",
            "Batch Training Loss =  0.5965455770492554\n",
            "Batch Training Loss =  0.5593696236610413\n",
            "Batch Training Loss =  0.33923226594924927\n",
            "Batch Training Loss =  0.2627750635147095\n",
            "Batch Training Loss =  0.31471818685531616\n",
            "Batch Training Loss =  0.4655752182006836\n",
            "Batch Training Loss =  0.4223349094390869\n",
            "Batch Training Loss =  0.7535925507545471\n",
            "Batch Training Loss =  0.3175755441188812\n",
            "Batch Training Loss =  0.4836173355579376\n",
            "Batch Training Loss =  0.573315441608429\n",
            "Batch Training Loss =  0.6404904127120972\n",
            "Batch Training Loss =  0.3922778367996216\n",
            "Batch Training Loss =  0.42164215445518494\n",
            "Batch Training Loss =  0.32940828800201416\n",
            "Batch Training Loss =  0.42442256212234497\n",
            "Batch Training Loss =  0.606737494468689\n",
            "Batch Training Loss =  0.6967109441757202\n",
            "Batch Training Loss =  0.44282296299934387\n",
            "Batch Training Loss =  0.49913474917411804\n",
            "Batch Training Loss =  0.5628030300140381\n",
            "Batch Training Loss =  0.46223583817481995\n",
            "Batch Training Loss =  0.38331782817840576\n",
            "Batch Training Loss =  0.4354861080646515\n",
            "Batch Training Loss =  0.34257081151008606\n",
            "Validation Loss in this epoch is 0.422\n",
            "This is  83 th epoch\n",
            "Batch Training Loss =  0.3224566578865051\n",
            "Batch Training Loss =  0.21209704875946045\n",
            "Batch Training Loss =  0.47650396823883057\n",
            "Batch Training Loss =  0.35098952054977417\n",
            "Batch Training Loss =  0.44079703092575073\n",
            "Batch Training Loss =  0.3110036253929138\n",
            "Batch Training Loss =  0.3972531855106354\n",
            "Batch Training Loss =  0.380990207195282\n",
            "Batch Training Loss =  0.37966495752334595\n",
            "Batch Training Loss =  0.454445481300354\n",
            "Batch Training Loss =  0.4597831964492798\n",
            "Batch Training Loss =  0.5015656352043152\n",
            "Batch Training Loss =  0.37779369950294495\n",
            "Batch Training Loss =  0.37478533387184143\n",
            "Batch Training Loss =  0.4100354313850403\n",
            "Batch Training Loss =  0.38584986329078674\n",
            "Batch Training Loss =  0.32724758982658386\n",
            "Batch Training Loss =  0.5508036613464355\n",
            "Batch Training Loss =  0.3859177827835083\n",
            "Batch Training Loss =  0.2726716697216034\n",
            "Batch Training Loss =  0.2853517234325409\n",
            "Batch Training Loss =  0.3735172748565674\n",
            "Batch Training Loss =  0.6135305762290955\n",
            "Batch Training Loss =  0.3744775652885437\n",
            "Batch Training Loss =  0.31945040822029114\n",
            "Batch Training Loss =  0.2682129442691803\n",
            "Batch Training Loss =  0.7972715497016907\n",
            "Batch Training Loss =  0.9382293820381165\n",
            "Batch Training Loss =  0.7723132371902466\n",
            "Batch Training Loss =  0.7475863099098206\n",
            "Batch Training Loss =  0.7886534929275513\n",
            "Batch Training Loss =  0.5541161298751831\n",
            "Validation Loss in this epoch is 0.494\n",
            "This is  84 th epoch\n",
            "Batch Training Loss =  0.49315810203552246\n",
            "Batch Training Loss =  0.5300780534744263\n",
            "Batch Training Loss =  0.4832068979740143\n",
            "Batch Training Loss =  0.7577568888664246\n",
            "Batch Training Loss =  0.6834101676940918\n",
            "Batch Training Loss =  0.38955843448638916\n",
            "Batch Training Loss =  0.5245972871780396\n",
            "Batch Training Loss =  0.5640625953674316\n",
            "Batch Training Loss =  0.6310938000679016\n",
            "Batch Training Loss =  0.5251942276954651\n",
            "Batch Training Loss =  0.4538022577762604\n",
            "Batch Training Loss =  0.41281309723854065\n",
            "Batch Training Loss =  0.34473830461502075\n",
            "Batch Training Loss =  0.32106858491897583\n",
            "Batch Training Loss =  0.2788049876689911\n",
            "Batch Training Loss =  0.45274144411087036\n",
            "Batch Training Loss =  0.5058958530426025\n",
            "Batch Training Loss =  0.38837772607803345\n",
            "Batch Training Loss =  0.4191797971725464\n",
            "Batch Training Loss =  0.43543294072151184\n",
            "Batch Training Loss =  0.4232654869556427\n",
            "Batch Training Loss =  0.326177179813385\n",
            "Batch Training Loss =  0.5891781449317932\n",
            "Batch Training Loss =  0.49968498945236206\n",
            "Batch Training Loss =  0.3256078064441681\n",
            "Batch Training Loss =  0.2694985270500183\n",
            "Batch Training Loss =  0.27931058406829834\n",
            "Batch Training Loss =  0.5130046010017395\n",
            "Batch Training Loss =  0.7960646748542786\n",
            "Batch Training Loss =  0.6244719624519348\n",
            "Batch Training Loss =  0.4201025366783142\n",
            "Batch Training Loss =  0.40441253781318665\n",
            "Validation Loss in this epoch is 0.395\n",
            "This is  85 th epoch\n",
            "Batch Training Loss =  0.41024041175842285\n",
            "Batch Training Loss =  0.4244571030139923\n",
            "Batch Training Loss =  0.2758389711380005\n",
            "Batch Training Loss =  0.4350624084472656\n",
            "Batch Training Loss =  0.40306025743484497\n",
            "Batch Training Loss =  0.6331651210784912\n",
            "Batch Training Loss =  0.6601750254631042\n",
            "Batch Training Loss =  0.5016117095947266\n",
            "Batch Training Loss =  0.354246586561203\n",
            "Batch Training Loss =  0.3781074583530426\n",
            "Batch Training Loss =  0.35418882966041565\n",
            "Batch Training Loss =  0.4127275347709656\n",
            "Batch Training Loss =  0.28126537799835205\n",
            "Batch Training Loss =  0.28564709424972534\n",
            "Batch Training Loss =  0.3980647623538971\n",
            "Batch Training Loss =  0.5348618030548096\n",
            "Batch Training Loss =  0.6379326581954956\n",
            "Batch Training Loss =  0.7001891136169434\n",
            "Batch Training Loss =  0.38233816623687744\n",
            "Batch Training Loss =  0.35183361172676086\n",
            "Batch Training Loss =  0.5425107479095459\n",
            "Batch Training Loss =  0.2926290035247803\n",
            "Batch Training Loss =  0.4522976279258728\n",
            "Batch Training Loss =  0.3255435526371002\n",
            "Batch Training Loss =  0.5821774005889893\n",
            "Batch Training Loss =  0.32989487051963806\n",
            "Batch Training Loss =  0.47744113206863403\n",
            "Batch Training Loss =  0.3892470598220825\n",
            "Batch Training Loss =  0.5501847267150879\n",
            "Batch Training Loss =  0.6962661147117615\n",
            "Batch Training Loss =  0.27872809767723083\n",
            "Batch Training Loss =  0.3077187240123749\n",
            "Validation Loss in this epoch is 0.389\n",
            "This is  86 th epoch\n",
            "Batch Training Loss =  0.2695542573928833\n",
            "Batch Training Loss =  0.3888968229293823\n",
            "Batch Training Loss =  0.3095862567424774\n",
            "Batch Training Loss =  0.35397011041641235\n",
            "Batch Training Loss =  0.3734389543533325\n",
            "Batch Training Loss =  0.3844158947467804\n",
            "Batch Training Loss =  0.267356276512146\n",
            "Batch Training Loss =  0.4971749782562256\n",
            "Batch Training Loss =  0.434195339679718\n",
            "Batch Training Loss =  0.23166117072105408\n",
            "Batch Training Loss =  0.3922586441040039\n",
            "Batch Training Loss =  0.3924904763698578\n",
            "Batch Training Loss =  0.45405036211013794\n",
            "Batch Training Loss =  0.278403103351593\n",
            "Batch Training Loss =  0.3457527458667755\n",
            "Batch Training Loss =  0.5914880037307739\n",
            "Batch Training Loss =  0.7578308582305908\n",
            "Batch Training Loss =  0.8399665951728821\n",
            "Batch Training Loss =  0.682873010635376\n",
            "Batch Training Loss =  0.6956100463867188\n",
            "Batch Training Loss =  0.49595126509666443\n",
            "Batch Training Loss =  0.3803987205028534\n",
            "Batch Training Loss =  0.3256511986255646\n",
            "Batch Training Loss =  0.3836459815502167\n",
            "Batch Training Loss =  0.38243499398231506\n",
            "Batch Training Loss =  0.7876567840576172\n",
            "Batch Training Loss =  0.7968572378158569\n",
            "Batch Training Loss =  0.9074072241783142\n",
            "Batch Training Loss =  0.6382019519805908\n",
            "Batch Training Loss =  0.5915157198905945\n",
            "Batch Training Loss =  0.36892595887184143\n",
            "Batch Training Loss =  0.29469963908195496\n",
            "Validation Loss in this epoch is 0.546\n",
            "This is  87 th epoch\n",
            "Batch Training Loss =  0.4099767804145813\n",
            "Batch Training Loss =  0.5783723592758179\n",
            "Batch Training Loss =  0.26702675223350525\n",
            "Batch Training Loss =  0.35396960377693176\n",
            "Batch Training Loss =  0.31769973039627075\n",
            "Batch Training Loss =  0.40513402223587036\n",
            "Batch Training Loss =  0.3563591241836548\n",
            "Batch Training Loss =  0.37320995330810547\n",
            "Batch Training Loss =  0.4087398052215576\n",
            "Batch Training Loss =  0.3656114339828491\n",
            "Batch Training Loss =  0.4504421651363373\n",
            "Batch Training Loss =  0.3329193592071533\n",
            "Batch Training Loss =  0.28450772166252136\n",
            "Batch Training Loss =  0.4434431493282318\n",
            "Batch Training Loss =  1.1409934759140015\n",
            "Batch Training Loss =  0.8186812400817871\n",
            "Batch Training Loss =  0.8730723857879639\n",
            "Batch Training Loss =  0.8912062644958496\n",
            "Batch Training Loss =  0.7583874464035034\n",
            "Batch Training Loss =  0.4514904320240021\n",
            "Batch Training Loss =  0.558983564376831\n",
            "Batch Training Loss =  0.6149070858955383\n",
            "Batch Training Loss =  0.720804750919342\n",
            "Batch Training Loss =  0.6307400465011597\n",
            "Batch Training Loss =  0.5453433394432068\n",
            "Batch Training Loss =  0.37822434306144714\n",
            "Batch Training Loss =  0.593417763710022\n",
            "Batch Training Loss =  0.39821377396583557\n",
            "Batch Training Loss =  0.4574996531009674\n",
            "Batch Training Loss =  0.880643367767334\n",
            "Batch Training Loss =  0.7301200032234192\n",
            "Batch Training Loss =  0.5950290560722351\n",
            "Validation Loss in this epoch is 0.547\n",
            "This is  88 th epoch\n",
            "Batch Training Loss =  0.45579469203948975\n",
            "Batch Training Loss =  0.3660033047199249\n",
            "Batch Training Loss =  0.43217167258262634\n",
            "Batch Training Loss =  0.45757266879081726\n",
            "Batch Training Loss =  0.46808215975761414\n",
            "Batch Training Loss =  0.47207000851631165\n",
            "Batch Training Loss =  0.5047962069511414\n",
            "Batch Training Loss =  0.45150357484817505\n",
            "Batch Training Loss =  0.46318718791007996\n",
            "Batch Training Loss =  0.4952724575996399\n",
            "Batch Training Loss =  0.4430171847343445\n",
            "Batch Training Loss =  0.459224134683609\n",
            "Batch Training Loss =  0.4354201555252075\n",
            "Batch Training Loss =  0.48478084802627563\n",
            "Batch Training Loss =  0.43727099895477295\n",
            "Batch Training Loss =  0.4445447623729706\n",
            "Batch Training Loss =  0.3679233491420746\n",
            "Batch Training Loss =  0.3640320897102356\n",
            "Batch Training Loss =  0.37142741680145264\n",
            "Batch Training Loss =  0.529805064201355\n",
            "Batch Training Loss =  0.49041953682899475\n",
            "Batch Training Loss =  0.35380715131759644\n",
            "Batch Training Loss =  0.37234288454055786\n",
            "Batch Training Loss =  0.42388734221458435\n",
            "Batch Training Loss =  0.37715384364128113\n",
            "Batch Training Loss =  0.4317166209220886\n",
            "Batch Training Loss =  0.21173156797885895\n",
            "Batch Training Loss =  0.3297480642795563\n",
            "Batch Training Loss =  0.32524093985557556\n",
            "Batch Training Loss =  0.5469707250595093\n",
            "Batch Training Loss =  0.8612948656082153\n",
            "Batch Training Loss =  0.6970245838165283\n",
            "Validation Loss in this epoch is 0.454\n",
            "This is  89 th epoch\n",
            "Batch Training Loss =  0.40787315368652344\n",
            "Batch Training Loss =  1.1596596240997314\n",
            "Batch Training Loss =  0.8575379848480225\n",
            "Batch Training Loss =  0.8680238127708435\n",
            "Batch Training Loss =  0.8168922662734985\n",
            "Batch Training Loss =  0.6325228214263916\n",
            "Batch Training Loss =  0.7056452035903931\n",
            "Batch Training Loss =  0.7113952040672302\n",
            "Batch Training Loss =  0.6471505165100098\n",
            "Batch Training Loss =  0.5301690697669983\n",
            "Batch Training Loss =  0.4298246502876282\n",
            "Batch Training Loss =  0.6901087760925293\n",
            "Batch Training Loss =  0.7049630880355835\n",
            "Batch Training Loss =  0.8930835723876953\n",
            "Batch Training Loss =  0.7308297157287598\n",
            "Batch Training Loss =  0.8235542178153992\n",
            "Batch Training Loss =  0.7447709441184998\n",
            "Batch Training Loss =  0.5657369494438171\n",
            "Batch Training Loss =  0.46452465653419495\n",
            "Batch Training Loss =  0.5741656422615051\n",
            "Batch Training Loss =  0.5935037732124329\n",
            "Batch Training Loss =  0.5106501579284668\n",
            "Batch Training Loss =  0.46359339356422424\n",
            "Batch Training Loss =  0.6124692559242249\n",
            "Batch Training Loss =  0.38279372453689575\n",
            "Batch Training Loss =  0.39750805497169495\n",
            "Batch Training Loss =  0.5011351108551025\n",
            "Batch Training Loss =  1.2959846258163452\n",
            "Batch Training Loss =  0.8057095408439636\n",
            "Batch Training Loss =  0.8485368490219116\n",
            "Batch Training Loss =  0.6033957004547119\n",
            "Batch Training Loss =  0.6960934400558472\n",
            "Validation Loss in this epoch is 0.715\n",
            "This is  90 th epoch\n",
            "Batch Training Loss =  0.7584007382392883\n",
            "Batch Training Loss =  0.7973305583000183\n",
            "Batch Training Loss =  0.7319824695587158\n",
            "Batch Training Loss =  0.7224485278129578\n",
            "Batch Training Loss =  0.7140437960624695\n",
            "Batch Training Loss =  0.6550847887992859\n",
            "Batch Training Loss =  0.6730508208274841\n",
            "Batch Training Loss =  0.623554527759552\n",
            "Batch Training Loss =  0.6404208540916443\n",
            "Batch Training Loss =  0.5715535879135132\n",
            "Batch Training Loss =  0.563900887966156\n",
            "Batch Training Loss =  0.4352942705154419\n",
            "Batch Training Loss =  0.46758925914764404\n",
            "Batch Training Loss =  0.5566007494926453\n",
            "Batch Training Loss =  0.4387422204017639\n",
            "Batch Training Loss =  0.5285692811012268\n",
            "Batch Training Loss =  0.5804076194763184\n",
            "Batch Training Loss =  0.688173234462738\n",
            "Batch Training Loss =  0.7193251252174377\n",
            "Batch Training Loss =  0.6211768984794617\n",
            "Batch Training Loss =  0.5568276643753052\n",
            "Batch Training Loss =  0.5026368498802185\n",
            "Batch Training Loss =  0.522696852684021\n",
            "Batch Training Loss =  0.3748028874397278\n",
            "Batch Training Loss =  0.46782180666923523\n",
            "Batch Training Loss =  0.36580538749694824\n",
            "Batch Training Loss =  0.37692633271217346\n",
            "Batch Training Loss =  0.43001365661621094\n",
            "Batch Training Loss =  0.7713468670845032\n",
            "Batch Training Loss =  0.7507340312004089\n",
            "Batch Training Loss =  0.6032758951187134\n",
            "Batch Training Loss =  0.5821530222892761\n",
            "Validation Loss in this epoch is 0.516\n",
            "This is  91 th epoch\n",
            "Batch Training Loss =  0.5082552433013916\n",
            "Batch Training Loss =  0.43781977891921997\n",
            "Batch Training Loss =  0.3715772330760956\n",
            "Batch Training Loss =  0.3778972923755646\n",
            "Batch Training Loss =  0.40194571018218994\n",
            "Batch Training Loss =  0.4140275716781616\n",
            "Batch Training Loss =  0.36603301763534546\n",
            "Batch Training Loss =  0.37049734592437744\n",
            "Batch Training Loss =  0.3820078372955322\n",
            "Batch Training Loss =  0.3770945370197296\n",
            "Batch Training Loss =  0.38229918479919434\n",
            "Batch Training Loss =  0.4044637978076935\n",
            "Batch Training Loss =  0.5181244015693665\n",
            "Batch Training Loss =  0.36618572473526\n",
            "Batch Training Loss =  0.4395849108695984\n",
            "Batch Training Loss =  0.43529629707336426\n",
            "Batch Training Loss =  0.4877913296222687\n",
            "Batch Training Loss =  0.39207664132118225\n",
            "Batch Training Loss =  0.3536985516548157\n",
            "Batch Training Loss =  0.392350971698761\n",
            "Batch Training Loss =  0.35752594470977783\n",
            "Batch Training Loss =  0.2917861342430115\n",
            "Batch Training Loss =  0.2488272786140442\n",
            "Batch Training Loss =  0.5028208494186401\n",
            "Batch Training Loss =  0.4185604155063629\n",
            "Batch Training Loss =  0.29657551646232605\n",
            "Batch Training Loss =  0.42630860209465027\n",
            "Batch Training Loss =  0.4698997437953949\n",
            "Batch Training Loss =  0.39019036293029785\n",
            "Batch Training Loss =  0.3498648405075073\n",
            "Batch Training Loss =  0.812181830406189\n",
            "Batch Training Loss =  0.730875551700592\n",
            "Validation Loss in this epoch is 0.749\n",
            "This is  92 th epoch\n",
            "Batch Training Loss =  0.8905516266822815\n",
            "Batch Training Loss =  0.8940975666046143\n",
            "Batch Training Loss =  0.6287797689437866\n",
            "Batch Training Loss =  0.6724005341529846\n",
            "Batch Training Loss =  0.6090897917747498\n",
            "Batch Training Loss =  0.570135235786438\n",
            "Batch Training Loss =  0.4307745099067688\n",
            "Batch Training Loss =  0.38122063875198364\n",
            "Batch Training Loss =  0.3224017024040222\n",
            "Batch Training Loss =  0.45975223183631897\n",
            "Batch Training Loss =  0.4064503014087677\n",
            "Batch Training Loss =  0.406514048576355\n",
            "Batch Training Loss =  0.38906535506248474\n",
            "Batch Training Loss =  0.45205843448638916\n",
            "Batch Training Loss =  0.4029964804649353\n",
            "Batch Training Loss =  0.5139535069465637\n",
            "Batch Training Loss =  0.48783740401268005\n",
            "Batch Training Loss =  0.46369704604148865\n",
            "Batch Training Loss =  0.5387887954711914\n",
            "Batch Training Loss =  0.3278903067111969\n",
            "Batch Training Loss =  0.3780668377876282\n",
            "Batch Training Loss =  0.40230828523635864\n",
            "Batch Training Loss =  0.44530707597732544\n",
            "Batch Training Loss =  0.61834317445755\n",
            "Batch Training Loss =  0.3693224787712097\n",
            "Batch Training Loss =  0.40515679121017456\n",
            "Batch Training Loss =  0.3322945833206177\n",
            "Batch Training Loss =  0.40557122230529785\n",
            "Batch Training Loss =  0.482461154460907\n",
            "Batch Training Loss =  0.3466339111328125\n",
            "Batch Training Loss =  0.432346910238266\n",
            "Batch Training Loss =  0.3630903661251068\n",
            "Validation Loss in this epoch is 0.421\n",
            "This is  93 th epoch\n",
            "Batch Training Loss =  0.4043625295162201\n",
            "Batch Training Loss =  0.31472891569137573\n",
            "Batch Training Loss =  0.5805011987686157\n",
            "Batch Training Loss =  0.5280068516731262\n",
            "Batch Training Loss =  0.4836891293525696\n",
            "Batch Training Loss =  0.2996981143951416\n",
            "Batch Training Loss =  0.4074147939682007\n",
            "Batch Training Loss =  0.5054049491882324\n",
            "Batch Training Loss =  0.40428438782691956\n",
            "Batch Training Loss =  0.35955744981765747\n",
            "Batch Training Loss =  0.25418564677238464\n",
            "Batch Training Loss =  0.45774325728416443\n",
            "Batch Training Loss =  0.5421213507652283\n",
            "Batch Training Loss =  0.3934711515903473\n",
            "Batch Training Loss =  0.46740561723709106\n",
            "Batch Training Loss =  0.8013760447502136\n",
            "Batch Training Loss =  0.66466224193573\n",
            "Batch Training Loss =  0.6766359806060791\n",
            "Batch Training Loss =  0.7301625609397888\n",
            "Batch Training Loss =  0.4956108033657074\n",
            "Batch Training Loss =  0.41818466782569885\n",
            "Batch Training Loss =  0.37869536876678467\n",
            "Batch Training Loss =  0.5247893333435059\n",
            "Batch Training Loss =  1.3253684043884277\n",
            "Batch Training Loss =  0.6335513591766357\n",
            "Batch Training Loss =  0.7947608828544617\n",
            "Batch Training Loss =  0.797843337059021\n",
            "Batch Training Loss =  0.6472409963607788\n",
            "Batch Training Loss =  0.557359516620636\n",
            "Batch Training Loss =  0.3510982096195221\n",
            "Batch Training Loss =  0.2992207407951355\n",
            "Batch Training Loss =  0.3911713659763336\n",
            "Validation Loss in this epoch is 0.436\n",
            "This is  94 th epoch\n",
            "Batch Training Loss =  0.48122039437294006\n",
            "Batch Training Loss =  0.4207831919193268\n",
            "Batch Training Loss =  0.5120656490325928\n",
            "Batch Training Loss =  0.361446350812912\n",
            "Batch Training Loss =  0.45918625593185425\n",
            "Batch Training Loss =  0.41710564494132996\n",
            "Batch Training Loss =  0.682864248752594\n",
            "Batch Training Loss =  0.5052359104156494\n",
            "Batch Training Loss =  0.49125728011131287\n",
            "Batch Training Loss =  0.5063223242759705\n",
            "Batch Training Loss =  0.3782918453216553\n",
            "Batch Training Loss =  0.4229118824005127\n",
            "Batch Training Loss =  0.3167348802089691\n",
            "Batch Training Loss =  0.30280840396881104\n",
            "Batch Training Loss =  0.4912760555744171\n",
            "Batch Training Loss =  0.48394447565078735\n",
            "Batch Training Loss =  0.7552812099456787\n",
            "Batch Training Loss =  0.35153889656066895\n",
            "Batch Training Loss =  0.43449240922927856\n",
            "Batch Training Loss =  0.45305606722831726\n",
            "Batch Training Loss =  0.39263975620269775\n",
            "Batch Training Loss =  0.40331634879112244\n",
            "Batch Training Loss =  0.3041529953479767\n",
            "Batch Training Loss =  0.43745312094688416\n",
            "Batch Training Loss =  0.6473429799079895\n",
            "Batch Training Loss =  0.27600327134132385\n",
            "Batch Training Loss =  0.36053693294525146\n",
            "Batch Training Loss =  0.3062293529510498\n",
            "Batch Training Loss =  0.3241967260837555\n",
            "Batch Training Loss =  0.41298431158065796\n",
            "Batch Training Loss =  0.27977681159973145\n",
            "Batch Training Loss =  0.3143346905708313\n",
            "Validation Loss in this epoch is 0.899\n",
            "This is  95 th epoch\n",
            "Batch Training Loss =  0.5067405104637146\n",
            "Batch Training Loss =  0.7050521969795227\n",
            "Batch Training Loss =  0.6319946050643921\n",
            "Batch Training Loss =  0.49138787388801575\n",
            "Batch Training Loss =  0.6511141061782837\n",
            "Batch Training Loss =  0.5013239979743958\n",
            "Batch Training Loss =  0.6509478092193604\n",
            "Batch Training Loss =  0.5949229598045349\n",
            "Batch Training Loss =  0.50441575050354\n",
            "Batch Training Loss =  0.4847687780857086\n",
            "Batch Training Loss =  0.5707978010177612\n",
            "Batch Training Loss =  0.3655622601509094\n",
            "Batch Training Loss =  0.3814370334148407\n",
            "Batch Training Loss =  0.36461398005485535\n",
            "Batch Training Loss =  0.33636215329170227\n",
            "Batch Training Loss =  0.25198960304260254\n",
            "Batch Training Loss =  0.3536580204963684\n",
            "Batch Training Loss =  0.3991365134716034\n",
            "Batch Training Loss =  0.4363308846950531\n",
            "Batch Training Loss =  0.48185035586357117\n",
            "Batch Training Loss =  0.6734380125999451\n",
            "Batch Training Loss =  0.3778710663318634\n",
            "Batch Training Loss =  0.6076237559318542\n",
            "Batch Training Loss =  0.6257736682891846\n",
            "Batch Training Loss =  0.4220028221607208\n",
            "Batch Training Loss =  0.37195613980293274\n",
            "Batch Training Loss =  0.2553730010986328\n",
            "Batch Training Loss =  0.4099443852901459\n",
            "Batch Training Loss =  0.5516670346260071\n",
            "Batch Training Loss =  0.3491547703742981\n",
            "Batch Training Loss =  0.29646357893943787\n",
            "Batch Training Loss =  0.5267937183380127\n",
            "Validation Loss in this epoch is 0.507\n",
            "This is  96 th epoch\n",
            "Batch Training Loss =  0.44362398982048035\n",
            "Batch Training Loss =  0.7735309600830078\n",
            "Batch Training Loss =  0.7964219450950623\n",
            "Batch Training Loss =  0.6838575601577759\n",
            "Batch Training Loss =  0.6831876635551453\n",
            "Batch Training Loss =  0.46338021755218506\n",
            "Batch Training Loss =  0.5917983055114746\n",
            "Batch Training Loss =  0.6544086337089539\n",
            "Batch Training Loss =  0.42647212743759155\n",
            "Batch Training Loss =  0.6565019488334656\n",
            "Batch Training Loss =  0.6993793249130249\n",
            "Batch Training Loss =  0.6483334898948669\n",
            "Batch Training Loss =  0.6120221614837646\n",
            "Batch Training Loss =  0.5315372943878174\n",
            "Batch Training Loss =  0.3696494400501251\n",
            "Batch Training Loss =  0.5962362885475159\n",
            "Batch Training Loss =  0.5752841830253601\n",
            "Batch Training Loss =  0.35971519351005554\n",
            "Batch Training Loss =  0.47573521733283997\n",
            "Batch Training Loss =  0.44047850370407104\n",
            "Batch Training Loss =  0.4851537048816681\n",
            "Batch Training Loss =  0.44887134432792664\n",
            "Batch Training Loss =  0.4006873667240143\n",
            "Batch Training Loss =  0.4255346655845642\n",
            "Batch Training Loss =  0.43605679273605347\n",
            "Batch Training Loss =  0.4740038216114044\n",
            "Batch Training Loss =  0.3950696885585785\n",
            "Batch Training Loss =  0.37585845589637756\n",
            "Batch Training Loss =  0.4594757556915283\n",
            "Batch Training Loss =  0.2808986008167267\n",
            "Batch Training Loss =  0.29016363620758057\n",
            "Batch Training Loss =  0.2827054262161255\n",
            "Validation Loss in this epoch is 0.444\n",
            "This is  97 th epoch\n",
            "Batch Training Loss =  0.2991337180137634\n",
            "Batch Training Loss =  0.9282051920890808\n",
            "Batch Training Loss =  0.9153178930282593\n",
            "Batch Training Loss =  0.7561829090118408\n",
            "Batch Training Loss =  0.8796570301055908\n",
            "Batch Training Loss =  0.6901670694351196\n",
            "Batch Training Loss =  0.7578538656234741\n",
            "Batch Training Loss =  0.6592342257499695\n",
            "Batch Training Loss =  0.6120288372039795\n",
            "Batch Training Loss =  0.46994078159332275\n",
            "Batch Training Loss =  0.5269634127616882\n",
            "Batch Training Loss =  0.4846181869506836\n",
            "Batch Training Loss =  0.7271508574485779\n",
            "Batch Training Loss =  0.5975480079650879\n",
            "Batch Training Loss =  0.5899852514266968\n",
            "Batch Training Loss =  0.38145512342453003\n",
            "Batch Training Loss =  0.46950221061706543\n",
            "Batch Training Loss =  0.5458600521087646\n",
            "Batch Training Loss =  0.40550050139427185\n",
            "Batch Training Loss =  0.492494136095047\n",
            "Batch Training Loss =  0.5847393274307251\n",
            "Batch Training Loss =  0.7218464612960815\n",
            "Batch Training Loss =  0.7932339310646057\n",
            "Batch Training Loss =  0.7199360132217407\n",
            "Batch Training Loss =  0.7436157464981079\n",
            "Batch Training Loss =  0.5988041758537292\n",
            "Batch Training Loss =  0.39708152413368225\n",
            "Batch Training Loss =  0.408457487821579\n",
            "Batch Training Loss =  0.5234346985816956\n",
            "Batch Training Loss =  0.9594998955726624\n",
            "Batch Training Loss =  0.7235620021820068\n",
            "Batch Training Loss =  0.7047168612480164\n",
            "Validation Loss in this epoch is 0.517\n",
            "This is  98 th epoch\n",
            "Batch Training Loss =  0.4675317108631134\n",
            "Batch Training Loss =  0.33090975880622864\n",
            "Batch Training Loss =  0.5087716579437256\n",
            "Batch Training Loss =  0.5788397789001465\n",
            "Batch Training Loss =  0.4787077307701111\n",
            "Batch Training Loss =  0.34314703941345215\n",
            "Batch Training Loss =  0.3602956533432007\n",
            "Batch Training Loss =  0.3521672785282135\n",
            "Batch Training Loss =  0.46414417028427124\n",
            "Batch Training Loss =  0.4254382848739624\n",
            "Batch Training Loss =  0.649191677570343\n",
            "Batch Training Loss =  0.2329481542110443\n",
            "Batch Training Loss =  0.5882316827774048\n",
            "Batch Training Loss =  0.6084544062614441\n",
            "Batch Training Loss =  0.41631263494491577\n",
            "Batch Training Loss =  0.4065442681312561\n",
            "Batch Training Loss =  0.43898144364356995\n",
            "Batch Training Loss =  0.4212636947631836\n",
            "Batch Training Loss =  0.5696458220481873\n",
            "Batch Training Loss =  0.6354516744613647\n",
            "Batch Training Loss =  0.5110394954681396\n",
            "Batch Training Loss =  0.42846786975860596\n",
            "Batch Training Loss =  0.45743778347969055\n",
            "Batch Training Loss =  0.378418892621994\n",
            "Batch Training Loss =  0.38639652729034424\n",
            "Batch Training Loss =  0.5087255835533142\n",
            "Batch Training Loss =  0.39175671339035034\n",
            "Batch Training Loss =  0.4945923686027527\n",
            "Batch Training Loss =  0.5697968006134033\n",
            "Batch Training Loss =  0.5756193399429321\n",
            "Batch Training Loss =  0.4396701157093048\n",
            "Batch Training Loss =  0.5207657814025879\n",
            "Validation Loss in this epoch is 0.497\n",
            "This is  99 th epoch\n",
            "Batch Training Loss =  0.558488130569458\n",
            "Batch Training Loss =  0.4659847021102905\n",
            "Batch Training Loss =  0.38097745180130005\n",
            "Batch Training Loss =  0.39586445689201355\n",
            "Batch Training Loss =  0.3352471888065338\n",
            "Batch Training Loss =  0.4585534632205963\n",
            "Batch Training Loss =  0.4255802631378174\n",
            "Batch Training Loss =  0.5379530787467957\n",
            "Batch Training Loss =  0.4538981318473816\n",
            "Batch Training Loss =  0.47254809737205505\n",
            "Batch Training Loss =  0.3962438106536865\n",
            "Batch Training Loss =  0.4291169345378876\n",
            "Batch Training Loss =  0.3151721954345703\n",
            "Batch Training Loss =  0.7271374464035034\n",
            "Batch Training Loss =  0.6316482424736023\n",
            "Batch Training Loss =  0.6027079820632935\n",
            "Batch Training Loss =  0.5672509670257568\n",
            "Batch Training Loss =  0.430471271276474\n",
            "Batch Training Loss =  0.30229854583740234\n",
            "Batch Training Loss =  0.43892642855644226\n",
            "Batch Training Loss =  0.34090301394462585\n",
            "Batch Training Loss =  0.3257826268672943\n",
            "Batch Training Loss =  0.3259209096431732\n",
            "Batch Training Loss =  0.4851629137992859\n",
            "Batch Training Loss =  0.80623459815979\n",
            "Batch Training Loss =  0.5604146718978882\n",
            "Batch Training Loss =  0.4210086464881897\n",
            "Batch Training Loss =  0.3073793053627014\n",
            "Batch Training Loss =  0.35139983892440796\n",
            "Batch Training Loss =  0.40125250816345215\n",
            "Batch Training Loss =  0.4312862753868103\n",
            "Batch Training Loss =  0.3590833246707916\n",
            "Validation Loss in this epoch is 0.411\n",
            "This is  100 th epoch\n",
            "Batch Training Loss =  0.35917794704437256\n",
            "Batch Training Loss =  0.6724921464920044\n",
            "Batch Training Loss =  0.37802374362945557\n",
            "Batch Training Loss =  0.32275235652923584\n",
            "Batch Training Loss =  0.3231721520423889\n",
            "Batch Training Loss =  0.41934993863105774\n",
            "Batch Training Loss =  0.4531521499156952\n",
            "Batch Training Loss =  0.33644890785217285\n",
            "Batch Training Loss =  0.3098021447658539\n",
            "Batch Training Loss =  0.33738794922828674\n",
            "Batch Training Loss =  0.3832096457481384\n",
            "Batch Training Loss =  0.416029155254364\n",
            "Batch Training Loss =  0.36225396394729614\n",
            "Batch Training Loss =  0.2999515235424042\n",
            "Batch Training Loss =  0.397978812456131\n",
            "Batch Training Loss =  0.746718168258667\n",
            "Batch Training Loss =  0.8312879204750061\n",
            "Batch Training Loss =  0.5255701541900635\n",
            "Batch Training Loss =  0.44967418909072876\n",
            "Batch Training Loss =  0.44884243607521057\n",
            "Batch Training Loss =  0.726205587387085\n",
            "Batch Training Loss =  0.8545045256614685\n",
            "Batch Training Loss =  0.8651427626609802\n",
            "Batch Training Loss =  0.8505361676216125\n",
            "Batch Training Loss =  0.8589670062065125\n",
            "Batch Training Loss =  0.9101768732070923\n",
            "Batch Training Loss =  0.8717621564865112\n",
            "Batch Training Loss =  0.8028790354728699\n",
            "Batch Training Loss =  0.7859625220298767\n",
            "Batch Training Loss =  0.6970072388648987\n",
            "Batch Training Loss =  0.7387943863868713\n",
            "Batch Training Loss =  0.7048754096031189\n",
            "Validation Loss in this epoch is 0.676\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6949451565742493\n",
            "Batch Training Loss =  0.7459973692893982\n",
            "Batch Training Loss =  0.8898369073867798\n",
            "Batch Training Loss =  0.6750066876411438\n",
            "Batch Training Loss =  0.712810218334198\n",
            "Batch Training Loss =  0.7607122659683228\n",
            "Batch Training Loss =  0.6910959482192993\n",
            "Batch Training Loss =  0.688126802444458\n",
            "Batch Training Loss =  0.6831002235412598\n",
            "Batch Training Loss =  0.6726053357124329\n",
            "Batch Training Loss =  0.653968870639801\n",
            "Batch Training Loss =  0.626737117767334\n",
            "Batch Training Loss =  0.6901149749755859\n",
            "Batch Training Loss =  0.8967676758766174\n",
            "Batch Training Loss =  0.6808651685714722\n",
            "Batch Training Loss =  0.6984189748764038\n",
            "Batch Training Loss =  0.6827111840248108\n",
            "Batch Training Loss =  0.68938148021698\n",
            "Batch Training Loss =  0.6744780540466309\n",
            "Batch Training Loss =  0.6707758903503418\n",
            "Batch Training Loss =  0.6588723659515381\n",
            "Batch Training Loss =  0.6785588264465332\n",
            "Batch Training Loss =  0.6833307147026062\n",
            "Batch Training Loss =  0.674773097038269\n",
            "Batch Training Loss =  0.6423998475074768\n",
            "Batch Training Loss =  0.634958028793335\n",
            "Batch Training Loss =  0.6292835474014282\n",
            "Batch Training Loss =  0.5993789434432983\n",
            "Batch Training Loss =  0.6024979948997498\n",
            "Batch Training Loss =  0.7537805438041687\n",
            "Batch Training Loss =  0.7519555687904358\n",
            "Batch Training Loss =  0.6827489733695984\n",
            "Validation Loss in this epoch is 0.667\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.6634926795959473\n",
            "Batch Training Loss =  0.6331984996795654\n",
            "Batch Training Loss =  0.6082620024681091\n",
            "Batch Training Loss =  0.5936236381530762\n",
            "Batch Training Loss =  0.597730278968811\n",
            "Batch Training Loss =  0.580593466758728\n",
            "Batch Training Loss =  0.6504848599433899\n",
            "Batch Training Loss =  0.565483808517456\n",
            "Batch Training Loss =  0.5312004685401917\n",
            "Batch Training Loss =  0.5115798711776733\n",
            "Batch Training Loss =  0.5634997487068176\n",
            "Batch Training Loss =  1.9090057611465454\n",
            "Batch Training Loss =  0.6866869926452637\n",
            "Batch Training Loss =  0.6752107739448547\n",
            "Batch Training Loss =  0.6970474123954773\n",
            "Batch Training Loss =  0.6522271037101746\n",
            "Batch Training Loss =  0.6794350147247314\n",
            "Batch Training Loss =  0.6591464877128601\n",
            "Batch Training Loss =  0.6937309503555298\n",
            "Batch Training Loss =  0.6828885674476624\n",
            "Batch Training Loss =  0.6740329265594482\n",
            "Batch Training Loss =  0.6654776930809021\n",
            "Batch Training Loss =  0.6775326728820801\n",
            "Batch Training Loss =  0.6677398681640625\n",
            "Batch Training Loss =  0.6439412832260132\n",
            "Batch Training Loss =  0.6392600536346436\n",
            "Batch Training Loss =  0.6519020795822144\n",
            "Batch Training Loss =  0.6122669577598572\n",
            "Batch Training Loss =  0.6445457935333252\n",
            "Batch Training Loss =  0.7124555706977844\n",
            "Batch Training Loss =  0.6563263535499573\n",
            "Batch Training Loss =  0.6468683481216431\n",
            "Validation Loss in this epoch is 0.607\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.5867526531219482\n",
            "Batch Training Loss =  0.5929491519927979\n",
            "Batch Training Loss =  1.1634999513626099\n",
            "Batch Training Loss =  0.766115128993988\n",
            "Batch Training Loss =  0.6679041385650635\n",
            "Batch Training Loss =  0.6906436085700989\n",
            "Batch Training Loss =  0.6696911454200745\n",
            "Batch Training Loss =  0.6756880879402161\n",
            "Batch Training Loss =  0.6702520847320557\n",
            "Batch Training Loss =  0.6206942200660706\n",
            "Batch Training Loss =  0.6268692016601562\n",
            "Batch Training Loss =  0.5791773796081543\n",
            "Batch Training Loss =  0.5892982482910156\n",
            "Batch Training Loss =  0.5816802382469177\n",
            "Batch Training Loss =  0.7015732526779175\n",
            "Batch Training Loss =  0.8218419551849365\n",
            "Batch Training Loss =  0.6962785124778748\n",
            "Batch Training Loss =  0.6829923391342163\n",
            "Batch Training Loss =  0.6343358755111694\n",
            "Batch Training Loss =  0.6049007177352905\n",
            "Batch Training Loss =  0.539252519607544\n",
            "Batch Training Loss =  0.6359063982963562\n",
            "Batch Training Loss =  0.8049294352531433\n",
            "Batch Training Loss =  0.6636128425598145\n",
            "Batch Training Loss =  0.6286373138427734\n",
            "Batch Training Loss =  0.5978606343269348\n",
            "Batch Training Loss =  0.5588998794555664\n",
            "Batch Training Loss =  0.611909806728363\n",
            "Batch Training Loss =  0.5589360594749451\n",
            "Batch Training Loss =  0.49186789989471436\n",
            "Batch Training Loss =  0.5925166010856628\n",
            "Batch Training Loss =  0.5080901980400085\n",
            "Validation Loss in this epoch is 0.576\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.49811431765556335\n",
            "Batch Training Loss =  0.45676833391189575\n",
            "Batch Training Loss =  0.8784471154212952\n",
            "Batch Training Loss =  0.7701255083084106\n",
            "Batch Training Loss =  0.6874722242355347\n",
            "Batch Training Loss =  0.6570063829421997\n",
            "Batch Training Loss =  0.6289450526237488\n",
            "Batch Training Loss =  0.6515920758247375\n",
            "Batch Training Loss =  0.6191126704216003\n",
            "Batch Training Loss =  0.7250964641571045\n",
            "Batch Training Loss =  0.6173869967460632\n",
            "Batch Training Loss =  0.5563330054283142\n",
            "Batch Training Loss =  0.4885714650154114\n",
            "Batch Training Loss =  0.5682642459869385\n",
            "Batch Training Loss =  0.4912455677986145\n",
            "Batch Training Loss =  0.751696765422821\n",
            "Batch Training Loss =  0.9011422991752625\n",
            "Batch Training Loss =  0.6854438185691833\n",
            "Batch Training Loss =  0.6696785688400269\n",
            "Batch Training Loss =  0.658595860004425\n",
            "Batch Training Loss =  0.6423385739326477\n",
            "Batch Training Loss =  0.6260276436805725\n",
            "Batch Training Loss =  0.5709226727485657\n",
            "Batch Training Loss =  0.5663437843322754\n",
            "Batch Training Loss =  0.5615144968032837\n",
            "Batch Training Loss =  0.7320310473442078\n",
            "Batch Training Loss =  0.6608074903488159\n",
            "Batch Training Loss =  0.6454347372055054\n",
            "Batch Training Loss =  0.6705529093742371\n",
            "Batch Training Loss =  0.6095687747001648\n",
            "Batch Training Loss =  0.5558392405509949\n",
            "Batch Training Loss =  0.531817615032196\n",
            "Validation Loss in this epoch is 0.550\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.5672364234924316\n",
            "Batch Training Loss =  0.8003337979316711\n",
            "Batch Training Loss =  0.6408438086509705\n",
            "Batch Training Loss =  0.5484916567802429\n",
            "Batch Training Loss =  0.49858543276786804\n",
            "Batch Training Loss =  0.45941925048828125\n",
            "Batch Training Loss =  0.6288690567016602\n",
            "Batch Training Loss =  0.7854855060577393\n",
            "Batch Training Loss =  0.5947688221931458\n",
            "Batch Training Loss =  0.5628989338874817\n",
            "Batch Training Loss =  0.5041377544403076\n",
            "Batch Training Loss =  0.5257676839828491\n",
            "Batch Training Loss =  0.6168484091758728\n",
            "Batch Training Loss =  0.5323547720909119\n",
            "Batch Training Loss =  0.6432248950004578\n",
            "Batch Training Loss =  0.6368716955184937\n",
            "Batch Training Loss =  0.6079747676849365\n",
            "Batch Training Loss =  0.5396558046340942\n",
            "Batch Training Loss =  0.48830145597457886\n",
            "Batch Training Loss =  0.4330880641937256\n",
            "Batch Training Loss =  0.3758135139942169\n",
            "Batch Training Loss =  0.559165358543396\n",
            "Batch Training Loss =  0.5548092722892761\n",
            "Batch Training Loss =  0.533623993396759\n",
            "Batch Training Loss =  0.502229630947113\n",
            "Batch Training Loss =  0.4585360586643219\n",
            "Batch Training Loss =  0.41856932640075684\n",
            "Batch Training Loss =  0.5194844603538513\n",
            "Batch Training Loss =  1.1051825284957886\n",
            "Batch Training Loss =  0.7635862231254578\n",
            "Batch Training Loss =  0.6417325735092163\n",
            "Batch Training Loss =  0.6230618357658386\n",
            "Validation Loss in this epoch is 0.610\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.5769607424736023\n",
            "Batch Training Loss =  0.5232532620429993\n",
            "Batch Training Loss =  0.6400269269943237\n",
            "Batch Training Loss =  0.6310343146324158\n",
            "Batch Training Loss =  0.6124826669692993\n",
            "Batch Training Loss =  0.5444374680519104\n",
            "Batch Training Loss =  0.5304295420646667\n",
            "Batch Training Loss =  0.5819865465164185\n",
            "Batch Training Loss =  0.49892905354499817\n",
            "Batch Training Loss =  0.6028627753257751\n",
            "Batch Training Loss =  0.5730978846549988\n",
            "Batch Training Loss =  0.5243505835533142\n",
            "Batch Training Loss =  0.5855167508125305\n",
            "Batch Training Loss =  0.7119510173797607\n",
            "Batch Training Loss =  0.5990043878555298\n",
            "Batch Training Loss =  0.5531738996505737\n",
            "Batch Training Loss =  0.6336678862571716\n",
            "Batch Training Loss =  0.541832447052002\n",
            "Batch Training Loss =  0.4241607189178467\n",
            "Batch Training Loss =  0.576463520526886\n",
            "Batch Training Loss =  1.087585687637329\n",
            "Batch Training Loss =  0.5900928378105164\n",
            "Batch Training Loss =  0.6035831570625305\n",
            "Batch Training Loss =  0.6214559674263\n",
            "Batch Training Loss =  0.5238270163536072\n",
            "Batch Training Loss =  0.49211615324020386\n",
            "Batch Training Loss =  0.5532242655754089\n",
            "Batch Training Loss =  0.5239207744598389\n",
            "Batch Training Loss =  0.4919929802417755\n",
            "Batch Training Loss =  0.4196659326553345\n",
            "Batch Training Loss =  0.4214841425418854\n",
            "Batch Training Loss =  0.7924360632896423\n",
            "Validation Loss in this epoch is 0.921\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  1.1187355518341064\n",
            "Batch Training Loss =  0.7118451595306396\n",
            "Batch Training Loss =  0.7125292420387268\n",
            "Batch Training Loss =  0.7155635952949524\n",
            "Batch Training Loss =  0.68084317445755\n",
            "Batch Training Loss =  0.6788719296455383\n",
            "Batch Training Loss =  0.6596632599830627\n",
            "Batch Training Loss =  0.6510375738143921\n",
            "Batch Training Loss =  0.623831033706665\n",
            "Batch Training Loss =  0.5120528936386108\n",
            "Batch Training Loss =  0.5886361598968506\n",
            "Batch Training Loss =  0.6592102646827698\n",
            "Batch Training Loss =  0.5779471397399902\n",
            "Batch Training Loss =  0.4658883810043335\n",
            "Batch Training Loss =  0.45709091424942017\n",
            "Batch Training Loss =  0.4977726638317108\n",
            "Batch Training Loss =  0.622734010219574\n",
            "Batch Training Loss =  0.7691285610198975\n",
            "Batch Training Loss =  0.6388325095176697\n",
            "Batch Training Loss =  0.6278904676437378\n",
            "Batch Training Loss =  0.5169473886489868\n",
            "Batch Training Loss =  0.6943586468696594\n",
            "Batch Training Loss =  0.5856817960739136\n",
            "Batch Training Loss =  0.5794224739074707\n",
            "Batch Training Loss =  0.5226429104804993\n",
            "Batch Training Loss =  0.5296450257301331\n",
            "Batch Training Loss =  0.5454157590866089\n",
            "Batch Training Loss =  0.44278189539909363\n",
            "Batch Training Loss =  0.5018609762191772\n",
            "Batch Training Loss =  0.6151723265647888\n",
            "Batch Training Loss =  0.7324881553649902\n",
            "Batch Training Loss =  0.6040038466453552\n",
            "Validation Loss in this epoch is 0.556\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.594739556312561\n",
            "Batch Training Loss =  0.5353598594665527\n",
            "Batch Training Loss =  0.5429973006248474\n",
            "Batch Training Loss =  0.4569965600967407\n",
            "Batch Training Loss =  0.43483906984329224\n",
            "Batch Training Loss =  0.4151704013347626\n",
            "Batch Training Loss =  0.5133811235427856\n",
            "Batch Training Loss =  0.6111005544662476\n",
            "Batch Training Loss =  0.4302607774734497\n",
            "Batch Training Loss =  0.3943798542022705\n",
            "Batch Training Loss =  0.4802251160144806\n",
            "Batch Training Loss =  0.4587119221687317\n",
            "Batch Training Loss =  0.47044795751571655\n",
            "Batch Training Loss =  0.9375981688499451\n",
            "Batch Training Loss =  0.5208423733711243\n",
            "Batch Training Loss =  0.5358057618141174\n",
            "Batch Training Loss =  0.4094966948032379\n",
            "Batch Training Loss =  0.5683724880218506\n",
            "Batch Training Loss =  0.5924085974693298\n",
            "Batch Training Loss =  0.44093653559684753\n",
            "Batch Training Loss =  0.5832968950271606\n",
            "Batch Training Loss =  0.6468890309333801\n",
            "Batch Training Loss =  0.42549675703048706\n",
            "Batch Training Loss =  0.4916652739048004\n",
            "Batch Training Loss =  0.4693716764450073\n",
            "Batch Training Loss =  0.5445951819419861\n",
            "Batch Training Loss =  0.49497413635253906\n",
            "Batch Training Loss =  0.5584160089492798\n",
            "Batch Training Loss =  0.46631938219070435\n",
            "Batch Training Loss =  0.3920435309410095\n",
            "Batch Training Loss =  0.45049849152565\n",
            "Batch Training Loss =  0.47266510128974915\n",
            "Validation Loss in this epoch is 0.805\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.7700908184051514\n",
            "Batch Training Loss =  0.4283607006072998\n",
            "Batch Training Loss =  0.35330504179000854\n",
            "Batch Training Loss =  0.4477861821651459\n",
            "Batch Training Loss =  0.5327338576316833\n",
            "Batch Training Loss =  0.44867148995399475\n",
            "Batch Training Loss =  0.4618420898914337\n",
            "Batch Training Loss =  0.42010220885276794\n",
            "Batch Training Loss =  0.49728912115097046\n",
            "Batch Training Loss =  0.3216075003147125\n",
            "Batch Training Loss =  0.4851425290107727\n",
            "Batch Training Loss =  0.6616158485412598\n",
            "Batch Training Loss =  0.9592041969299316\n",
            "Batch Training Loss =  0.677493691444397\n",
            "Batch Training Loss =  0.7638108730316162\n",
            "Batch Training Loss =  0.7755934000015259\n",
            "Batch Training Loss =  0.8015159368515015\n",
            "Batch Training Loss =  0.6661618947982788\n",
            "Batch Training Loss =  0.595661461353302\n",
            "Batch Training Loss =  0.6443009376525879\n",
            "Batch Training Loss =  0.5225216150283813\n",
            "Batch Training Loss =  0.7920796275138855\n",
            "Batch Training Loss =  0.7602418661117554\n",
            "Batch Training Loss =  0.7209751009941101\n",
            "Batch Training Loss =  0.654148280620575\n",
            "Batch Training Loss =  0.5213382244110107\n",
            "Batch Training Loss =  0.5070921182632446\n",
            "Batch Training Loss =  0.5202107429504395\n",
            "Batch Training Loss =  0.46706435084342957\n",
            "Batch Training Loss =  0.482843279838562\n",
            "Batch Training Loss =  0.4415972828865051\n",
            "Batch Training Loss =  0.4789176881313324\n",
            "Validation Loss in this epoch is 0.953\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.923602283000946\n",
            "Batch Training Loss =  0.5253716111183167\n",
            "Batch Training Loss =  0.48900192975997925\n",
            "Batch Training Loss =  0.4726465940475464\n",
            "Batch Training Loss =  0.5304362177848816\n",
            "Batch Training Loss =  0.5394289493560791\n",
            "Batch Training Loss =  0.6718623638153076\n",
            "Batch Training Loss =  0.5045473575592041\n",
            "Batch Training Loss =  0.44227275252342224\n",
            "Batch Training Loss =  0.3480781614780426\n",
            "Batch Training Loss =  0.40054768323898315\n",
            "Batch Training Loss =  0.5926811099052429\n",
            "Batch Training Loss =  0.9773644804954529\n",
            "Batch Training Loss =  0.6556686162948608\n",
            "Batch Training Loss =  0.4807188808917999\n",
            "Batch Training Loss =  0.5282566547393799\n",
            "Batch Training Loss =  0.6234006285667419\n",
            "Batch Training Loss =  0.5902984142303467\n",
            "Batch Training Loss =  0.6860169768333435\n",
            "Batch Training Loss =  0.48545533418655396\n",
            "Batch Training Loss =  0.47696346044540405\n",
            "Batch Training Loss =  0.3911367356777191\n",
            "Batch Training Loss =  0.5458992123603821\n",
            "Batch Training Loss =  0.5679681897163391\n",
            "Batch Training Loss =  0.6530277132987976\n",
            "Batch Training Loss =  0.504908561706543\n",
            "Batch Training Loss =  0.40742218494415283\n",
            "Batch Training Loss =  0.46855083107948303\n",
            "Batch Training Loss =  0.4740733206272125\n",
            "Batch Training Loss =  0.6384260058403015\n",
            "Batch Training Loss =  0.5054630041122437\n",
            "Batch Training Loss =  0.5390158295631409\n",
            "Validation Loss in this epoch is 0.476\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.44690918922424316\n",
            "Batch Training Loss =  0.39228421449661255\n",
            "Batch Training Loss =  0.5009194016456604\n",
            "Batch Training Loss =  0.45600906014442444\n",
            "Batch Training Loss =  0.578228771686554\n",
            "Batch Training Loss =  0.43364182114601135\n",
            "Batch Training Loss =  0.41963180899620056\n",
            "Batch Training Loss =  0.4809172749519348\n",
            "Batch Training Loss =  0.4147723913192749\n",
            "Batch Training Loss =  0.4457992613315582\n",
            "Batch Training Loss =  0.2929064929485321\n",
            "Batch Training Loss =  0.6709098815917969\n",
            "Batch Training Loss =  0.4529365301132202\n",
            "Batch Training Loss =  0.5089766979217529\n",
            "Batch Training Loss =  0.3875105381011963\n",
            "Batch Training Loss =  0.3886632025241852\n",
            "Batch Training Loss =  0.5255786776542664\n",
            "Batch Training Loss =  0.5099206566810608\n",
            "Batch Training Loss =  0.6813768744468689\n",
            "Batch Training Loss =  0.4617384672164917\n",
            "Batch Training Loss =  0.4512360095977783\n",
            "Batch Training Loss =  0.5063275098800659\n",
            "Batch Training Loss =  0.5156788229942322\n",
            "Batch Training Loss =  0.5505366325378418\n",
            "Batch Training Loss =  0.5673147439956665\n",
            "Batch Training Loss =  0.40970975160598755\n",
            "Batch Training Loss =  0.3674335479736328\n",
            "Batch Training Loss =  0.32507094740867615\n",
            "Batch Training Loss =  0.517329216003418\n",
            "Batch Training Loss =  0.8978906869888306\n",
            "Batch Training Loss =  0.3911518156528473\n",
            "Batch Training Loss =  0.5868306159973145\n",
            "Validation Loss in this epoch is 0.649\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.5827953219413757\n",
            "Batch Training Loss =  0.5501663684844971\n",
            "Batch Training Loss =  0.4534637928009033\n",
            "Batch Training Loss =  0.39116477966308594\n",
            "Batch Training Loss =  0.36815524101257324\n",
            "Batch Training Loss =  0.4469546377658844\n",
            "Batch Training Loss =  0.470340371131897\n",
            "Batch Training Loss =  0.477957159280777\n",
            "Batch Training Loss =  0.4123478829860687\n",
            "Batch Training Loss =  0.4399772584438324\n",
            "Batch Training Loss =  0.44648265838623047\n",
            "Batch Training Loss =  0.6656416058540344\n",
            "Batch Training Loss =  0.650408148765564\n",
            "Batch Training Loss =  0.5917776823043823\n",
            "Batch Training Loss =  0.5359742641448975\n",
            "Batch Training Loss =  0.5220119953155518\n",
            "Batch Training Loss =  0.4135534167289734\n",
            "Batch Training Loss =  0.47682392597198486\n",
            "Batch Training Loss =  0.561930239200592\n",
            "Batch Training Loss =  0.42511600255966187\n",
            "Batch Training Loss =  0.31347352266311646\n",
            "Batch Training Loss =  0.4451112747192383\n",
            "Batch Training Loss =  0.4798375070095062\n",
            "Batch Training Loss =  0.3792915642261505\n",
            "Batch Training Loss =  0.4895929992198944\n",
            "Batch Training Loss =  0.5072841644287109\n",
            "Batch Training Loss =  0.4373045265674591\n",
            "Batch Training Loss =  0.4791490137577057\n",
            "Batch Training Loss =  0.33788397908210754\n",
            "Batch Training Loss =  0.30069735646247864\n",
            "Batch Training Loss =  0.44610387086868286\n",
            "Batch Training Loss =  0.8647276163101196\n",
            "Validation Loss in this epoch is 0.889\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.8397977948188782\n",
            "Batch Training Loss =  0.759616494178772\n",
            "Batch Training Loss =  0.7288172841072083\n",
            "Batch Training Loss =  0.7683893442153931\n",
            "Batch Training Loss =  0.6549718379974365\n",
            "Batch Training Loss =  0.6036585569381714\n",
            "Batch Training Loss =  0.5939421057701111\n",
            "Batch Training Loss =  0.5459269881248474\n",
            "Batch Training Loss =  0.5137620568275452\n",
            "Batch Training Loss =  0.6328324675559998\n",
            "Batch Training Loss =  0.4121820032596588\n",
            "Batch Training Loss =  0.4334973394870758\n",
            "Batch Training Loss =  0.49604666233062744\n",
            "Batch Training Loss =  0.7336768507957458\n",
            "Batch Training Loss =  0.46962615847587585\n",
            "Batch Training Loss =  0.5631501078605652\n",
            "Batch Training Loss =  0.509528636932373\n",
            "Batch Training Loss =  0.5989460945129395\n",
            "Batch Training Loss =  0.5331212878227234\n",
            "Batch Training Loss =  0.4752303659915924\n",
            "Batch Training Loss =  0.37485581636428833\n",
            "Batch Training Loss =  0.491415798664093\n",
            "Batch Training Loss =  0.43583688139915466\n",
            "Batch Training Loss =  0.46003949642181396\n",
            "Batch Training Loss =  0.48050743341445923\n",
            "Batch Training Loss =  0.46970364451408386\n",
            "Batch Training Loss =  0.5771746635437012\n",
            "Batch Training Loss =  0.5214386582374573\n",
            "Batch Training Loss =  0.5026443004608154\n",
            "Batch Training Loss =  0.4888868033885956\n",
            "Batch Training Loss =  0.4669302999973297\n",
            "Batch Training Loss =  0.5056964159011841\n",
            "Validation Loss in this epoch is 0.435\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.3466498553752899\n",
            "Batch Training Loss =  0.32769492268562317\n",
            "Batch Training Loss =  0.5816773772239685\n",
            "Batch Training Loss =  0.3534016013145447\n",
            "Batch Training Loss =  0.3364177346229553\n",
            "Batch Training Loss =  0.413371205329895\n",
            "Batch Training Loss =  1.051552176475525\n",
            "Batch Training Loss =  0.6285158395767212\n",
            "Batch Training Loss =  0.7119367718696594\n",
            "Batch Training Loss =  0.6910750865936279\n",
            "Batch Training Loss =  0.5937552452087402\n",
            "Batch Training Loss =  0.5717723965644836\n",
            "Batch Training Loss =  0.5592323541641235\n",
            "Batch Training Loss =  0.4861263632774353\n",
            "Batch Training Loss =  0.6320490837097168\n",
            "Batch Training Loss =  0.9722699522972107\n",
            "Batch Training Loss =  0.574827253818512\n",
            "Batch Training Loss =  0.4933263063430786\n",
            "Batch Training Loss =  0.5609316229820251\n",
            "Batch Training Loss =  0.5735240578651428\n",
            "Batch Training Loss =  0.46215274930000305\n",
            "Batch Training Loss =  0.43718382716178894\n",
            "Batch Training Loss =  0.4911211133003235\n",
            "Batch Training Loss =  0.536785364151001\n",
            "Batch Training Loss =  0.9402047991752625\n",
            "Batch Training Loss =  0.6051498055458069\n",
            "Batch Training Loss =  0.512728214263916\n",
            "Batch Training Loss =  0.9209717512130737\n",
            "Batch Training Loss =  0.697049081325531\n",
            "Batch Training Loss =  0.7816662788391113\n",
            "Batch Training Loss =  0.6902100443840027\n",
            "Batch Training Loss =  0.7202844023704529\n",
            "Validation Loss in this epoch is 0.650\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6449518799781799\n",
            "Batch Training Loss =  0.6328661441802979\n",
            "Batch Training Loss =  0.5836919546127319\n",
            "Batch Training Loss =  0.5513870716094971\n",
            "Batch Training Loss =  0.4807001054286957\n",
            "Batch Training Loss =  0.40618982911109924\n",
            "Batch Training Loss =  0.4318951666355133\n",
            "Batch Training Loss =  0.38598576188087463\n",
            "Batch Training Loss =  0.4290531873703003\n",
            "Batch Training Loss =  0.7420040369033813\n",
            "Batch Training Loss =  0.7813831567764282\n",
            "Batch Training Loss =  0.6707630157470703\n",
            "Batch Training Loss =  0.6454187035560608\n",
            "Batch Training Loss =  0.5845547318458557\n",
            "Batch Training Loss =  0.4735066890716553\n",
            "Batch Training Loss =  0.44767865538597107\n",
            "Batch Training Loss =  0.45157530903816223\n",
            "Batch Training Loss =  0.43085429072380066\n",
            "Batch Training Loss =  0.4302588403224945\n",
            "Batch Training Loss =  0.4947401285171509\n",
            "Batch Training Loss =  0.3326762020587921\n",
            "Batch Training Loss =  0.574350893497467\n",
            "Batch Training Loss =  0.8682636022567749\n",
            "Batch Training Loss =  0.5696418881416321\n",
            "Batch Training Loss =  0.5993247032165527\n",
            "Batch Training Loss =  0.6908658742904663\n",
            "Batch Training Loss =  0.5602359175682068\n",
            "Batch Training Loss =  0.5051054954528809\n",
            "Batch Training Loss =  0.4816732704639435\n",
            "Batch Training Loss =  0.4287683963775635\n",
            "Batch Training Loss =  0.3315964639186859\n",
            "Batch Training Loss =  0.29379546642303467\n",
            "Validation Loss in this epoch is 0.408\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.4454106092453003\n",
            "Batch Training Loss =  0.61774080991745\n",
            "Batch Training Loss =  0.5867254734039307\n",
            "Batch Training Loss =  0.6079437136650085\n",
            "Batch Training Loss =  0.48685675859451294\n",
            "Batch Training Loss =  0.3806896507740021\n",
            "Batch Training Loss =  0.25389307737350464\n",
            "Batch Training Loss =  0.40215572714805603\n",
            "Batch Training Loss =  0.5716421604156494\n",
            "Batch Training Loss =  0.6145541667938232\n",
            "Batch Training Loss =  0.5285537838935852\n",
            "Batch Training Loss =  0.37494751811027527\n",
            "Batch Training Loss =  0.3780646026134491\n",
            "Batch Training Loss =  0.3781713843345642\n",
            "Batch Training Loss =  0.30029407143592834\n",
            "Batch Training Loss =  0.5385743379592896\n",
            "Batch Training Loss =  0.4308958947658539\n",
            "Batch Training Loss =  0.48768600821495056\n",
            "Batch Training Loss =  0.538402259349823\n",
            "Batch Training Loss =  0.4655802845954895\n",
            "Batch Training Loss =  0.3141656219959259\n",
            "Batch Training Loss =  0.36723780632019043\n",
            "Batch Training Loss =  0.8069105744361877\n",
            "Batch Training Loss =  0.7122951745986938\n",
            "Batch Training Loss =  0.7143080830574036\n",
            "Batch Training Loss =  0.5986883044242859\n",
            "Batch Training Loss =  0.569710373878479\n",
            "Batch Training Loss =  0.5815986394882202\n",
            "Batch Training Loss =  0.5306782126426697\n",
            "Batch Training Loss =  0.48909351229667664\n",
            "Batch Training Loss =  0.4323899447917938\n",
            "Batch Training Loss =  0.47896647453308105\n",
            "Validation Loss in this epoch is 0.414\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.3726767599582672\n",
            "Batch Training Loss =  0.30049553513526917\n",
            "Batch Training Loss =  0.426512748003006\n",
            "Batch Training Loss =  0.49647361040115356\n",
            "Batch Training Loss =  0.3750951290130615\n",
            "Batch Training Loss =  0.46704527735710144\n",
            "Batch Training Loss =  0.5003315210342407\n",
            "Batch Training Loss =  0.5672656893730164\n",
            "Batch Training Loss =  0.4311707317829132\n",
            "Batch Training Loss =  0.4569163918495178\n",
            "Batch Training Loss =  0.42571109533309937\n",
            "Batch Training Loss =  0.35485637187957764\n",
            "Batch Training Loss =  0.5291962623596191\n",
            "Batch Training Loss =  0.41697025299072266\n",
            "Batch Training Loss =  0.37608879804611206\n",
            "Batch Training Loss =  0.579296886920929\n",
            "Batch Training Loss =  0.5300060510635376\n",
            "Batch Training Loss =  0.45413947105407715\n",
            "Batch Training Loss =  0.4038970470428467\n",
            "Batch Training Loss =  0.4617452621459961\n",
            "Batch Training Loss =  0.6017035245895386\n",
            "Batch Training Loss =  0.5646485686302185\n",
            "Batch Training Loss =  0.48696786165237427\n",
            "Batch Training Loss =  0.4607331454753876\n",
            "Batch Training Loss =  0.5314773321151733\n",
            "Batch Training Loss =  0.40759608149528503\n",
            "Batch Training Loss =  0.4045802652835846\n",
            "Batch Training Loss =  0.4407159388065338\n",
            "Batch Training Loss =  0.45418599247932434\n",
            "Batch Training Loss =  0.44030582904815674\n",
            "Batch Training Loss =  0.4891709089279175\n",
            "Batch Training Loss =  0.2631782293319702\n",
            "Validation Loss in this epoch is 0.394\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.3771756887435913\n",
            "Batch Training Loss =  0.3450404405593872\n",
            "Batch Training Loss =  0.6427670121192932\n",
            "Batch Training Loss =  0.5757802128791809\n",
            "Batch Training Loss =  0.48363053798675537\n",
            "Batch Training Loss =  0.38146185874938965\n",
            "Batch Training Loss =  0.4556642174720764\n",
            "Batch Training Loss =  0.3894050717353821\n",
            "Batch Training Loss =  0.3101886808872223\n",
            "Batch Training Loss =  0.45324432849884033\n",
            "Batch Training Loss =  0.46110618114471436\n",
            "Batch Training Loss =  0.42435863614082336\n",
            "Batch Training Loss =  0.4386260211467743\n",
            "Batch Training Loss =  0.4002819359302521\n",
            "Batch Training Loss =  0.489190012216568\n",
            "Batch Training Loss =  0.538297176361084\n",
            "Batch Training Loss =  0.3783029317855835\n",
            "Batch Training Loss =  0.35109928250312805\n",
            "Batch Training Loss =  0.3678511083126068\n",
            "Batch Training Loss =  0.25770044326782227\n",
            "Batch Training Loss =  0.7691048383712769\n",
            "Batch Training Loss =  1.7117866277694702\n",
            "Batch Training Loss =  0.6776866316795349\n",
            "Batch Training Loss =  0.7302747964859009\n",
            "Batch Training Loss =  0.7256098389625549\n",
            "Batch Training Loss =  0.711936354637146\n",
            "Batch Training Loss =  0.6955526471138\n",
            "Batch Training Loss =  0.7072972059249878\n",
            "Batch Training Loss =  0.7113788723945618\n",
            "Batch Training Loss =  0.6787221431732178\n",
            "Batch Training Loss =  0.6943784356117249\n",
            "Batch Training Loss =  0.6948271989822388\n",
            "Validation Loss in this epoch is 0.668\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.6778894662857056\n",
            "Batch Training Loss =  0.6681795716285706\n",
            "Batch Training Loss =  0.6794202923774719\n",
            "Batch Training Loss =  0.6354228258132935\n",
            "Batch Training Loss =  0.6993536949157715\n",
            "Batch Training Loss =  0.675218939781189\n",
            "Batch Training Loss =  0.6518810987472534\n",
            "Batch Training Loss =  0.5927523970603943\n",
            "Batch Training Loss =  0.7620841860771179\n",
            "Batch Training Loss =  0.6815041899681091\n",
            "Batch Training Loss =  0.6693239808082581\n",
            "Batch Training Loss =  0.6386141777038574\n",
            "Batch Training Loss =  0.652506947517395\n",
            "Batch Training Loss =  0.664940595626831\n",
            "Batch Training Loss =  0.6589170694351196\n",
            "Batch Training Loss =  0.618262767791748\n",
            "Batch Training Loss =  0.5979037284851074\n",
            "Batch Training Loss =  0.5725465416908264\n",
            "Batch Training Loss =  0.5902321338653564\n",
            "Batch Training Loss =  0.7452595233917236\n",
            "Batch Training Loss =  0.6932823061943054\n",
            "Batch Training Loss =  0.6944625377655029\n",
            "Batch Training Loss =  0.6717725396156311\n",
            "Batch Training Loss =  0.6527644395828247\n",
            "Batch Training Loss =  0.6402509212493896\n",
            "Batch Training Loss =  0.6143564581871033\n",
            "Batch Training Loss =  0.655937135219574\n",
            "Batch Training Loss =  0.6739145517349243\n",
            "Batch Training Loss =  0.6025118827819824\n",
            "Batch Training Loss =  0.6962692141532898\n",
            "Batch Training Loss =  0.7222510576248169\n",
            "Batch Training Loss =  0.6307497620582581\n",
            "Validation Loss in this epoch is 0.609\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.630562961101532\n",
            "Batch Training Loss =  0.5968832969665527\n",
            "Batch Training Loss =  0.5925507545471191\n",
            "Batch Training Loss =  0.5226470232009888\n",
            "Batch Training Loss =  0.6603845357894897\n",
            "Batch Training Loss =  0.6416324377059937\n",
            "Batch Training Loss =  0.6656349301338196\n",
            "Batch Training Loss =  0.5687514543533325\n",
            "Batch Training Loss =  0.5628762245178223\n",
            "Batch Training Loss =  0.5015661716461182\n",
            "Batch Training Loss =  0.4914439916610718\n",
            "Batch Training Loss =  0.5397809147834778\n",
            "Batch Training Loss =  0.47776395082473755\n",
            "Batch Training Loss =  0.4732844829559326\n",
            "Batch Training Loss =  0.4902183413505554\n",
            "Batch Training Loss =  0.8620828986167908\n",
            "Batch Training Loss =  0.7251169681549072\n",
            "Batch Training Loss =  0.6930193901062012\n",
            "Batch Training Loss =  0.6470080614089966\n",
            "Batch Training Loss =  0.7249847650527954\n",
            "Batch Training Loss =  0.5832845568656921\n",
            "Batch Training Loss =  0.663555920124054\n",
            "Batch Training Loss =  0.6515685319900513\n",
            "Batch Training Loss =  0.6860945224761963\n",
            "Batch Training Loss =  0.6733599305152893\n",
            "Batch Training Loss =  0.592190146446228\n",
            "Batch Training Loss =  0.5963906645774841\n",
            "Batch Training Loss =  0.5607650279998779\n",
            "Batch Training Loss =  0.5956456661224365\n",
            "Batch Training Loss =  0.6412348747253418\n",
            "Batch Training Loss =  0.5089084506034851\n",
            "Batch Training Loss =  0.5609786510467529\n",
            "Validation Loss in this epoch is 0.684\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6312982439994812\n",
            "Batch Training Loss =  0.6237243413925171\n",
            "Batch Training Loss =  0.532509446144104\n",
            "Batch Training Loss =  0.5756003856658936\n",
            "Batch Training Loss =  0.5418241024017334\n",
            "Batch Training Loss =  0.7911455035209656\n",
            "Batch Training Loss =  0.7430095076560974\n",
            "Batch Training Loss =  0.6292594075202942\n",
            "Batch Training Loss =  0.6278719902038574\n",
            "Batch Training Loss =  0.5825905799865723\n",
            "Batch Training Loss =  0.6606126427650452\n",
            "Batch Training Loss =  0.5737076997756958\n",
            "Batch Training Loss =  0.5229063630104065\n",
            "Batch Training Loss =  0.5545852780342102\n",
            "Batch Training Loss =  0.7220445275306702\n",
            "Batch Training Loss =  0.761146068572998\n",
            "Batch Training Loss =  0.6699668765068054\n",
            "Batch Training Loss =  0.7548915147781372\n",
            "Batch Training Loss =  0.7193022966384888\n",
            "Batch Training Loss =  0.6630228161811829\n",
            "Batch Training Loss =  0.6378061175346375\n",
            "Batch Training Loss =  0.6341003775596619\n",
            "Batch Training Loss =  0.647229015827179\n",
            "Batch Training Loss =  0.627964198589325\n",
            "Batch Training Loss =  0.6701464056968689\n",
            "Batch Training Loss =  0.5977583527565002\n",
            "Batch Training Loss =  0.5523350834846497\n",
            "Batch Training Loss =  0.6122581362724304\n",
            "Batch Training Loss =  0.5547563433647156\n",
            "Batch Training Loss =  0.6408432126045227\n",
            "Batch Training Loss =  0.45172935724258423\n",
            "Batch Training Loss =  0.7114089131355286\n",
            "Validation Loss in this epoch is 0.735\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6801368594169617\n",
            "Batch Training Loss =  0.724884033203125\n",
            "Batch Training Loss =  0.7171789407730103\n",
            "Batch Training Loss =  0.6515934467315674\n",
            "Batch Training Loss =  0.621329128742218\n",
            "Batch Training Loss =  0.5897881388664246\n",
            "Batch Training Loss =  0.5510400533676147\n",
            "Batch Training Loss =  0.5159810781478882\n",
            "Batch Training Loss =  0.6318956613540649\n",
            "Batch Training Loss =  0.6596978902816772\n",
            "Batch Training Loss =  0.7165690064430237\n",
            "Batch Training Loss =  0.724048912525177\n",
            "Batch Training Loss =  0.6667600870132446\n",
            "Batch Training Loss =  0.620063066482544\n",
            "Batch Training Loss =  0.6203595399856567\n",
            "Batch Training Loss =  0.5842447280883789\n",
            "Batch Training Loss =  0.6158604621887207\n",
            "Batch Training Loss =  0.5125190615653992\n",
            "Batch Training Loss =  0.5467008948326111\n",
            "Batch Training Loss =  0.5285740494728088\n",
            "Batch Training Loss =  0.5040707588195801\n",
            "Batch Training Loss =  0.5377464294433594\n",
            "Batch Training Loss =  0.49848538637161255\n",
            "Batch Training Loss =  0.45211800932884216\n",
            "Batch Training Loss =  0.5667558908462524\n",
            "Batch Training Loss =  0.6670615673065186\n",
            "Batch Training Loss =  0.6141586899757385\n",
            "Batch Training Loss =  0.36101776361465454\n",
            "Batch Training Loss =  0.7236042618751526\n",
            "Batch Training Loss =  0.8156631588935852\n",
            "Batch Training Loss =  0.7777650952339172\n",
            "Batch Training Loss =  0.6017756462097168\n",
            "Validation Loss in this epoch is 0.585\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.5868812799453735\n",
            "Batch Training Loss =  0.5737558007240295\n",
            "Batch Training Loss =  0.5668070316314697\n",
            "Batch Training Loss =  0.6294916272163391\n",
            "Batch Training Loss =  0.5547427535057068\n",
            "Batch Training Loss =  0.5116398334503174\n",
            "Batch Training Loss =  0.5468184947967529\n",
            "Batch Training Loss =  0.5574507117271423\n",
            "Batch Training Loss =  0.6290575265884399\n",
            "Batch Training Loss =  0.883736252784729\n",
            "Batch Training Loss =  0.6915838122367859\n",
            "Batch Training Loss =  0.6372434496879578\n",
            "Batch Training Loss =  0.4835221767425537\n",
            "Batch Training Loss =  0.45258763432502747\n",
            "Batch Training Loss =  0.41201457381248474\n",
            "Batch Training Loss =  0.30069053173065186\n",
            "Batch Training Loss =  0.4535641074180603\n",
            "Batch Training Loss =  0.9806193709373474\n",
            "Batch Training Loss =  0.4916318655014038\n",
            "Batch Training Loss =  0.6468616724014282\n",
            "Batch Training Loss =  0.7272730469703674\n",
            "Batch Training Loss =  0.669453501701355\n",
            "Batch Training Loss =  0.6220046877861023\n",
            "Batch Training Loss =  0.44980278611183167\n",
            "Batch Training Loss =  0.5263434052467346\n",
            "Batch Training Loss =  0.44443947076797485\n",
            "Batch Training Loss =  0.5412730574607849\n",
            "Batch Training Loss =  0.39997178316116333\n",
            "Batch Training Loss =  0.420301616191864\n",
            "Batch Training Loss =  0.5233692526817322\n",
            "Batch Training Loss =  0.9168859720230103\n",
            "Batch Training Loss =  0.6535131931304932\n",
            "Validation Loss in this epoch is 0.607\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.5981307029724121\n",
            "Batch Training Loss =  0.5048418641090393\n",
            "Batch Training Loss =  0.5567772388458252\n",
            "Batch Training Loss =  0.5089895725250244\n",
            "Batch Training Loss =  0.4458818733692169\n",
            "Batch Training Loss =  0.4885576367378235\n",
            "Batch Training Loss =  0.5758146643638611\n",
            "Batch Training Loss =  0.7170292139053345\n",
            "Batch Training Loss =  0.593471109867096\n",
            "Batch Training Loss =  0.5944259762763977\n",
            "Batch Training Loss =  0.4694488048553467\n",
            "Batch Training Loss =  0.3393908739089966\n",
            "Batch Training Loss =  0.26332592964172363\n",
            "Batch Training Loss =  0.29475656151771545\n",
            "Batch Training Loss =  0.25333985686302185\n",
            "Batch Training Loss =  0.39260995388031006\n",
            "Batch Training Loss =  0.6976373195648193\n",
            "Batch Training Loss =  1.8258744478225708\n",
            "Batch Training Loss =  0.7112157344818115\n",
            "Batch Training Loss =  0.6467770338058472\n",
            "Batch Training Loss =  0.7023478746414185\n",
            "Batch Training Loss =  0.6518685817718506\n",
            "Batch Training Loss =  0.6191188097000122\n",
            "Batch Training Loss =  0.5662885904312134\n",
            "Batch Training Loss =  0.5167096853256226\n",
            "Batch Training Loss =  0.6036590933799744\n",
            "Batch Training Loss =  0.5602297782897949\n",
            "Batch Training Loss =  0.5187457799911499\n",
            "Batch Training Loss =  0.5745996832847595\n",
            "Batch Training Loss =  0.5832141637802124\n",
            "Batch Training Loss =  0.4968128204345703\n",
            "Batch Training Loss =  0.5113981366157532\n",
            "Validation Loss in this epoch is 0.621\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.5719231963157654\n",
            "Batch Training Loss =  0.5688965916633606\n",
            "Batch Training Loss =  0.4763341546058655\n",
            "Batch Training Loss =  0.482665479183197\n",
            "Batch Training Loss =  0.5093855261802673\n",
            "Batch Training Loss =  0.4045650064945221\n",
            "Batch Training Loss =  0.4654942452907562\n",
            "Batch Training Loss =  0.46725597977638245\n",
            "Batch Training Loss =  0.4475380778312683\n",
            "Batch Training Loss =  0.6624220013618469\n",
            "Batch Training Loss =  0.6039862036705017\n",
            "Batch Training Loss =  0.5592098832130432\n",
            "Batch Training Loss =  0.4385055899620056\n",
            "Batch Training Loss =  0.3842959702014923\n",
            "Batch Training Loss =  0.49121779203414917\n",
            "Batch Training Loss =  0.32782799005508423\n",
            "Batch Training Loss =  0.34594103693962097\n",
            "Batch Training Loss =  0.47270476818084717\n",
            "Batch Training Loss =  0.7659444212913513\n",
            "Batch Training Loss =  0.5381008386611938\n",
            "Batch Training Loss =  0.42994803190231323\n",
            "Batch Training Loss =  0.4953346252441406\n",
            "Batch Training Loss =  0.40715354681015015\n",
            "Batch Training Loss =  0.521028995513916\n",
            "Batch Training Loss =  0.6718934774398804\n",
            "Batch Training Loss =  0.43126749992370605\n",
            "Batch Training Loss =  0.415217787027359\n",
            "Batch Training Loss =  0.3634549081325531\n",
            "Batch Training Loss =  0.4390299320220947\n",
            "Batch Training Loss =  0.7127305865287781\n",
            "Batch Training Loss =  0.3797553777694702\n",
            "Batch Training Loss =  0.5091557502746582\n",
            "Validation Loss in this epoch is 0.525\n",
            "This is  26 th epoch\n",
            "Batch Training Loss =  0.4318876564502716\n",
            "Batch Training Loss =  0.4891381859779358\n",
            "Batch Training Loss =  0.42139294743537903\n",
            "Batch Training Loss =  0.39596039056777954\n",
            "Batch Training Loss =  0.372351735830307\n",
            "Batch Training Loss =  0.3586232662200928\n",
            "Batch Training Loss =  0.7598302364349365\n",
            "Batch Training Loss =  0.8847934603691101\n",
            "Batch Training Loss =  0.6647428274154663\n",
            "Batch Training Loss =  0.548672616481781\n",
            "Batch Training Loss =  0.5485714673995972\n",
            "Batch Training Loss =  0.4047989249229431\n",
            "Batch Training Loss =  0.4176402688026428\n",
            "Batch Training Loss =  0.47718971967697144\n",
            "Batch Training Loss =  0.5306724309921265\n",
            "Batch Training Loss =  0.5032269954681396\n",
            "Batch Training Loss =  0.34520652890205383\n",
            "Batch Training Loss =  0.3941684067249298\n",
            "Batch Training Loss =  0.5172633528709412\n",
            "Batch Training Loss =  0.7350044846534729\n",
            "Batch Training Loss =  0.6551731824874878\n",
            "Batch Training Loss =  0.5131394267082214\n",
            "Batch Training Loss =  0.3887094259262085\n",
            "Batch Training Loss =  0.3436039388179779\n",
            "Batch Training Loss =  0.4152848720550537\n",
            "Batch Training Loss =  0.6036229729652405\n",
            "Batch Training Loss =  0.6061583161354065\n",
            "Batch Training Loss =  0.4492098093032837\n",
            "Batch Training Loss =  0.4218229353427887\n",
            "Batch Training Loss =  0.5365729331970215\n",
            "Batch Training Loss =  0.4528450071811676\n",
            "Batch Training Loss =  0.40065449476242065\n",
            "Validation Loss in this epoch is 0.537\n",
            "This is  27 th epoch\n",
            "Batch Training Loss =  0.5645109415054321\n",
            "Batch Training Loss =  0.46334579586982727\n",
            "Batch Training Loss =  0.3801269233226776\n",
            "Batch Training Loss =  0.3259567320346832\n",
            "Batch Training Loss =  0.37699180841445923\n",
            "Batch Training Loss =  0.2511560320854187\n",
            "Batch Training Loss =  0.7278695106506348\n",
            "Batch Training Loss =  1.0127028226852417\n",
            "Batch Training Loss =  0.6819537878036499\n",
            "Batch Training Loss =  0.6843392848968506\n",
            "Batch Training Loss =  0.5410707592964172\n",
            "Batch Training Loss =  0.5608758330345154\n",
            "Batch Training Loss =  0.601938784122467\n",
            "Batch Training Loss =  0.5573716163635254\n",
            "Batch Training Loss =  0.5489453077316284\n",
            "Batch Training Loss =  0.5232950448989868\n",
            "Batch Training Loss =  0.5892792344093323\n",
            "Batch Training Loss =  0.5094854831695557\n",
            "Batch Training Loss =  0.4082663059234619\n",
            "Batch Training Loss =  0.27121105790138245\n",
            "Batch Training Loss =  0.49743327498435974\n",
            "Batch Training Loss =  1.0441594123840332\n",
            "Batch Training Loss =  0.5812342166900635\n",
            "Batch Training Loss =  0.518852949142456\n",
            "Batch Training Loss =  0.541109025478363\n",
            "Batch Training Loss =  0.5400936603546143\n",
            "Batch Training Loss =  0.41365498304367065\n",
            "Batch Training Loss =  0.476785808801651\n",
            "Batch Training Loss =  0.3989865779876709\n",
            "Batch Training Loss =  0.5462221503257751\n",
            "Batch Training Loss =  0.32305726408958435\n",
            "Batch Training Loss =  0.2860916554927826\n",
            "Validation Loss in this epoch is 0.419\n",
            "This is  28 th epoch\n",
            "Batch Training Loss =  0.3836641311645508\n",
            "Batch Training Loss =  0.6739469170570374\n",
            "Batch Training Loss =  0.7117115259170532\n",
            "Batch Training Loss =  0.505327045917511\n",
            "Batch Training Loss =  0.4539242088794708\n",
            "Batch Training Loss =  0.4844003915786743\n",
            "Batch Training Loss =  0.28538110852241516\n",
            "Batch Training Loss =  0.323394775390625\n",
            "Batch Training Loss =  0.5012288689613342\n",
            "Batch Training Loss =  0.5455741882324219\n",
            "Batch Training Loss =  0.4256538152694702\n",
            "Batch Training Loss =  0.4370479881763458\n",
            "Batch Training Loss =  0.26635977625846863\n",
            "Batch Training Loss =  0.4865725636482239\n",
            "Batch Training Loss =  0.729606032371521\n",
            "Batch Training Loss =  0.5592972636222839\n",
            "Batch Training Loss =  0.5428591966629028\n",
            "Batch Training Loss =  0.4830702841281891\n",
            "Batch Training Loss =  0.45774418115615845\n",
            "Batch Training Loss =  0.28806138038635254\n",
            "Batch Training Loss =  0.33472689986228943\n",
            "Batch Training Loss =  0.4435778558254242\n",
            "Batch Training Loss =  0.3526141345500946\n",
            "Batch Training Loss =  0.7403949499130249\n",
            "Batch Training Loss =  0.5343072414398193\n",
            "Batch Training Loss =  0.4042273759841919\n",
            "Batch Training Loss =  0.45146965980529785\n",
            "Batch Training Loss =  0.3593396842479706\n",
            "Batch Training Loss =  0.40378138422966003\n",
            "Batch Training Loss =  0.511833667755127\n",
            "Batch Training Loss =  0.40357574820518494\n",
            "Batch Training Loss =  0.34209561347961426\n",
            "Validation Loss in this epoch is 0.383\n",
            "This is  29 th epoch\n",
            "Batch Training Loss =  0.33865025639533997\n",
            "Batch Training Loss =  0.26708126068115234\n",
            "Batch Training Loss =  0.6096251606941223\n",
            "Batch Training Loss =  0.6580267548561096\n",
            "Batch Training Loss =  0.4586884379386902\n",
            "Batch Training Loss =  0.41497334837913513\n",
            "Batch Training Loss =  0.4958690404891968\n",
            "Batch Training Loss =  0.5035468339920044\n",
            "Batch Training Loss =  0.49952781200408936\n",
            "Batch Training Loss =  0.5995901823043823\n",
            "Batch Training Loss =  0.4455847442150116\n",
            "Batch Training Loss =  0.4724047780036926\n",
            "Batch Training Loss =  0.38444221019744873\n",
            "Batch Training Loss =  0.446572482585907\n",
            "Batch Training Loss =  0.29681795835494995\n",
            "Batch Training Loss =  0.27334555983543396\n",
            "Batch Training Loss =  0.26054465770721436\n",
            "Batch Training Loss =  0.3811070919036865\n",
            "Batch Training Loss =  0.4970141649246216\n",
            "Batch Training Loss =  0.557640016078949\n",
            "Batch Training Loss =  0.5077226161956787\n",
            "Batch Training Loss =  0.4350755214691162\n",
            "Batch Training Loss =  0.3264559507369995\n",
            "Batch Training Loss =  0.5122230052947998\n",
            "Batch Training Loss =  0.4782659411430359\n",
            "Batch Training Loss =  0.3287484347820282\n",
            "Batch Training Loss =  0.2715384066104889\n",
            "Batch Training Loss =  0.44948437809944153\n",
            "Batch Training Loss =  0.5821974873542786\n",
            "Batch Training Loss =  0.41624996066093445\n",
            "Batch Training Loss =  0.29232361912727356\n",
            "Batch Training Loss =  0.35082972049713135\n",
            "Validation Loss in this epoch is 0.388\n",
            "This is  30 th epoch\n",
            "Batch Training Loss =  0.44200384616851807\n",
            "Batch Training Loss =  0.5128892064094543\n",
            "Batch Training Loss =  0.4641740322113037\n",
            "Batch Training Loss =  0.44888046383857727\n",
            "Batch Training Loss =  0.41057124733924866\n",
            "Batch Training Loss =  0.3840491771697998\n",
            "Batch Training Loss =  0.3719945251941681\n",
            "Batch Training Loss =  0.4650720953941345\n",
            "Batch Training Loss =  0.4924168288707733\n",
            "Batch Training Loss =  0.41987258195877075\n",
            "Batch Training Loss =  0.3808378577232361\n",
            "Batch Training Loss =  0.24206681549549103\n",
            "Batch Training Loss =  0.4243851900100708\n",
            "Batch Training Loss =  0.40540754795074463\n",
            "Batch Training Loss =  0.832530677318573\n",
            "Batch Training Loss =  0.4602515697479248\n",
            "Batch Training Loss =  0.46937668323516846\n",
            "Batch Training Loss =  0.4454059302806854\n",
            "Batch Training Loss =  0.3684029281139374\n",
            "Batch Training Loss =  0.41570404171943665\n",
            "Batch Training Loss =  0.3334581255912781\n",
            "Batch Training Loss =  0.42191529273986816\n",
            "Batch Training Loss =  0.25449708104133606\n",
            "Batch Training Loss =  0.40348708629608154\n",
            "Batch Training Loss =  0.4223014712333679\n",
            "Batch Training Loss =  0.3975132703781128\n",
            "Batch Training Loss =  0.4006747007369995\n",
            "Batch Training Loss =  0.7031401991844177\n",
            "Batch Training Loss =  0.5175907015800476\n",
            "Batch Training Loss =  0.5023649334907532\n",
            "Batch Training Loss =  0.48744499683380127\n",
            "Batch Training Loss =  0.42021602392196655\n",
            "Validation Loss in this epoch is 0.425\n",
            "This is  31 th epoch\n",
            "Batch Training Loss =  0.3869858682155609\n",
            "Batch Training Loss =  0.40497517585754395\n",
            "Batch Training Loss =  0.4295072853565216\n",
            "Batch Training Loss =  0.3441716432571411\n",
            "Batch Training Loss =  0.3576902151107788\n",
            "Batch Training Loss =  0.4528310000896454\n",
            "Batch Training Loss =  0.46429243683815\n",
            "Batch Training Loss =  0.4621349573135376\n",
            "Batch Training Loss =  0.2753414213657379\n",
            "Batch Training Loss =  0.4204646050930023\n",
            "Batch Training Loss =  0.4232570230960846\n",
            "Batch Training Loss =  0.3158286511898041\n",
            "Batch Training Loss =  0.3997780978679657\n",
            "Batch Training Loss =  0.7616469860076904\n",
            "Batch Training Loss =  0.8128354549407959\n",
            "Batch Training Loss =  0.6126878261566162\n",
            "Batch Training Loss =  0.6341982483863831\n",
            "Batch Training Loss =  0.46102839708328247\n",
            "Batch Training Loss =  0.4336748421192169\n",
            "Batch Training Loss =  0.4462622404098511\n",
            "Batch Training Loss =  0.4598643481731415\n",
            "Batch Training Loss =  0.3816990554332733\n",
            "Batch Training Loss =  0.32954758405685425\n",
            "Batch Training Loss =  0.5738376379013062\n",
            "Batch Training Loss =  0.44390788674354553\n",
            "Batch Training Loss =  0.4607473611831665\n",
            "Batch Training Loss =  0.33554568886756897\n",
            "Batch Training Loss =  0.2600507438182831\n",
            "Batch Training Loss =  0.36440908908843994\n",
            "Batch Training Loss =  0.23797191679477692\n",
            "Batch Training Loss =  0.34217414259910583\n",
            "Batch Training Loss =  0.30496910214424133\n",
            "Validation Loss in this epoch is 0.540\n",
            "This is  32 th epoch\n",
            "Batch Training Loss =  0.5187639594078064\n",
            "Batch Training Loss =  0.5955547094345093\n",
            "Batch Training Loss =  0.453428715467453\n",
            "Batch Training Loss =  0.3986164331436157\n",
            "Batch Training Loss =  0.39370572566986084\n",
            "Batch Training Loss =  0.3494623899459839\n",
            "Batch Training Loss =  0.4446614384651184\n",
            "Batch Training Loss =  0.3675631582736969\n",
            "Batch Training Loss =  0.4993933439254761\n",
            "Batch Training Loss =  0.3862983286380768\n",
            "Batch Training Loss =  0.5712161660194397\n",
            "Batch Training Loss =  0.3861865699291229\n",
            "Batch Training Loss =  0.27565261721611023\n",
            "Batch Training Loss =  0.2572372555732727\n",
            "Batch Training Loss =  0.5456016063690186\n",
            "Batch Training Loss =  0.5315898656845093\n",
            "Batch Training Loss =  0.48475778102874756\n",
            "Batch Training Loss =  0.4290737211704254\n",
            "Batch Training Loss =  0.30272766947746277\n",
            "Batch Training Loss =  0.289928674697876\n",
            "Batch Training Loss =  0.4287220239639282\n",
            "Batch Training Loss =  0.4686569571495056\n",
            "Batch Training Loss =  0.8326254487037659\n",
            "Batch Training Loss =  0.46278953552246094\n",
            "Batch Training Loss =  0.4802725613117218\n",
            "Batch Training Loss =  0.3955550789833069\n",
            "Batch Training Loss =  0.527979850769043\n",
            "Batch Training Loss =  0.5293226838111877\n",
            "Batch Training Loss =  0.5448084473609924\n",
            "Batch Training Loss =  0.546898603439331\n",
            "Batch Training Loss =  0.38193270564079285\n",
            "Batch Training Loss =  0.3389315903186798\n",
            "Validation Loss in this epoch is 0.418\n",
            "This is  33 th epoch\n",
            "Batch Training Loss =  0.3052808344364166\n",
            "Batch Training Loss =  0.3523854911327362\n",
            "Batch Training Loss =  0.3266445994377136\n",
            "Batch Training Loss =  0.42850834131240845\n",
            "Batch Training Loss =  0.47676151990890503\n",
            "Batch Training Loss =  0.3273894488811493\n",
            "Batch Training Loss =  0.5099332332611084\n",
            "Batch Training Loss =  0.3639465570449829\n",
            "Batch Training Loss =  0.40783730149269104\n",
            "Batch Training Loss =  0.43579140305519104\n",
            "Batch Training Loss =  1.087410569190979\n",
            "Batch Training Loss =  0.5018723607063293\n",
            "Batch Training Loss =  0.6101598739624023\n",
            "Batch Training Loss =  0.548759937286377\n",
            "Batch Training Loss =  0.500481128692627\n",
            "Batch Training Loss =  0.5325083136558533\n",
            "Batch Training Loss =  0.499980092048645\n",
            "Batch Training Loss =  0.39692461490631104\n",
            "Batch Training Loss =  0.41072604060173035\n",
            "Batch Training Loss =  0.3075920045375824\n",
            "Batch Training Loss =  0.5858151316642761\n",
            "Batch Training Loss =  0.9190115332603455\n",
            "Batch Training Loss =  0.459428608417511\n",
            "Batch Training Loss =  0.5359905362129211\n",
            "Batch Training Loss =  0.49883583188056946\n",
            "Batch Training Loss =  0.4971110224723816\n",
            "Batch Training Loss =  0.4232071042060852\n",
            "Batch Training Loss =  0.4737423062324524\n",
            "Batch Training Loss =  0.516358494758606\n",
            "Batch Training Loss =  0.3825165927410126\n",
            "Batch Training Loss =  0.5521401762962341\n",
            "Batch Training Loss =  0.46793174743652344\n",
            "Validation Loss in this epoch is 0.425\n",
            "This is  34 th epoch\n",
            "Batch Training Loss =  0.38872668147087097\n",
            "Batch Training Loss =  0.3562113642692566\n",
            "Batch Training Loss =  0.4620022475719452\n",
            "Batch Training Loss =  0.3364179730415344\n",
            "Batch Training Loss =  0.4217449128627777\n",
            "Batch Training Loss =  0.3753189146518707\n",
            "Batch Training Loss =  0.4610699415206909\n",
            "Batch Training Loss =  0.2933264970779419\n",
            "Batch Training Loss =  0.26492631435394287\n",
            "Batch Training Loss =  0.45354554057121277\n",
            "Batch Training Loss =  0.3860783278942108\n",
            "Batch Training Loss =  0.41751959919929504\n",
            "Batch Training Loss =  0.36099743843078613\n",
            "Batch Training Loss =  0.3325355052947998\n",
            "Batch Training Loss =  0.36444324254989624\n",
            "Batch Training Loss =  0.4175618886947632\n",
            "Batch Training Loss =  0.2496475726366043\n",
            "Batch Training Loss =  0.5220637917518616\n",
            "Batch Training Loss =  0.6150221824645996\n",
            "Batch Training Loss =  0.385986328125\n",
            "Batch Training Loss =  0.30259275436401367\n",
            "Batch Training Loss =  0.2729310691356659\n",
            "Batch Training Loss =  0.5010325908660889\n",
            "Batch Training Loss =  0.4985615015029907\n",
            "Batch Training Loss =  0.6890939474105835\n",
            "Batch Training Loss =  0.4333142340183258\n",
            "Batch Training Loss =  0.4861486554145813\n",
            "Batch Training Loss =  0.3615066409111023\n",
            "Batch Training Loss =  0.3872818052768707\n",
            "Batch Training Loss =  0.5275893807411194\n",
            "Batch Training Loss =  0.6182323098182678\n",
            "Batch Training Loss =  0.3033485412597656\n",
            "Validation Loss in this epoch is 0.440\n",
            "This is  35 th epoch\n",
            "Batch Training Loss =  0.3880902826786041\n",
            "Batch Training Loss =  0.3195357918739319\n",
            "Batch Training Loss =  0.3438500463962555\n",
            "Batch Training Loss =  0.3818531334400177\n",
            "Batch Training Loss =  0.4102135896682739\n",
            "Batch Training Loss =  0.6658907532691956\n",
            "Batch Training Loss =  0.7850006818771362\n",
            "Batch Training Loss =  0.5820208191871643\n",
            "Batch Training Loss =  0.5617005825042725\n",
            "Batch Training Loss =  0.3876895010471344\n",
            "Batch Training Loss =  0.4362017810344696\n",
            "Batch Training Loss =  0.3031884431838989\n",
            "Batch Training Loss =  0.30636829137802124\n",
            "Batch Training Loss =  0.3032723665237427\n",
            "Batch Training Loss =  0.32882019877433777\n",
            "Batch Training Loss =  0.735663652420044\n",
            "Batch Training Loss =  1.272984266281128\n",
            "Batch Training Loss =  0.6846749782562256\n",
            "Batch Training Loss =  0.7111344337463379\n",
            "Batch Training Loss =  0.6738927364349365\n",
            "Batch Training Loss =  0.5845986008644104\n",
            "Batch Training Loss =  0.6618295907974243\n",
            "Batch Training Loss =  0.48773252964019775\n",
            "Batch Training Loss =  0.48709025979042053\n",
            "Batch Training Loss =  0.3612399399280548\n",
            "Batch Training Loss =  0.3005547821521759\n",
            "Batch Training Loss =  0.41007182002067566\n",
            "Batch Training Loss =  0.34531915187835693\n",
            "Batch Training Loss =  0.5167185068130493\n",
            "Batch Training Loss =  0.36882635951042175\n",
            "Batch Training Loss =  0.3620739281177521\n",
            "Batch Training Loss =  0.5734202265739441\n",
            "Validation Loss in this epoch is 0.485\n",
            "This is  36 th epoch\n",
            "Batch Training Loss =  0.48821285367012024\n",
            "Batch Training Loss =  0.32922789454460144\n",
            "Batch Training Loss =  0.36316952109336853\n",
            "Batch Training Loss =  0.42428889870643616\n",
            "Batch Training Loss =  0.47125443816185\n",
            "Batch Training Loss =  0.3325880169868469\n",
            "Batch Training Loss =  0.37452611327171326\n",
            "Batch Training Loss =  0.42172032594680786\n",
            "Batch Training Loss =  0.3542209565639496\n",
            "Batch Training Loss =  0.3476240038871765\n",
            "Batch Training Loss =  0.3269767463207245\n",
            "Batch Training Loss =  0.3797162175178528\n",
            "Batch Training Loss =  0.3274165093898773\n",
            "Batch Training Loss =  0.38785839080810547\n",
            "Batch Training Loss =  0.3450414538383484\n",
            "Batch Training Loss =  0.26867324113845825\n",
            "Batch Training Loss =  0.37293094396591187\n",
            "Batch Training Loss =  0.4444998502731323\n",
            "Batch Training Loss =  0.8860849142074585\n",
            "Batch Training Loss =  0.751789927482605\n",
            "Batch Training Loss =  0.6047550439834595\n",
            "Batch Training Loss =  0.5196088552474976\n",
            "Batch Training Loss =  0.3953752815723419\n",
            "Batch Training Loss =  0.3274337649345398\n",
            "Batch Training Loss =  0.3775625228881836\n",
            "Batch Training Loss =  0.38706883788108826\n",
            "Batch Training Loss =  0.4781434237957001\n",
            "Batch Training Loss =  0.3401685357093811\n",
            "Batch Training Loss =  0.547783613204956\n",
            "Batch Training Loss =  0.6066516041755676\n",
            "Batch Training Loss =  0.6526666283607483\n",
            "Batch Training Loss =  0.4730914831161499\n",
            "Validation Loss in this epoch is 0.421\n",
            "This is  37 th epoch\n",
            "Batch Training Loss =  0.38738304376602173\n",
            "Batch Training Loss =  0.33965012431144714\n",
            "Batch Training Loss =  0.6209151744842529\n",
            "Batch Training Loss =  0.4968571364879608\n",
            "Batch Training Loss =  0.27298951148986816\n",
            "Batch Training Loss =  0.28283339738845825\n",
            "Batch Training Loss =  0.30527910590171814\n",
            "Batch Training Loss =  0.17931313812732697\n",
            "Batch Training Loss =  0.3703686594963074\n",
            "Batch Training Loss =  0.30803796648979187\n",
            "Batch Training Loss =  0.2937648296356201\n",
            "Batch Training Loss =  0.48080384731292725\n",
            "Batch Training Loss =  0.407476544380188\n",
            "Batch Training Loss =  0.5064699649810791\n",
            "Batch Training Loss =  0.33089762926101685\n",
            "Batch Training Loss =  0.4489542245864868\n",
            "Batch Training Loss =  0.42030268907546997\n",
            "Batch Training Loss =  0.39759960770606995\n",
            "Batch Training Loss =  0.3175777792930603\n",
            "Batch Training Loss =  0.4282791018486023\n",
            "Batch Training Loss =  0.4438791275024414\n",
            "Batch Training Loss =  0.44392526149749756\n",
            "Batch Training Loss =  0.3368651270866394\n",
            "Batch Training Loss =  0.3952334523200989\n",
            "Batch Training Loss =  0.2783561944961548\n",
            "Batch Training Loss =  0.5025389194488525\n",
            "Batch Training Loss =  0.7426565289497375\n",
            "Batch Training Loss =  0.4407695531845093\n",
            "Batch Training Loss =  0.40916383266448975\n",
            "Batch Training Loss =  0.33912768959999084\n",
            "Batch Training Loss =  0.2240043580532074\n",
            "Batch Training Loss =  0.3588981628417969\n",
            "Validation Loss in this epoch is 0.367\n",
            "This is  38 th epoch\n",
            "Batch Training Loss =  0.33277618885040283\n",
            "Batch Training Loss =  0.38387662172317505\n",
            "Batch Training Loss =  0.4443749189376831\n",
            "Batch Training Loss =  0.6524890065193176\n",
            "Batch Training Loss =  0.6988732218742371\n",
            "Batch Training Loss =  0.5089700818061829\n",
            "Batch Training Loss =  0.6142236590385437\n",
            "Batch Training Loss =  0.5265044569969177\n",
            "Batch Training Loss =  0.3526417911052704\n",
            "Batch Training Loss =  0.2591334581375122\n",
            "Batch Training Loss =  0.3750697672367096\n",
            "Batch Training Loss =  0.30156609416007996\n",
            "Batch Training Loss =  0.38992810249328613\n",
            "Batch Training Loss =  0.4143127501010895\n",
            "Batch Training Loss =  0.44723424315452576\n",
            "Batch Training Loss =  0.28374800086021423\n",
            "Batch Training Loss =  0.28777599334716797\n",
            "Batch Training Loss =  0.28655382990837097\n",
            "Batch Training Loss =  0.2843583822250366\n",
            "Batch Training Loss =  0.47075313329696655\n",
            "Batch Training Loss =  0.6521270275115967\n",
            "Batch Training Loss =  0.39244160056114197\n",
            "Batch Training Loss =  0.3589979410171509\n",
            "Batch Training Loss =  0.3534976840019226\n",
            "Batch Training Loss =  0.27066102623939514\n",
            "Batch Training Loss =  0.45165398716926575\n",
            "Batch Training Loss =  0.4541242718696594\n",
            "Batch Training Loss =  0.24006707966327667\n",
            "Batch Training Loss =  0.30406269431114197\n",
            "Batch Training Loss =  0.49324944615364075\n",
            "Batch Training Loss =  0.39986860752105713\n",
            "Batch Training Loss =  0.44112733006477356\n",
            "Validation Loss in this epoch is 0.381\n",
            "This is  39 th epoch\n",
            "Batch Training Loss =  0.38212376832962036\n",
            "Batch Training Loss =  0.44036945700645447\n",
            "Batch Training Loss =  0.48771217465400696\n",
            "Batch Training Loss =  0.31793347001075745\n",
            "Batch Training Loss =  0.19778494536876678\n",
            "Batch Training Loss =  0.26447951793670654\n",
            "Batch Training Loss =  0.6028759479522705\n",
            "Batch Training Loss =  0.4537906348705292\n",
            "Batch Training Loss =  0.531855583190918\n",
            "Batch Training Loss =  0.439004510641098\n",
            "Batch Training Loss =  0.5092745423316956\n",
            "Batch Training Loss =  0.38807299733161926\n",
            "Batch Training Loss =  0.3032892346382141\n",
            "Batch Training Loss =  0.35583561658859253\n",
            "Batch Training Loss =  0.26775020360946655\n",
            "Batch Training Loss =  0.28034964203834534\n",
            "Batch Training Loss =  0.40707406401634216\n",
            "Batch Training Loss =  0.8176690936088562\n",
            "Batch Training Loss =  0.5379937291145325\n",
            "Batch Training Loss =  0.5425033569335938\n",
            "Batch Training Loss =  0.41815799474716187\n",
            "Batch Training Loss =  0.43868470191955566\n",
            "Batch Training Loss =  0.39354750514030457\n",
            "Batch Training Loss =  0.4044640362262726\n",
            "Batch Training Loss =  0.3937678039073944\n",
            "Batch Training Loss =  0.341309130191803\n",
            "Batch Training Loss =  0.32516059279441833\n",
            "Batch Training Loss =  0.3701367974281311\n",
            "Batch Training Loss =  0.4376813769340515\n",
            "Batch Training Loss =  0.4854329824447632\n",
            "Batch Training Loss =  0.5612878203392029\n",
            "Batch Training Loss =  0.4612358510494232\n",
            "Validation Loss in this epoch is 0.435\n",
            "This is  40 th epoch\n",
            "Batch Training Loss =  0.4082309603691101\n",
            "Batch Training Loss =  0.41898632049560547\n",
            "Batch Training Loss =  0.2557351589202881\n",
            "Batch Training Loss =  0.43264514207839966\n",
            "Batch Training Loss =  0.3010393977165222\n",
            "Batch Training Loss =  0.3660683035850525\n",
            "Batch Training Loss =  0.4036180078983307\n",
            "Batch Training Loss =  0.28774359822273254\n",
            "Batch Training Loss =  0.30458351969718933\n",
            "Batch Training Loss =  0.667855441570282\n",
            "Batch Training Loss =  0.6007665395736694\n",
            "Batch Training Loss =  0.5014421343803406\n",
            "Batch Training Loss =  0.39459192752838135\n",
            "Batch Training Loss =  0.34831473231315613\n",
            "Batch Training Loss =  0.31528621912002563\n",
            "Batch Training Loss =  0.3407207727432251\n",
            "Batch Training Loss =  0.3913264572620392\n",
            "Batch Training Loss =  0.39163535833358765\n",
            "Batch Training Loss =  0.4758297801017761\n",
            "Batch Training Loss =  0.3335389792919159\n",
            "Batch Training Loss =  0.4004186689853668\n",
            "Batch Training Loss =  0.3284459412097931\n",
            "Batch Training Loss =  0.33797580003738403\n",
            "Batch Training Loss =  0.3853740990161896\n",
            "Batch Training Loss =  0.43273505568504333\n",
            "Batch Training Loss =  0.5381600260734558\n",
            "Batch Training Loss =  0.39907023310661316\n",
            "Batch Training Loss =  0.2476034313440323\n",
            "Batch Training Loss =  0.4690973162651062\n",
            "Batch Training Loss =  0.39530208706855774\n",
            "Batch Training Loss =  0.3882826268672943\n",
            "Batch Training Loss =  0.35244616866111755\n",
            "Validation Loss in this epoch is 0.381\n",
            "This is  41 th epoch\n",
            "Batch Training Loss =  0.3413495123386383\n",
            "Batch Training Loss =  0.23211248219013214\n",
            "Batch Training Loss =  0.33458656072616577\n",
            "Batch Training Loss =  0.28201666474342346\n",
            "Batch Training Loss =  0.42504748702049255\n",
            "Batch Training Loss =  0.5132404565811157\n",
            "Batch Training Loss =  0.4303455054759979\n",
            "Batch Training Loss =  0.39607253670692444\n",
            "Batch Training Loss =  0.2953434884548187\n",
            "Batch Training Loss =  0.21389050781726837\n",
            "Batch Training Loss =  0.4057416021823883\n",
            "Batch Training Loss =  0.7359364032745361\n",
            "Batch Training Loss =  0.6265207529067993\n",
            "Batch Training Loss =  0.4401874244213104\n",
            "Batch Training Loss =  0.3585701286792755\n",
            "Batch Training Loss =  0.40712520480155945\n",
            "Batch Training Loss =  0.3215186297893524\n",
            "Batch Training Loss =  0.38498836755752563\n",
            "Batch Training Loss =  0.3645820617675781\n",
            "Batch Training Loss =  0.457573801279068\n",
            "Batch Training Loss =  1.1689525842666626\n",
            "Batch Training Loss =  0.37698784470558167\n",
            "Batch Training Loss =  0.4664328694343567\n",
            "Batch Training Loss =  0.46398109197616577\n",
            "Batch Training Loss =  0.4916723668575287\n",
            "Batch Training Loss =  0.46189162135124207\n",
            "Batch Training Loss =  0.6035870909690857\n",
            "Batch Training Loss =  0.48861512541770935\n",
            "Batch Training Loss =  0.3730103373527527\n",
            "Batch Training Loss =  0.3621620535850525\n",
            "Batch Training Loss =  0.48162296414375305\n",
            "Batch Training Loss =  0.6289437413215637\n",
            "Validation Loss in this epoch is 0.466\n",
            "This is  42 th epoch\n",
            "Batch Training Loss =  0.5297591090202332\n",
            "Batch Training Loss =  0.417715847492218\n",
            "Batch Training Loss =  0.3379441201686859\n",
            "Batch Training Loss =  0.2571595311164856\n",
            "Batch Training Loss =  0.298519104719162\n",
            "Batch Training Loss =  0.3793652653694153\n",
            "Batch Training Loss =  0.24290890991687775\n",
            "Batch Training Loss =  0.3934629559516907\n",
            "Batch Training Loss =  0.3889405429363251\n",
            "Batch Training Loss =  0.710066020488739\n",
            "Batch Training Loss =  0.8825362920761108\n",
            "Batch Training Loss =  0.5676082372665405\n",
            "Batch Training Loss =  0.4767845571041107\n",
            "Batch Training Loss =  0.40899109840393066\n",
            "Batch Training Loss =  0.4298820197582245\n",
            "Batch Training Loss =  0.3474951684474945\n",
            "Batch Training Loss =  0.4337215721607208\n",
            "Batch Training Loss =  0.30473417043685913\n",
            "Batch Training Loss =  0.44811466336250305\n",
            "Batch Training Loss =  0.42433789372444153\n",
            "Batch Training Loss =  0.3483273983001709\n",
            "Batch Training Loss =  0.27433332800865173\n",
            "Batch Training Loss =  0.2982095777988434\n",
            "Batch Training Loss =  0.4447307288646698\n",
            "Batch Training Loss =  1.205938696861267\n",
            "Batch Training Loss =  1.0612133741378784\n",
            "Batch Training Loss =  0.6957836747169495\n",
            "Batch Training Loss =  0.7199816703796387\n",
            "Batch Training Loss =  0.6569040417671204\n",
            "Batch Training Loss =  0.57278972864151\n",
            "Batch Training Loss =  0.6312735080718994\n",
            "Batch Training Loss =  0.6868855953216553\n",
            "Validation Loss in this epoch is 0.611\n",
            "This is  43 th epoch\n",
            "Batch Training Loss =  0.5859596133232117\n",
            "Batch Training Loss =  0.720192015171051\n",
            "Batch Training Loss =  0.5934195518493652\n",
            "Batch Training Loss =  0.5972068905830383\n",
            "Batch Training Loss =  0.6083083152770996\n",
            "Batch Training Loss =  0.5528872609138489\n",
            "Batch Training Loss =  0.6199811100959778\n",
            "Batch Training Loss =  0.5842029452323914\n",
            "Batch Training Loss =  0.5472531318664551\n",
            "Batch Training Loss =  0.55741286277771\n",
            "Batch Training Loss =  0.518597424030304\n",
            "Batch Training Loss =  0.45130595564842224\n",
            "Batch Training Loss =  0.536733090877533\n",
            "Batch Training Loss =  0.5341670513153076\n",
            "Batch Training Loss =  0.5481299757957458\n",
            "Batch Training Loss =  0.49329429864883423\n",
            "Batch Training Loss =  0.4718952178955078\n",
            "Batch Training Loss =  0.5245036482810974\n",
            "Batch Training Loss =  0.4146202802658081\n",
            "Batch Training Loss =  0.6238271594047546\n",
            "Batch Training Loss =  0.5647587180137634\n",
            "Batch Training Loss =  0.4602645933628082\n",
            "Batch Training Loss =  0.3485446274280548\n",
            "Batch Training Loss =  0.4486922323703766\n",
            "Batch Training Loss =  0.9634990692138672\n",
            "Batch Training Loss =  0.4901362657546997\n",
            "Batch Training Loss =  0.4185287058353424\n",
            "Batch Training Loss =  0.4288426339626312\n",
            "Batch Training Loss =  0.46455061435699463\n",
            "Batch Training Loss =  0.5697076916694641\n",
            "Batch Training Loss =  0.5660697817802429\n",
            "Batch Training Loss =  0.44388172030448914\n",
            "Validation Loss in this epoch is 0.386\n",
            "This is  44 th epoch\n",
            "Batch Training Loss =  0.3490523099899292\n",
            "Batch Training Loss =  0.4060095548629761\n",
            "Batch Training Loss =  0.3976074457168579\n",
            "Batch Training Loss =  0.22719690203666687\n",
            "Batch Training Loss =  0.2837183475494385\n",
            "Batch Training Loss =  0.396854430437088\n",
            "Batch Training Loss =  0.5346893072128296\n",
            "Batch Training Loss =  0.5157981514930725\n",
            "Batch Training Loss =  0.47903764247894287\n",
            "Batch Training Loss =  0.35975226759910583\n",
            "Batch Training Loss =  0.3529402017593384\n",
            "Batch Training Loss =  0.3540348410606384\n",
            "Batch Training Loss =  0.39440250396728516\n",
            "Batch Training Loss =  0.2770764231681824\n",
            "Batch Training Loss =  0.27426835894584656\n",
            "Batch Training Loss =  0.6044172644615173\n",
            "Batch Training Loss =  0.42581650614738464\n",
            "Batch Training Loss =  0.3726558983325958\n",
            "Batch Training Loss =  0.30058538913726807\n",
            "Batch Training Loss =  0.410739928483963\n",
            "Batch Training Loss =  0.38783860206604004\n",
            "Batch Training Loss =  0.4029684066772461\n",
            "Batch Training Loss =  0.33246147632598877\n",
            "Batch Training Loss =  0.3778194189071655\n",
            "Batch Training Loss =  0.4825551509857178\n",
            "Batch Training Loss =  0.34458500146865845\n",
            "Batch Training Loss =  0.319437175989151\n",
            "Batch Training Loss =  0.48597508668899536\n",
            "Batch Training Loss =  0.47745537757873535\n",
            "Batch Training Loss =  0.4127925634384155\n",
            "Batch Training Loss =  0.28728094696998596\n",
            "Batch Training Loss =  0.3254380524158478\n",
            "Validation Loss in this epoch is 0.347\n",
            "This is  45 th epoch\n",
            "Batch Training Loss =  0.30998432636260986\n",
            "Batch Training Loss =  0.4970008134841919\n",
            "Batch Training Loss =  0.38147544860839844\n",
            "Batch Training Loss =  0.4393438994884491\n",
            "Batch Training Loss =  0.3906184136867523\n",
            "Batch Training Loss =  0.3576302230358124\n",
            "Batch Training Loss =  0.36238735914230347\n",
            "Batch Training Loss =  0.2601899802684784\n",
            "Batch Training Loss =  0.3680531978607178\n",
            "Batch Training Loss =  0.4938598573207855\n",
            "Batch Training Loss =  0.6870428323745728\n",
            "Batch Training Loss =  0.409417986869812\n",
            "Batch Training Loss =  0.3756183385848999\n",
            "Batch Training Loss =  0.3800469934940338\n",
            "Batch Training Loss =  0.3089243173599243\n",
            "Batch Training Loss =  0.4119710624217987\n",
            "Batch Training Loss =  0.4195537567138672\n",
            "Batch Training Loss =  0.4583916962146759\n",
            "Batch Training Loss =  0.49984419345855713\n",
            "Batch Training Loss =  0.414029985666275\n",
            "Batch Training Loss =  0.3577440679073334\n",
            "Batch Training Loss =  0.37393254041671753\n",
            "Batch Training Loss =  0.4700615406036377\n",
            "Batch Training Loss =  0.4589635729789734\n",
            "Batch Training Loss =  0.3851803243160248\n",
            "Batch Training Loss =  0.3984867036342621\n",
            "Batch Training Loss =  0.30793628096580505\n",
            "Batch Training Loss =  0.223599374294281\n",
            "Batch Training Loss =  0.396747887134552\n",
            "Batch Training Loss =  0.5881960391998291\n",
            "Batch Training Loss =  0.41438624262809753\n",
            "Batch Training Loss =  0.3167879581451416\n",
            "Validation Loss in this epoch is 0.352\n",
            "This is  46 th epoch\n",
            "Batch Training Loss =  0.2727227807044983\n",
            "Batch Training Loss =  0.3961438238620758\n",
            "Batch Training Loss =  0.4542248845100403\n",
            "Batch Training Loss =  0.2581193149089813\n",
            "Batch Training Loss =  0.3159026801586151\n",
            "Batch Training Loss =  0.297471821308136\n",
            "Batch Training Loss =  0.40022751688957214\n",
            "Batch Training Loss =  0.23977713286876678\n",
            "Batch Training Loss =  0.303842693567276\n",
            "Batch Training Loss =  0.5287602543830872\n",
            "Batch Training Loss =  0.7218363881111145\n",
            "Batch Training Loss =  0.5390381813049316\n",
            "Batch Training Loss =  0.4577232599258423\n",
            "Batch Training Loss =  0.3132473826408386\n",
            "Batch Training Loss =  0.295935720205307\n",
            "Batch Training Loss =  0.2820062041282654\n",
            "Batch Training Loss =  0.27652573585510254\n",
            "Batch Training Loss =  0.5131572484970093\n",
            "Batch Training Loss =  0.5383222103118896\n",
            "Batch Training Loss =  0.682518482208252\n",
            "Batch Training Loss =  0.5287855863571167\n",
            "Batch Training Loss =  0.49065327644348145\n",
            "Batch Training Loss =  0.38505080342292786\n",
            "Batch Training Loss =  0.3769437372684479\n",
            "Batch Training Loss =  0.2441951334476471\n",
            "Batch Training Loss =  0.3915271759033203\n",
            "Batch Training Loss =  0.36120131611824036\n",
            "Batch Training Loss =  0.195118710398674\n",
            "Batch Training Loss =  0.5093576312065125\n",
            "Batch Training Loss =  0.30060791969299316\n",
            "Batch Training Loss =  0.2748061716556549\n",
            "Batch Training Loss =  0.21498149633407593\n",
            "Validation Loss in this epoch is 0.344\n",
            "This is  47 th epoch\n",
            "Batch Training Loss =  0.3475206792354584\n",
            "Batch Training Loss =  0.21049273014068604\n",
            "Batch Training Loss =  0.5662185549736023\n",
            "Batch Training Loss =  1.1351341009140015\n",
            "Batch Training Loss =  0.6006473302841187\n",
            "Batch Training Loss =  0.5637322068214417\n",
            "Batch Training Loss =  0.5539633631706238\n",
            "Batch Training Loss =  0.594146192073822\n",
            "Batch Training Loss =  0.5810112953186035\n",
            "Batch Training Loss =  0.5034170746803284\n",
            "Batch Training Loss =  0.49733006954193115\n",
            "Batch Training Loss =  0.4932484030723572\n",
            "Batch Training Loss =  0.5687938928604126\n",
            "Batch Training Loss =  0.5143550634384155\n",
            "Batch Training Loss =  0.4463239014148712\n",
            "Batch Training Loss =  0.588344931602478\n",
            "Batch Training Loss =  0.5701012015342712\n",
            "Batch Training Loss =  0.4542790353298187\n",
            "Batch Training Loss =  0.5043886303901672\n",
            "Batch Training Loss =  0.4386487901210785\n",
            "Batch Training Loss =  0.5754940509796143\n",
            "Batch Training Loss =  0.65764981508255\n",
            "Batch Training Loss =  0.5302194356918335\n",
            "Batch Training Loss =  0.4676544666290283\n",
            "Batch Training Loss =  0.4945420026779175\n",
            "Batch Training Loss =  0.8382198810577393\n",
            "Batch Training Loss =  0.738057017326355\n",
            "Batch Training Loss =  0.5135974287986755\n",
            "Batch Training Loss =  0.4887315630912781\n",
            "Batch Training Loss =  0.5125240087509155\n",
            "Batch Training Loss =  0.45833373069763184\n",
            "Batch Training Loss =  0.4528440833091736\n",
            "Validation Loss in this epoch is 0.539\n",
            "This is  48 th epoch\n",
            "Batch Training Loss =  0.4671080410480499\n",
            "Batch Training Loss =  0.42228931188583374\n",
            "Batch Training Loss =  0.5229719877243042\n",
            "Batch Training Loss =  0.7534964680671692\n",
            "Batch Training Loss =  0.7591467499732971\n",
            "Batch Training Loss =  0.6759697794914246\n",
            "Batch Training Loss =  0.5934998989105225\n",
            "Batch Training Loss =  0.45784080028533936\n",
            "Batch Training Loss =  0.4098726511001587\n",
            "Batch Training Loss =  0.28515011072158813\n",
            "Batch Training Loss =  0.41664570569992065\n",
            "Batch Training Loss =  0.3496131896972656\n",
            "Batch Training Loss =  0.5713306665420532\n",
            "Batch Training Loss =  1.0531184673309326\n",
            "Batch Training Loss =  0.46219974756240845\n",
            "Batch Training Loss =  0.5039730072021484\n",
            "Batch Training Loss =  0.5393548011779785\n",
            "Batch Training Loss =  0.40332552790641785\n",
            "Batch Training Loss =  0.42638832330703735\n",
            "Batch Training Loss =  0.3803553283214569\n",
            "Batch Training Loss =  0.49696436524391174\n",
            "Batch Training Loss =  0.44138237833976746\n",
            "Batch Training Loss =  0.3197391629219055\n",
            "Batch Training Loss =  0.3792663514614105\n",
            "Batch Training Loss =  0.38025161623954773\n",
            "Batch Training Loss =  0.4137483537197113\n",
            "Batch Training Loss =  0.3031061589717865\n",
            "Batch Training Loss =  0.4135726988315582\n",
            "Batch Training Loss =  0.4585948884487152\n",
            "Batch Training Loss =  0.49299824237823486\n",
            "Batch Training Loss =  0.4171256124973297\n",
            "Batch Training Loss =  0.3588641285896301\n",
            "Validation Loss in this epoch is 0.370\n",
            "This is  49 th epoch\n",
            "Batch Training Loss =  0.46007370948791504\n",
            "Batch Training Loss =  0.3068627417087555\n",
            "Batch Training Loss =  0.3962785005569458\n",
            "Batch Training Loss =  0.4767933189868927\n",
            "Batch Training Loss =  0.6546403765678406\n",
            "Batch Training Loss =  0.539531946182251\n",
            "Batch Training Loss =  0.41178402304649353\n",
            "Batch Training Loss =  0.33997806906700134\n",
            "Batch Training Loss =  0.42069461941719055\n",
            "Batch Training Loss =  0.2504848837852478\n",
            "Batch Training Loss =  0.3040194809436798\n",
            "Batch Training Loss =  0.9095661640167236\n",
            "Batch Training Loss =  0.4343179762363434\n",
            "Batch Training Loss =  0.4173000752925873\n",
            "Batch Training Loss =  0.4929594099521637\n",
            "Batch Training Loss =  0.43101829290390015\n",
            "Batch Training Loss =  0.3914986848831177\n",
            "Batch Training Loss =  0.3126179277896881\n",
            "Batch Training Loss =  0.33189770579338074\n",
            "Batch Training Loss =  0.40315085649490356\n",
            "Batch Training Loss =  0.5769948363304138\n",
            "Batch Training Loss =  0.4039952754974365\n",
            "Batch Training Loss =  0.4102252125740051\n",
            "Batch Training Loss =  0.34427115321159363\n",
            "Batch Training Loss =  0.4144289493560791\n",
            "Batch Training Loss =  0.39617010951042175\n",
            "Batch Training Loss =  0.3049249053001404\n",
            "Batch Training Loss =  0.36953598260879517\n",
            "Batch Training Loss =  0.5123217701911926\n",
            "Batch Training Loss =  0.400508850812912\n",
            "Batch Training Loss =  0.33318182826042175\n",
            "Batch Training Loss =  0.3261949419975281\n",
            "Validation Loss in this epoch is 0.487\n",
            "This is  50 th epoch\n",
            "Batch Training Loss =  0.3549988269805908\n",
            "Batch Training Loss =  0.5070894360542297\n",
            "Batch Training Loss =  0.5612332820892334\n",
            "Batch Training Loss =  0.41258570551872253\n",
            "Batch Training Loss =  0.3040962517261505\n",
            "Batch Training Loss =  0.3790777325630188\n",
            "Batch Training Loss =  0.3162078261375427\n",
            "Batch Training Loss =  0.328267365694046\n",
            "Batch Training Loss =  1.005061149597168\n",
            "Batch Training Loss =  1.0145050287246704\n",
            "Batch Training Loss =  0.7179334759712219\n",
            "Batch Training Loss =  0.68182772397995\n",
            "Batch Training Loss =  0.7042542099952698\n",
            "Batch Training Loss =  0.6258902549743652\n",
            "Batch Training Loss =  0.6184697151184082\n",
            "Batch Training Loss =  0.5920304656028748\n",
            "Batch Training Loss =  0.6102664470672607\n",
            "Batch Training Loss =  0.4734431505203247\n",
            "Batch Training Loss =  0.5353598594665527\n",
            "Batch Training Loss =  0.41713765263557434\n",
            "Batch Training Loss =  0.32650384306907654\n",
            "Batch Training Loss =  0.3902740478515625\n",
            "Batch Training Loss =  0.300861120223999\n",
            "Batch Training Loss =  0.3549465835094452\n",
            "Batch Training Loss =  0.6828728318214417\n",
            "Batch Training Loss =  1.2742741107940674\n",
            "Batch Training Loss =  0.5971609354019165\n",
            "Batch Training Loss =  0.5263776183128357\n",
            "Batch Training Loss =  0.5025954246520996\n",
            "Batch Training Loss =  0.4815424680709839\n",
            "Batch Training Loss =  0.7070074677467346\n",
            "Batch Training Loss =  0.6126039624214172\n",
            "Validation Loss in this epoch is 0.557\n",
            "This is  51 th epoch\n",
            "Batch Training Loss =  0.4883492588996887\n",
            "Batch Training Loss =  0.6666852831840515\n",
            "Batch Training Loss =  0.5231614112854004\n",
            "Batch Training Loss =  0.5184250473976135\n",
            "Batch Training Loss =  0.5259566903114319\n",
            "Batch Training Loss =  0.5161042213439941\n",
            "Batch Training Loss =  0.5596969127655029\n",
            "Batch Training Loss =  0.6032187342643738\n",
            "Batch Training Loss =  0.5160170793533325\n",
            "Batch Training Loss =  0.4457698464393616\n",
            "Batch Training Loss =  0.41892462968826294\n",
            "Batch Training Loss =  0.4427061080932617\n",
            "Batch Training Loss =  0.5046856999397278\n",
            "Batch Training Loss =  0.3976459503173828\n",
            "Batch Training Loss =  0.5489185452461243\n",
            "Batch Training Loss =  0.7762808203697205\n",
            "Batch Training Loss =  0.5118535757064819\n",
            "Batch Training Loss =  0.48136696219444275\n",
            "Batch Training Loss =  0.3730272650718689\n",
            "Batch Training Loss =  0.44571155309677124\n",
            "Batch Training Loss =  0.47486260533332825\n",
            "Batch Training Loss =  0.4285392761230469\n",
            "Batch Training Loss =  0.7876505255699158\n",
            "Batch Training Loss =  0.5944068431854248\n",
            "Batch Training Loss =  0.609157383441925\n",
            "Batch Training Loss =  0.3555256128311157\n",
            "Batch Training Loss =  0.39771130681037903\n",
            "Batch Training Loss =  0.3803376257419586\n",
            "Batch Training Loss =  0.3513486087322235\n",
            "Batch Training Loss =  0.42970532178878784\n",
            "Batch Training Loss =  0.7529657483100891\n",
            "Batch Training Loss =  0.8108545541763306\n",
            "Validation Loss in this epoch is 0.608\n",
            "This is  52 th epoch\n",
            "Batch Training Loss =  0.6107034087181091\n",
            "Batch Training Loss =  0.5992550253868103\n",
            "Batch Training Loss =  0.48256340622901917\n",
            "Batch Training Loss =  0.42572298645973206\n",
            "Batch Training Loss =  0.42507416009902954\n",
            "Batch Training Loss =  0.3447853922843933\n",
            "Batch Training Loss =  0.6142250299453735\n",
            "Batch Training Loss =  0.7495346665382385\n",
            "Batch Training Loss =  0.45864808559417725\n",
            "Batch Training Loss =  0.37558653950691223\n",
            "Batch Training Loss =  0.32308343052864075\n",
            "Batch Training Loss =  0.39266273379325867\n",
            "Batch Training Loss =  0.39194002747535706\n",
            "Batch Training Loss =  0.4182223081588745\n",
            "Batch Training Loss =  0.4608103930950165\n",
            "Batch Training Loss =  0.6948307752609253\n",
            "Batch Training Loss =  0.38268840312957764\n",
            "Batch Training Loss =  0.3687724173069\n",
            "Batch Training Loss =  0.28747430443763733\n",
            "Batch Training Loss =  0.503596305847168\n",
            "Batch Training Loss =  0.27121999859809875\n",
            "Batch Training Loss =  0.3260367214679718\n",
            "Batch Training Loss =  0.4163638949394226\n",
            "Batch Training Loss =  0.6921221017837524\n",
            "Batch Training Loss =  0.4796133041381836\n",
            "Batch Training Loss =  0.38041239976882935\n",
            "Batch Training Loss =  0.29897186160087585\n",
            "Batch Training Loss =  0.39288273453712463\n",
            "Batch Training Loss =  0.43083521723747253\n",
            "Batch Training Loss =  0.43904247879981995\n",
            "Batch Training Loss =  0.530885636806488\n",
            "Batch Training Loss =  0.38319405913352966\n",
            "Validation Loss in this epoch is 0.421\n",
            "This is  53 th epoch\n",
            "Batch Training Loss =  0.3794708549976349\n",
            "Batch Training Loss =  0.3106951415538788\n",
            "Batch Training Loss =  0.3980194926261902\n",
            "Batch Training Loss =  0.2519950866699219\n",
            "Batch Training Loss =  0.42104241251945496\n",
            "Batch Training Loss =  0.7723023295402527\n",
            "Batch Training Loss =  0.5716966390609741\n",
            "Batch Training Loss =  0.5706596970558167\n",
            "Batch Training Loss =  0.4722488820552826\n",
            "Batch Training Loss =  0.31740307807922363\n",
            "Batch Training Loss =  0.3646792471408844\n",
            "Batch Training Loss =  0.3575059771537781\n",
            "Batch Training Loss =  0.5997379422187805\n",
            "Batch Training Loss =  0.5072473287582397\n",
            "Batch Training Loss =  0.42816078662872314\n",
            "Batch Training Loss =  0.36437132954597473\n",
            "Batch Training Loss =  0.45531120896339417\n",
            "Batch Training Loss =  0.3369462192058563\n",
            "Batch Training Loss =  0.2602384388446808\n",
            "Batch Training Loss =  0.23231767117977142\n",
            "Batch Training Loss =  0.3162924647331238\n",
            "Batch Training Loss =  0.4405269920825958\n",
            "Batch Training Loss =  0.7846478223800659\n",
            "Batch Training Loss =  0.58497554063797\n",
            "Batch Training Loss =  0.4278720021247864\n",
            "Batch Training Loss =  0.43712612986564636\n",
            "Batch Training Loss =  0.4091220200061798\n",
            "Batch Training Loss =  0.32846778631210327\n",
            "Batch Training Loss =  0.42240893840789795\n",
            "Batch Training Loss =  0.22671549022197723\n",
            "Batch Training Loss =  0.25582700967788696\n",
            "Batch Training Loss =  0.34296074509620667\n",
            "Validation Loss in this epoch is 0.342\n",
            "This is  54 th epoch\n",
            "Batch Training Loss =  0.23743568360805511\n",
            "Batch Training Loss =  0.3260539770126343\n",
            "Batch Training Loss =  0.31147196888923645\n",
            "Batch Training Loss =  0.44373270869255066\n",
            "Batch Training Loss =  0.7528083920478821\n",
            "Batch Training Loss =  0.7582154870033264\n",
            "Batch Training Loss =  0.6144503951072693\n",
            "Batch Training Loss =  0.4851936399936676\n",
            "Batch Training Loss =  0.41215595602989197\n",
            "Batch Training Loss =  0.3326469361782074\n",
            "Batch Training Loss =  0.35025545954704285\n",
            "Batch Training Loss =  0.31936967372894287\n",
            "Batch Training Loss =  0.3922084867954254\n",
            "Batch Training Loss =  0.416473388671875\n",
            "Batch Training Loss =  0.6592556238174438\n",
            "Batch Training Loss =  0.4922083914279938\n",
            "Batch Training Loss =  0.5920019745826721\n",
            "Batch Training Loss =  0.4275158643722534\n",
            "Batch Training Loss =  0.4218544363975525\n",
            "Batch Training Loss =  0.3024976849555969\n",
            "Batch Training Loss =  0.273838609457016\n",
            "Batch Training Loss =  0.3552549481391907\n",
            "Batch Training Loss =  0.28406965732574463\n",
            "Batch Training Loss =  0.25702381134033203\n",
            "Batch Training Loss =  0.3285277187824249\n",
            "Batch Training Loss =  0.3920585513114929\n",
            "Batch Training Loss =  0.3021450638771057\n",
            "Batch Training Loss =  0.43662965297698975\n",
            "Batch Training Loss =  0.566480815410614\n",
            "Batch Training Loss =  0.3274722695350647\n",
            "Batch Training Loss =  0.37480491399765015\n",
            "Batch Training Loss =  0.35792404413223267\n",
            "Validation Loss in this epoch is 0.392\n",
            "This is  55 th epoch\n",
            "Batch Training Loss =  0.30085688829421997\n",
            "Batch Training Loss =  0.34386149048805237\n",
            "Batch Training Loss =  0.3174069821834564\n",
            "Batch Training Loss =  0.4291548430919647\n",
            "Batch Training Loss =  0.381675124168396\n",
            "Batch Training Loss =  0.44136661291122437\n",
            "Batch Training Loss =  0.3261314332485199\n",
            "Batch Training Loss =  0.28247836232185364\n",
            "Batch Training Loss =  0.26386168599128723\n",
            "Batch Training Loss =  0.33594173192977905\n",
            "Batch Training Loss =  0.38660699129104614\n",
            "Batch Training Loss =  0.3821164071559906\n",
            "Batch Training Loss =  0.36275333166122437\n",
            "Batch Training Loss =  0.3601410388946533\n",
            "Batch Training Loss =  0.26189321279525757\n",
            "Batch Training Loss =  0.6368364691734314\n",
            "Batch Training Loss =  0.8248559832572937\n",
            "Batch Training Loss =  0.5919105410575867\n",
            "Batch Training Loss =  0.6776679158210754\n",
            "Batch Training Loss =  0.5462709069252014\n",
            "Batch Training Loss =  0.5093442797660828\n",
            "Batch Training Loss =  0.4822101891040802\n",
            "Batch Training Loss =  0.3888762593269348\n",
            "Batch Training Loss =  0.35455557703971863\n",
            "Batch Training Loss =  0.4858064651489258\n",
            "Batch Training Loss =  0.4574153423309326\n",
            "Batch Training Loss =  0.4001118838787079\n",
            "Batch Training Loss =  0.3439902067184448\n",
            "Batch Training Loss =  0.16130536794662476\n",
            "Batch Training Loss =  0.41303935647010803\n",
            "Batch Training Loss =  0.47842586040496826\n",
            "Batch Training Loss =  0.8765988945960999\n",
            "Validation Loss in this epoch is 0.637\n",
            "This is  56 th epoch\n",
            "Batch Training Loss =  0.6995158195495605\n",
            "Batch Training Loss =  0.6273091435432434\n",
            "Batch Training Loss =  0.5311413407325745\n",
            "Batch Training Loss =  0.3844184875488281\n",
            "Batch Training Loss =  0.3693070709705353\n",
            "Batch Training Loss =  0.2849704921245575\n",
            "Batch Training Loss =  0.4507769048213959\n",
            "Batch Training Loss =  0.3789158761501312\n",
            "Batch Training Loss =  0.660226583480835\n",
            "Batch Training Loss =  0.5102823972702026\n",
            "Batch Training Loss =  0.3278377950191498\n",
            "Batch Training Loss =  0.3737787902355194\n",
            "Batch Training Loss =  0.35991206765174866\n",
            "Batch Training Loss =  0.6888095140457153\n",
            "Batch Training Loss =  0.5462160110473633\n",
            "Batch Training Loss =  0.4757647216320038\n",
            "Batch Training Loss =  0.4023977220058441\n",
            "Batch Training Loss =  0.2244144082069397\n",
            "Batch Training Loss =  0.3188199996948242\n",
            "Batch Training Loss =  0.33647850155830383\n",
            "Batch Training Loss =  0.3161503076553345\n",
            "Batch Training Loss =  0.3567022383213043\n",
            "Batch Training Loss =  0.6679767370223999\n",
            "Batch Training Loss =  0.4823133051395416\n",
            "Batch Training Loss =  0.4321633577346802\n",
            "Batch Training Loss =  0.39011096954345703\n",
            "Batch Training Loss =  0.3662193715572357\n",
            "Batch Training Loss =  0.33913207054138184\n",
            "Batch Training Loss =  0.3439745604991913\n",
            "Batch Training Loss =  0.5740777850151062\n",
            "Batch Training Loss =  0.6876177191734314\n",
            "Batch Training Loss =  0.5678504109382629\n",
            "Validation Loss in this epoch is 0.494\n",
            "This is  57 th epoch\n",
            "Batch Training Loss =  0.5488457679748535\n",
            "Batch Training Loss =  0.3422092795372009\n",
            "Batch Training Loss =  0.30460062623023987\n",
            "Batch Training Loss =  0.2765555679798126\n",
            "Batch Training Loss =  0.32426783442497253\n",
            "Batch Training Loss =  0.6474862694740295\n",
            "Batch Training Loss =  0.4655357599258423\n",
            "Batch Training Loss =  0.36691540479660034\n",
            "Batch Training Loss =  0.31682929396629333\n",
            "Batch Training Loss =  0.30625292658805847\n",
            "Batch Training Loss =  0.45994681119918823\n",
            "Batch Training Loss =  0.5659597516059875\n",
            "Batch Training Loss =  0.4957546889781952\n",
            "Batch Training Loss =  0.420567125082016\n",
            "Batch Training Loss =  0.3431035578250885\n",
            "Batch Training Loss =  0.3554016947746277\n",
            "Batch Training Loss =  0.26454541087150574\n",
            "Batch Training Loss =  0.2893732786178589\n",
            "Batch Training Loss =  0.597245454788208\n",
            "Batch Training Loss =  0.434693306684494\n",
            "Batch Training Loss =  0.3216915428638458\n",
            "Batch Training Loss =  0.37909069657325745\n",
            "Batch Training Loss =  0.34761881828308105\n",
            "Batch Training Loss =  0.4201081097126007\n",
            "Batch Training Loss =  0.35621964931488037\n",
            "Batch Training Loss =  0.3570024073123932\n",
            "Batch Training Loss =  0.34797465801239014\n",
            "Batch Training Loss =  0.4429495930671692\n",
            "Batch Training Loss =  0.8671538829803467\n",
            "Batch Training Loss =  0.6156873106956482\n",
            "Batch Training Loss =  0.5227590799331665\n",
            "Batch Training Loss =  0.45923247933387756\n",
            "Validation Loss in this epoch is 0.449\n",
            "This is  58 th epoch\n",
            "Batch Training Loss =  0.5615870952606201\n",
            "Batch Training Loss =  0.47268199920654297\n",
            "Batch Training Loss =  0.4154970943927765\n",
            "Batch Training Loss =  0.4584622085094452\n",
            "Batch Training Loss =  0.2710748314857483\n",
            "Batch Training Loss =  0.22981591522693634\n",
            "Batch Training Loss =  0.46272724866867065\n",
            "Batch Training Loss =  0.4890647232532501\n",
            "Batch Training Loss =  0.5050815343856812\n",
            "Batch Training Loss =  0.347880482673645\n",
            "Batch Training Loss =  0.2768170237541199\n",
            "Batch Training Loss =  0.3533533215522766\n",
            "Batch Training Loss =  0.42751139402389526\n",
            "Batch Training Loss =  0.35177016258239746\n",
            "Batch Training Loss =  0.2785898447036743\n",
            "Batch Training Loss =  0.37550458312034607\n",
            "Batch Training Loss =  0.3901703357696533\n",
            "Batch Training Loss =  0.42695385217666626\n",
            "Batch Training Loss =  0.26028504967689514\n",
            "Batch Training Loss =  0.5338456034660339\n",
            "Batch Training Loss =  0.767924427986145\n",
            "Batch Training Loss =  0.42865100502967834\n",
            "Batch Training Loss =  0.3582113981246948\n",
            "Batch Training Loss =  0.3527243137359619\n",
            "Batch Training Loss =  0.33322685956954956\n",
            "Batch Training Loss =  0.18141834437847137\n",
            "Batch Training Loss =  0.2711869180202484\n",
            "Batch Training Loss =  0.5240942239761353\n",
            "Batch Training Loss =  0.47396358847618103\n",
            "Batch Training Loss =  0.2963048219680786\n",
            "Batch Training Loss =  0.4068647027015686\n",
            "Batch Training Loss =  0.5840339064598083\n",
            "Validation Loss in this epoch is 0.443\n",
            "This is  59 th epoch\n",
            "Batch Training Loss =  0.40015947818756104\n",
            "Batch Training Loss =  0.35616183280944824\n",
            "Batch Training Loss =  0.3841361701488495\n",
            "Batch Training Loss =  0.30708178877830505\n",
            "Batch Training Loss =  0.33956217765808105\n",
            "Batch Training Loss =  0.23788852989673615\n",
            "Batch Training Loss =  0.13930517435073853\n",
            "Batch Training Loss =  0.39563849568367004\n",
            "Batch Training Loss =  0.2697039246559143\n",
            "Batch Training Loss =  0.31904757022857666\n",
            "Batch Training Loss =  0.4474348723888397\n",
            "Batch Training Loss =  0.7135426998138428\n",
            "Batch Training Loss =  0.4189275801181793\n",
            "Batch Training Loss =  0.32973921298980713\n",
            "Batch Training Loss =  0.40225398540496826\n",
            "Batch Training Loss =  0.3098553419113159\n",
            "Batch Training Loss =  0.3744068443775177\n",
            "Batch Training Loss =  0.39757782220840454\n",
            "Batch Training Loss =  0.27817824482917786\n",
            "Batch Training Loss =  0.24925029277801514\n",
            "Batch Training Loss =  0.417879194021225\n",
            "Batch Training Loss =  0.35258588194847107\n",
            "Batch Training Loss =  0.32721883058547974\n",
            "Batch Training Loss =  0.3943043649196625\n",
            "Batch Training Loss =  0.44729670882225037\n",
            "Batch Training Loss =  0.4898746907711029\n",
            "Batch Training Loss =  0.4170248806476593\n",
            "Batch Training Loss =  0.3143552541732788\n",
            "Batch Training Loss =  0.3315611183643341\n",
            "Batch Training Loss =  0.4486927390098572\n",
            "Batch Training Loss =  0.43146124482154846\n",
            "Batch Training Loss =  0.4451574981212616\n",
            "Validation Loss in this epoch is 0.390\n",
            "This is  60 th epoch\n",
            "Batch Training Loss =  0.33651596307754517\n",
            "Batch Training Loss =  0.4190400242805481\n",
            "Batch Training Loss =  0.36443862318992615\n",
            "Batch Training Loss =  0.31059619784355164\n",
            "Batch Training Loss =  0.3082575500011444\n",
            "Batch Training Loss =  0.40174999833106995\n",
            "Batch Training Loss =  0.5313335657119751\n",
            "Batch Training Loss =  0.34906676411628723\n",
            "Batch Training Loss =  0.33310872316360474\n",
            "Batch Training Loss =  0.3349221348762512\n",
            "Batch Training Loss =  0.19823114573955536\n",
            "Batch Training Loss =  0.32344627380371094\n",
            "Batch Training Loss =  0.36596766114234924\n",
            "Batch Training Loss =  0.4216819405555725\n",
            "Batch Training Loss =  0.6215226650238037\n",
            "Batch Training Loss =  0.4644092321395874\n",
            "Batch Training Loss =  0.41925469040870667\n",
            "Batch Training Loss =  0.3379380702972412\n",
            "Batch Training Loss =  0.26237472891807556\n",
            "Batch Training Loss =  0.4779110848903656\n",
            "Batch Training Loss =  0.40424782037734985\n",
            "Batch Training Loss =  0.5002648234367371\n",
            "Batch Training Loss =  0.27951332926750183\n",
            "Batch Training Loss =  0.3254960775375366\n",
            "Batch Training Loss =  0.4708251655101776\n",
            "Batch Training Loss =  0.30313587188720703\n",
            "Batch Training Loss =  0.28625214099884033\n",
            "Batch Training Loss =  0.40200746059417725\n",
            "Batch Training Loss =  0.330128014087677\n",
            "Batch Training Loss =  0.3488205075263977\n",
            "Batch Training Loss =  0.30078545212745667\n",
            "Batch Training Loss =  0.2773992419242859\n",
            "Validation Loss in this epoch is 0.339\n",
            "This is  61 th epoch\n",
            "Batch Training Loss =  0.37659385800361633\n",
            "Batch Training Loss =  0.27930036187171936\n",
            "Batch Training Loss =  0.4525578022003174\n",
            "Batch Training Loss =  0.46232399344444275\n",
            "Batch Training Loss =  0.41415414214134216\n",
            "Batch Training Loss =  0.313615620136261\n",
            "Batch Training Loss =  0.3723618686199188\n",
            "Batch Training Loss =  0.4537813663482666\n",
            "Batch Training Loss =  0.4908294677734375\n",
            "Batch Training Loss =  0.31596583127975464\n",
            "Batch Training Loss =  0.31353867053985596\n",
            "Batch Training Loss =  0.44135409593582153\n",
            "Batch Training Loss =  0.5249390602111816\n",
            "Batch Training Loss =  0.4718295633792877\n",
            "Batch Training Loss =  0.29928141832351685\n",
            "Batch Training Loss =  0.3112352788448334\n",
            "Batch Training Loss =  0.2902919352054596\n",
            "Batch Training Loss =  0.4550696611404419\n",
            "Batch Training Loss =  0.2923871874809265\n",
            "Batch Training Loss =  0.41512688994407654\n",
            "Batch Training Loss =  0.2869589328765869\n",
            "Batch Training Loss =  0.2935273349285126\n",
            "Batch Training Loss =  0.23818454146385193\n",
            "Batch Training Loss =  0.4656505286693573\n",
            "Batch Training Loss =  0.4080994725227356\n",
            "Batch Training Loss =  0.325267493724823\n",
            "Batch Training Loss =  0.3385794460773468\n",
            "Batch Training Loss =  0.46137192845344543\n",
            "Batch Training Loss =  0.41647419333457947\n",
            "Batch Training Loss =  0.24835461378097534\n",
            "Batch Training Loss =  0.39805757999420166\n",
            "Batch Training Loss =  0.39746353030204773\n",
            "Validation Loss in this epoch is 0.386\n",
            "This is  62 th epoch\n",
            "Batch Training Loss =  0.3302600085735321\n",
            "Batch Training Loss =  0.2766112983226776\n",
            "Batch Training Loss =  0.33229926228523254\n",
            "Batch Training Loss =  0.2751432955265045\n",
            "Batch Training Loss =  0.2656955122947693\n",
            "Batch Training Loss =  0.4038107395172119\n",
            "Batch Training Loss =  0.5058587789535522\n",
            "Batch Training Loss =  0.4478258490562439\n",
            "Batch Training Loss =  0.4604092538356781\n",
            "Batch Training Loss =  0.43840518593788147\n",
            "Batch Training Loss =  0.2987418472766876\n",
            "Batch Training Loss =  0.3327287435531616\n",
            "Batch Training Loss =  0.24340644478797913\n",
            "Batch Training Loss =  0.5138581395149231\n",
            "Batch Training Loss =  1.0140764713287354\n",
            "Batch Training Loss =  0.4502967894077301\n",
            "Batch Training Loss =  0.5382774472236633\n",
            "Batch Training Loss =  0.5162991285324097\n",
            "Batch Training Loss =  0.4491269588470459\n",
            "Batch Training Loss =  0.34171515703201294\n",
            "Batch Training Loss =  0.24668116867542267\n",
            "Batch Training Loss =  0.3496283292770386\n",
            "Batch Training Loss =  0.8545257449150085\n",
            "Batch Training Loss =  1.05535888671875\n",
            "Batch Training Loss =  0.6882057785987854\n",
            "Batch Training Loss =  0.6493560671806335\n",
            "Batch Training Loss =  0.7206088900566101\n",
            "Batch Training Loss =  0.6621446013450623\n",
            "Batch Training Loss =  0.6729264259338379\n",
            "Batch Training Loss =  0.6010923385620117\n",
            "Batch Training Loss =  0.556492030620575\n",
            "Batch Training Loss =  0.530332624912262\n",
            "Validation Loss in this epoch is 0.483\n",
            "This is  63 th epoch\n",
            "Batch Training Loss =  0.3987451493740082\n",
            "Batch Training Loss =  0.3874301016330719\n",
            "Batch Training Loss =  0.5018399953842163\n",
            "Batch Training Loss =  0.4997437000274658\n",
            "Batch Training Loss =  0.5790753960609436\n",
            "Batch Training Loss =  0.5561124086380005\n",
            "Batch Training Loss =  0.44994693994522095\n",
            "Batch Training Loss =  0.4128037989139557\n",
            "Batch Training Loss =  0.3845735788345337\n",
            "Batch Training Loss =  0.36743950843811035\n",
            "Batch Training Loss =  0.8162699341773987\n",
            "Batch Training Loss =  0.6724523305892944\n",
            "Batch Training Loss =  0.6085092425346375\n",
            "Batch Training Loss =  0.5711861252784729\n",
            "Batch Training Loss =  0.5001511573791504\n",
            "Batch Training Loss =  0.4281969368457794\n",
            "Batch Training Loss =  0.3112374246120453\n",
            "Batch Training Loss =  0.38675516843795776\n",
            "Batch Training Loss =  0.4508218467235565\n",
            "Batch Training Loss =  0.35349246859550476\n",
            "Batch Training Loss =  0.35070690512657166\n",
            "Batch Training Loss =  0.3146875202655792\n",
            "Batch Training Loss =  0.5272255539894104\n",
            "Batch Training Loss =  0.3232848048210144\n",
            "Batch Training Loss =  0.27344226837158203\n",
            "Batch Training Loss =  0.24257038533687592\n",
            "Batch Training Loss =  0.3359586298465729\n",
            "Batch Training Loss =  0.33529359102249146\n",
            "Batch Training Loss =  0.2890567183494568\n",
            "Batch Training Loss =  0.25933706760406494\n",
            "Batch Training Loss =  0.41256043314933777\n",
            "Batch Training Loss =  0.2459748089313507\n",
            "Validation Loss in this epoch is 0.455\n",
            "This is  64 th epoch\n",
            "Batch Training Loss =  0.4242686331272125\n",
            "Batch Training Loss =  0.35758140683174133\n",
            "Batch Training Loss =  0.2579953670501709\n",
            "Batch Training Loss =  0.3173711895942688\n",
            "Batch Training Loss =  0.29082798957824707\n",
            "Batch Training Loss =  0.3039841949939728\n",
            "Batch Training Loss =  0.25806498527526855\n",
            "Batch Training Loss =  0.453660786151886\n",
            "Batch Training Loss =  0.16435402631759644\n",
            "Batch Training Loss =  0.28205177187919617\n",
            "Batch Training Loss =  0.4943024516105652\n",
            "Batch Training Loss =  0.6301729679107666\n",
            "Batch Training Loss =  0.8516970872879028\n",
            "Batch Training Loss =  0.48677581548690796\n",
            "Batch Training Loss =  0.42887428402900696\n",
            "Batch Training Loss =  0.3638976812362671\n",
            "Batch Training Loss =  0.3428649604320526\n",
            "Batch Training Loss =  0.4401729106903076\n",
            "Batch Training Loss =  0.3010536730289459\n",
            "Batch Training Loss =  0.2668357193470001\n",
            "Batch Training Loss =  0.38783273100852966\n",
            "Batch Training Loss =  0.4385026693344116\n",
            "Batch Training Loss =  0.5551425814628601\n",
            "Batch Training Loss =  0.49107876420021057\n",
            "Batch Training Loss =  0.361765056848526\n",
            "Batch Training Loss =  0.4508490264415741\n",
            "Batch Training Loss =  0.3954865634441376\n",
            "Batch Training Loss =  0.37716904282569885\n",
            "Batch Training Loss =  0.39093831181526184\n",
            "Batch Training Loss =  0.33842384815216064\n",
            "Batch Training Loss =  0.3388083279132843\n",
            "Batch Training Loss =  0.36488479375839233\n",
            "Validation Loss in this epoch is 0.431\n",
            "This is  65 th epoch\n",
            "Batch Training Loss =  0.4129573106765747\n",
            "Batch Training Loss =  0.39210060238838196\n",
            "Batch Training Loss =  0.3788880407810211\n",
            "Batch Training Loss =  0.2875324487686157\n",
            "Batch Training Loss =  0.5498300790786743\n",
            "Batch Training Loss =  0.3575823903083801\n",
            "Batch Training Loss =  0.25293925404548645\n",
            "Batch Training Loss =  0.3593308925628662\n",
            "Batch Training Loss =  0.3463761508464813\n",
            "Batch Training Loss =  0.32285076379776\n",
            "Batch Training Loss =  0.48455533385276794\n",
            "Batch Training Loss =  0.3456169366836548\n",
            "Batch Training Loss =  0.378464937210083\n",
            "Batch Training Loss =  0.4099893569946289\n",
            "Batch Training Loss =  0.4626516103744507\n",
            "Batch Training Loss =  0.31708672642707825\n",
            "Batch Training Loss =  0.30614781379699707\n",
            "Batch Training Loss =  0.27991077303886414\n",
            "Batch Training Loss =  0.20206239819526672\n",
            "Batch Training Loss =  0.3919115364551544\n",
            "Batch Training Loss =  0.7606560587882996\n",
            "Batch Training Loss =  0.43518465757369995\n",
            "Batch Training Loss =  0.46474674344062805\n",
            "Batch Training Loss =  0.43117889761924744\n",
            "Batch Training Loss =  0.3298420011997223\n",
            "Batch Training Loss =  0.47541335225105286\n",
            "Batch Training Loss =  0.5512195825576782\n",
            "Batch Training Loss =  0.3505519926548004\n",
            "Batch Training Loss =  0.4531267285346985\n",
            "Batch Training Loss =  0.35686787962913513\n",
            "Batch Training Loss =  0.4013165533542633\n",
            "Batch Training Loss =  0.3976903557777405\n",
            "Validation Loss in this epoch is 0.399\n",
            "This is  66 th epoch\n",
            "Batch Training Loss =  0.26173073053359985\n",
            "Batch Training Loss =  0.30934324860572815\n",
            "Batch Training Loss =  0.35714423656463623\n",
            "Batch Training Loss =  0.45041438937187195\n",
            "Batch Training Loss =  0.6137148141860962\n",
            "Batch Training Loss =  0.604360818862915\n",
            "Batch Training Loss =  0.4220060706138611\n",
            "Batch Training Loss =  0.25861626863479614\n",
            "Batch Training Loss =  0.3457376956939697\n",
            "Batch Training Loss =  0.2523902356624603\n",
            "Batch Training Loss =  0.34766849875450134\n",
            "Batch Training Loss =  0.39518311619758606\n",
            "Batch Training Loss =  0.3573782444000244\n",
            "Batch Training Loss =  0.3451462686061859\n",
            "Batch Training Loss =  0.39976924657821655\n",
            "Batch Training Loss =  0.2857731878757477\n",
            "Batch Training Loss =  0.3200877904891968\n",
            "Batch Training Loss =  0.2811545133590698\n",
            "Batch Training Loss =  0.18611370027065277\n",
            "Batch Training Loss =  0.26989609003067017\n",
            "Batch Training Loss =  0.4627106487751007\n",
            "Batch Training Loss =  0.48845091462135315\n",
            "Batch Training Loss =  0.2971622347831726\n",
            "Batch Training Loss =  0.42784324288368225\n",
            "Batch Training Loss =  0.3897263705730438\n",
            "Batch Training Loss =  0.4489826560020447\n",
            "Batch Training Loss =  0.2948775589466095\n",
            "Batch Training Loss =  0.522843599319458\n",
            "Batch Training Loss =  0.4901476800441742\n",
            "Batch Training Loss =  0.4116194248199463\n",
            "Batch Training Loss =  0.30115315318107605\n",
            "Batch Training Loss =  0.3147661089897156\n",
            "Validation Loss in this epoch is 0.405\n",
            "This is  67 th epoch\n",
            "Batch Training Loss =  0.4062840938568115\n",
            "Batch Training Loss =  0.39203765988349915\n",
            "Batch Training Loss =  0.361286461353302\n",
            "Batch Training Loss =  0.4928613305091858\n",
            "Batch Training Loss =  0.25789135694503784\n",
            "Batch Training Loss =  0.39040181040763855\n",
            "Batch Training Loss =  0.293875128030777\n",
            "Batch Training Loss =  0.21719543635845184\n",
            "Batch Training Loss =  0.3776364028453827\n",
            "Batch Training Loss =  0.2849818766117096\n",
            "Batch Training Loss =  0.4726077914237976\n",
            "Batch Training Loss =  1.2751731872558594\n",
            "Batch Training Loss =  0.7993575930595398\n",
            "Batch Training Loss =  0.6147246360778809\n",
            "Batch Training Loss =  0.5708116292953491\n",
            "Batch Training Loss =  0.599021315574646\n",
            "Batch Training Loss =  0.6491447687149048\n",
            "Batch Training Loss =  0.5813360810279846\n",
            "Batch Training Loss =  0.638007640838623\n",
            "Batch Training Loss =  0.5919231176376343\n",
            "Batch Training Loss =  0.5298746228218079\n",
            "Batch Training Loss =  0.6179905533790588\n",
            "Batch Training Loss =  0.5522270798683167\n",
            "Batch Training Loss =  0.5277960896492004\n",
            "Batch Training Loss =  0.4998260736465454\n",
            "Batch Training Loss =  0.487369567155838\n",
            "Batch Training Loss =  0.5745262503623962\n",
            "Batch Training Loss =  0.563147783279419\n",
            "Batch Training Loss =  0.5671327114105225\n",
            "Batch Training Loss =  0.58625727891922\n",
            "Batch Training Loss =  0.4725121557712555\n",
            "Batch Training Loss =  0.4670531153678894\n",
            "Validation Loss in this epoch is 0.480\n",
            "This is  68 th epoch\n",
            "Batch Training Loss =  0.4477395713329315\n",
            "Batch Training Loss =  0.4860438108444214\n",
            "Batch Training Loss =  0.5903559327125549\n",
            "Batch Training Loss =  0.549998939037323\n",
            "Batch Training Loss =  0.41401466727256775\n",
            "Batch Training Loss =  0.4703468382358551\n",
            "Batch Training Loss =  0.5943619608879089\n",
            "Batch Training Loss =  0.6382513642311096\n",
            "Batch Training Loss =  0.7119269371032715\n",
            "Batch Training Loss =  0.6520100235939026\n",
            "Batch Training Loss =  0.6689058542251587\n",
            "Batch Training Loss =  0.6419984698295593\n",
            "Batch Training Loss =  0.5316125750541687\n",
            "Batch Training Loss =  0.4870815575122833\n",
            "Batch Training Loss =  0.4616546332836151\n",
            "Batch Training Loss =  0.47078078985214233\n",
            "Batch Training Loss =  0.39476868510246277\n",
            "Batch Training Loss =  0.3434271216392517\n",
            "Batch Training Loss =  0.34504127502441406\n",
            "Batch Training Loss =  0.45544102787971497\n",
            "Batch Training Loss =  0.4783381521701813\n",
            "Batch Training Loss =  0.34724321961402893\n",
            "Batch Training Loss =  0.29375168681144714\n",
            "Batch Training Loss =  0.24547003209590912\n",
            "Batch Training Loss =  0.3319765031337738\n",
            "Batch Training Loss =  0.6120493412017822\n",
            "Batch Training Loss =  0.4685310423374176\n",
            "Batch Training Loss =  0.42190250754356384\n",
            "Batch Training Loss =  0.44438257813453674\n",
            "Batch Training Loss =  0.2799760699272156\n",
            "Batch Training Loss =  0.3000272214412689\n",
            "Batch Training Loss =  0.4318917393684387\n",
            "Validation Loss in this epoch is 0.684\n",
            "This is  69 th epoch\n",
            "Batch Training Loss =  0.43775463104248047\n",
            "Batch Training Loss =  0.6075408458709717\n",
            "Batch Training Loss =  0.3022964596748352\n",
            "Batch Training Loss =  0.3491123914718628\n",
            "Batch Training Loss =  0.3153563439846039\n",
            "Batch Training Loss =  0.22575247287750244\n",
            "Batch Training Loss =  0.3590638041496277\n",
            "Batch Training Loss =  0.3094184100627899\n",
            "Batch Training Loss =  0.3974495232105255\n",
            "Batch Training Loss =  0.3665112257003784\n",
            "Batch Training Loss =  0.41521376371383667\n",
            "Batch Training Loss =  0.41074469685554504\n",
            "Batch Training Loss =  0.40395987033843994\n",
            "Batch Training Loss =  0.3496849536895752\n",
            "Batch Training Loss =  0.359127014875412\n",
            "Batch Training Loss =  0.5366445183753967\n",
            "Batch Training Loss =  0.859607994556427\n",
            "Batch Training Loss =  0.4649409353733063\n",
            "Batch Training Loss =  0.4333358705043793\n",
            "Batch Training Loss =  0.41734421253204346\n",
            "Batch Training Loss =  0.29184651374816895\n",
            "Batch Training Loss =  0.4210928976535797\n",
            "Batch Training Loss =  0.38486048579216003\n",
            "Batch Training Loss =  0.5147855877876282\n",
            "Batch Training Loss =  0.46180713176727295\n",
            "Batch Training Loss =  0.4471887946128845\n",
            "Batch Training Loss =  0.3106984794139862\n",
            "Batch Training Loss =  0.30470383167266846\n",
            "Batch Training Loss =  0.3147038221359253\n",
            "Batch Training Loss =  0.2932242453098297\n",
            "Batch Training Loss =  0.3137335181236267\n",
            "Batch Training Loss =  0.38784751296043396\n",
            "Validation Loss in this epoch is 0.441\n",
            "This is  70 th epoch\n",
            "Batch Training Loss =  0.3085108995437622\n",
            "Batch Training Loss =  0.24549837410449982\n",
            "Batch Training Loss =  0.3049604594707489\n",
            "Batch Training Loss =  0.29210224747657776\n",
            "Batch Training Loss =  0.6631901264190674\n",
            "Batch Training Loss =  0.9243928790092468\n",
            "Batch Training Loss =  0.5891931653022766\n",
            "Batch Training Loss =  0.5198344588279724\n",
            "Batch Training Loss =  0.40601128339767456\n",
            "Batch Training Loss =  0.3785964250564575\n",
            "Batch Training Loss =  0.48651883006095886\n",
            "Batch Training Loss =  0.40958741307258606\n",
            "Batch Training Loss =  0.2647002041339874\n",
            "Batch Training Loss =  0.25647127628326416\n",
            "Batch Training Loss =  0.2359660118818283\n",
            "Batch Training Loss =  0.45114240050315857\n",
            "Batch Training Loss =  0.5886595249176025\n",
            "Batch Training Loss =  0.39894217252731323\n",
            "Batch Training Loss =  0.3554989695549011\n",
            "Batch Training Loss =  0.3276480436325073\n",
            "Batch Training Loss =  0.3283262550830841\n",
            "Batch Training Loss =  0.2955379784107208\n",
            "Batch Training Loss =  0.5666012167930603\n",
            "Batch Training Loss =  0.6805341243743896\n",
            "Batch Training Loss =  0.6800047159194946\n",
            "Batch Training Loss =  0.5965273976325989\n",
            "Batch Training Loss =  0.5108189582824707\n",
            "Batch Training Loss =  0.41347038745880127\n",
            "Batch Training Loss =  0.3583974838256836\n",
            "Batch Training Loss =  0.21572168171405792\n",
            "Batch Training Loss =  0.396102637052536\n",
            "Batch Training Loss =  0.4966031014919281\n",
            "Validation Loss in this epoch is 0.549\n",
            "This is  71 th epoch\n",
            "Batch Training Loss =  0.7106484770774841\n",
            "Batch Training Loss =  0.4329412579536438\n",
            "Batch Training Loss =  0.29002898931503296\n",
            "Batch Training Loss =  0.3072504699230194\n",
            "Batch Training Loss =  0.25266480445861816\n",
            "Batch Training Loss =  0.6415398120880127\n",
            "Batch Training Loss =  0.31203311681747437\n",
            "Batch Training Loss =  0.4828048050403595\n",
            "Batch Training Loss =  0.6469832062721252\n",
            "Batch Training Loss =  0.37180233001708984\n",
            "Batch Training Loss =  0.42312636971473694\n",
            "Batch Training Loss =  0.3575526773929596\n",
            "Batch Training Loss =  0.326002299785614\n",
            "Batch Training Loss =  0.36386585235595703\n",
            "Batch Training Loss =  0.27301859855651855\n",
            "Batch Training Loss =  0.22590993344783783\n",
            "Batch Training Loss =  0.32902777194976807\n",
            "Batch Training Loss =  0.43217459321022034\n",
            "Batch Training Loss =  0.807830274105072\n",
            "Batch Training Loss =  0.476986289024353\n",
            "Batch Training Loss =  0.48895689845085144\n",
            "Batch Training Loss =  0.3471512496471405\n",
            "Batch Training Loss =  0.34461188316345215\n",
            "Batch Training Loss =  0.37590354681015015\n",
            "Batch Training Loss =  0.2501084804534912\n",
            "Batch Training Loss =  0.21845625340938568\n",
            "Batch Training Loss =  0.40827301144599915\n",
            "Batch Training Loss =  0.5386030077934265\n",
            "Batch Training Loss =  0.44864699244499207\n",
            "Batch Training Loss =  0.5352981686592102\n",
            "Batch Training Loss =  0.31393271684646606\n",
            "Batch Training Loss =  0.26656797528266907\n",
            "Validation Loss in this epoch is 0.325\n",
            "This is  72 th epoch\n",
            "Batch Training Loss =  0.17911677062511444\n",
            "Batch Training Loss =  0.2385719120502472\n",
            "Batch Training Loss =  0.2950816750526428\n",
            "Batch Training Loss =  0.3130645453929901\n",
            "Batch Training Loss =  0.33737218379974365\n",
            "Batch Training Loss =  0.6727408766746521\n",
            "Batch Training Loss =  0.40613946318626404\n",
            "Batch Training Loss =  0.3733924925327301\n",
            "Batch Training Loss =  0.26359447836875916\n",
            "Batch Training Loss =  0.3082231879234314\n",
            "Batch Training Loss =  0.3313680589199066\n",
            "Batch Training Loss =  0.5513561964035034\n",
            "Batch Training Loss =  0.3962056636810303\n",
            "Batch Training Loss =  0.45161929726600647\n",
            "Batch Training Loss =  0.34510597586631775\n",
            "Batch Training Loss =  0.341389000415802\n",
            "Batch Training Loss =  0.4395948648452759\n",
            "Batch Training Loss =  0.3731735646724701\n",
            "Batch Training Loss =  0.4565657675266266\n",
            "Batch Training Loss =  0.37651458382606506\n",
            "Batch Training Loss =  0.28089261054992676\n",
            "Batch Training Loss =  0.2566260099411011\n",
            "Batch Training Loss =  0.31549012660980225\n",
            "Batch Training Loss =  0.26629549264907837\n",
            "Batch Training Loss =  0.27925947308540344\n",
            "Batch Training Loss =  0.42959699034690857\n",
            "Batch Training Loss =  0.33331894874572754\n",
            "Batch Training Loss =  0.32366591691970825\n",
            "Batch Training Loss =  0.2947324514389038\n",
            "Batch Training Loss =  0.45000672340393066\n",
            "Batch Training Loss =  0.38086995482444763\n",
            "Batch Training Loss =  0.35046082735061646\n",
            "Validation Loss in this epoch is 0.397\n",
            "This is  73 th epoch\n",
            "Batch Training Loss =  0.42382434010505676\n",
            "Batch Training Loss =  0.43973308801651\n",
            "Batch Training Loss =  0.4331837594509125\n",
            "Batch Training Loss =  0.36984336376190186\n",
            "Batch Training Loss =  0.39214587211608887\n",
            "Batch Training Loss =  0.15750961005687714\n",
            "Batch Training Loss =  0.32761356234550476\n",
            "Batch Training Loss =  0.8293396830558777\n",
            "Batch Training Loss =  0.5333219766616821\n",
            "Batch Training Loss =  0.3871438503265381\n",
            "Batch Training Loss =  0.5705254077911377\n",
            "Batch Training Loss =  0.4949986934661865\n",
            "Batch Training Loss =  0.40874141454696655\n",
            "Batch Training Loss =  0.46399563550949097\n",
            "Batch Training Loss =  0.3265272080898285\n",
            "Batch Training Loss =  0.4242839813232422\n",
            "Batch Training Loss =  0.49937576055526733\n",
            "Batch Training Loss =  0.3263038694858551\n",
            "Batch Training Loss =  0.31410229206085205\n",
            "Batch Training Loss =  0.3689354658126831\n",
            "Batch Training Loss =  0.3922876715660095\n",
            "Batch Training Loss =  0.29448530077934265\n",
            "Batch Training Loss =  0.3503141701221466\n",
            "Batch Training Loss =  0.5128013491630554\n",
            "Batch Training Loss =  0.5400062799453735\n",
            "Batch Training Loss =  0.37140601873397827\n",
            "Batch Training Loss =  0.3213615119457245\n",
            "Batch Training Loss =  0.3156648874282837\n",
            "Batch Training Loss =  0.25823575258255005\n",
            "Batch Training Loss =  0.32496875524520874\n",
            "Batch Training Loss =  0.6099445223808289\n",
            "Batch Training Loss =  0.4272116720676422\n",
            "Validation Loss in this epoch is 0.414\n",
            "This is  74 th epoch\n",
            "Batch Training Loss =  0.47113654017448425\n",
            "Batch Training Loss =  0.5143009424209595\n",
            "Batch Training Loss =  0.49271759390830994\n",
            "Batch Training Loss =  0.4160103499889374\n",
            "Batch Training Loss =  0.27290159463882446\n",
            "Batch Training Loss =  0.3115130662918091\n",
            "Batch Training Loss =  0.24914906919002533\n",
            "Batch Training Loss =  0.22887201607227325\n",
            "Batch Training Loss =  0.38741642236709595\n",
            "Batch Training Loss =  0.6652557253837585\n",
            "Batch Training Loss =  0.5189599394798279\n",
            "Batch Training Loss =  0.5732656717300415\n",
            "Batch Training Loss =  0.47777336835861206\n",
            "Batch Training Loss =  0.26018092036247253\n",
            "Batch Training Loss =  0.33734646439552307\n",
            "Batch Training Loss =  0.4121863543987274\n",
            "Batch Training Loss =  0.3959188759326935\n",
            "Batch Training Loss =  0.33198559284210205\n",
            "Batch Training Loss =  0.3083384335041046\n",
            "Batch Training Loss =  0.3791712522506714\n",
            "Batch Training Loss =  0.33404579758644104\n",
            "Batch Training Loss =  0.2821162939071655\n",
            "Batch Training Loss =  0.5982248187065125\n",
            "Batch Training Loss =  0.4487083852291107\n",
            "Batch Training Loss =  0.35913482308387756\n",
            "Batch Training Loss =  0.33026859164237976\n",
            "Batch Training Loss =  0.3782084584236145\n",
            "Batch Training Loss =  0.27778780460357666\n",
            "Batch Training Loss =  0.33425799012184143\n",
            "Batch Training Loss =  0.33920133113861084\n",
            "Batch Training Loss =  0.29878729581832886\n",
            "Batch Training Loss =  0.3067646622657776\n",
            "Validation Loss in this epoch is 0.379\n",
            "This is  75 th epoch\n",
            "Batch Training Loss =  0.3633362650871277\n",
            "Batch Training Loss =  0.726895809173584\n",
            "Batch Training Loss =  0.8279477953910828\n",
            "Batch Training Loss =  0.6926127672195435\n",
            "Batch Training Loss =  0.6597394347190857\n",
            "Batch Training Loss =  0.6052238345146179\n",
            "Batch Training Loss =  0.5475634336471558\n",
            "Batch Training Loss =  0.4785289764404297\n",
            "Batch Training Loss =  0.34048375487327576\n",
            "Batch Training Loss =  0.26364004611968994\n",
            "Batch Training Loss =  0.22803859412670135\n",
            "Batch Training Loss =  0.2863421142101288\n",
            "Batch Training Loss =  0.3938632607460022\n",
            "Batch Training Loss =  0.3245960474014282\n",
            "Batch Training Loss =  0.3495218753814697\n",
            "Batch Training Loss =  0.5236008763313293\n",
            "Batch Training Loss =  0.5432299375534058\n",
            "Batch Training Loss =  0.4403247535228729\n",
            "Batch Training Loss =  0.3601419925689697\n",
            "Batch Training Loss =  0.38128387928009033\n",
            "Batch Training Loss =  0.2960904836654663\n",
            "Batch Training Loss =  0.5910724997520447\n",
            "Batch Training Loss =  0.38955461978912354\n",
            "Batch Training Loss =  0.4229601323604584\n",
            "Batch Training Loss =  0.3906063437461853\n",
            "Batch Training Loss =  0.31700944900512695\n",
            "Batch Training Loss =  0.36089783906936646\n",
            "Batch Training Loss =  0.3161686956882477\n",
            "Batch Training Loss =  0.39583781361579895\n",
            "Batch Training Loss =  0.3486359119415283\n",
            "Batch Training Loss =  0.31607234477996826\n",
            "Batch Training Loss =  0.2659154534339905\n",
            "Validation Loss in this epoch is 0.348\n",
            "This is  76 th epoch\n",
            "Batch Training Loss =  0.34919601678848267\n",
            "Batch Training Loss =  0.4133005440235138\n",
            "Batch Training Loss =  0.7486817240715027\n",
            "Batch Training Loss =  0.5651103258132935\n",
            "Batch Training Loss =  0.5397114753723145\n",
            "Batch Training Loss =  0.42522725462913513\n",
            "Batch Training Loss =  0.49348363280296326\n",
            "Batch Training Loss =  0.49155086278915405\n",
            "Batch Training Loss =  0.42773282527923584\n",
            "Batch Training Loss =  0.5184646248817444\n",
            "Batch Training Loss =  0.32686302065849304\n",
            "Batch Training Loss =  0.37692099809646606\n",
            "Batch Training Loss =  0.6697062849998474\n",
            "Batch Training Loss =  0.9283316135406494\n",
            "Batch Training Loss =  0.5952790975570679\n",
            "Batch Training Loss =  0.5121808052062988\n",
            "Batch Training Loss =  0.43445637822151184\n",
            "Batch Training Loss =  0.42537420988082886\n",
            "Batch Training Loss =  0.3415343463420868\n",
            "Batch Training Loss =  0.42860233783721924\n",
            "Batch Training Loss =  0.3335496485233307\n",
            "Batch Training Loss =  0.374225378036499\n",
            "Batch Training Loss =  0.3929879665374756\n",
            "Batch Training Loss =  0.34392136335372925\n",
            "Batch Training Loss =  0.30044230818748474\n",
            "Batch Training Loss =  0.24438542127609253\n",
            "Batch Training Loss =  0.3881394565105438\n",
            "Batch Training Loss =  0.5594673752784729\n",
            "Batch Training Loss =  0.30853530764579773\n",
            "Batch Training Loss =  0.3987180292606354\n",
            "Batch Training Loss =  0.37517592310905457\n",
            "Batch Training Loss =  0.30075380206108093\n",
            "Validation Loss in this epoch is 0.376\n",
            "This is  77 th epoch\n",
            "Batch Training Loss =  0.3251952826976776\n",
            "Batch Training Loss =  0.367206335067749\n",
            "Batch Training Loss =  0.20843464136123657\n",
            "Batch Training Loss =  0.3040865659713745\n",
            "Batch Training Loss =  0.5832720994949341\n",
            "Batch Training Loss =  0.4901924431324005\n",
            "Batch Training Loss =  0.4355192184448242\n",
            "Batch Training Loss =  0.36244502663612366\n",
            "Batch Training Loss =  0.26255494356155396\n",
            "Batch Training Loss =  0.2995247542858124\n",
            "Batch Training Loss =  0.29691627621650696\n",
            "Batch Training Loss =  0.5536719560623169\n",
            "Batch Training Loss =  0.2607819736003876\n",
            "Batch Training Loss =  0.22736701369285583\n",
            "Batch Training Loss =  0.24745507538318634\n",
            "Batch Training Loss =  0.5617423057556152\n",
            "Batch Training Loss =  1.211565613746643\n",
            "Batch Training Loss =  0.4241565763950348\n",
            "Batch Training Loss =  0.4292660653591156\n",
            "Batch Training Loss =  0.4350060522556305\n",
            "Batch Training Loss =  0.39923083782196045\n",
            "Batch Training Loss =  0.37544792890548706\n",
            "Batch Training Loss =  0.3888159692287445\n",
            "Batch Training Loss =  0.3593790531158447\n",
            "Batch Training Loss =  0.35485440492630005\n",
            "Batch Training Loss =  0.34252727031707764\n",
            "Batch Training Loss =  0.4451470971107483\n",
            "Batch Training Loss =  0.47817596793174744\n",
            "Batch Training Loss =  0.5487444400787354\n",
            "Batch Training Loss =  0.4971310496330261\n",
            "Batch Training Loss =  0.45025119185447693\n",
            "Batch Training Loss =  0.5010515451431274\n",
            "Validation Loss in this epoch is 0.597\n",
            "This is  78 th epoch\n",
            "Batch Training Loss =  0.37074434757232666\n",
            "Batch Training Loss =  0.29875048995018005\n",
            "Batch Training Loss =  0.44715622067451477\n",
            "Batch Training Loss =  0.6118833422660828\n",
            "Batch Training Loss =  0.7596928477287292\n",
            "Batch Training Loss =  0.46642518043518066\n",
            "Batch Training Loss =  0.343749463558197\n",
            "Batch Training Loss =  0.31401151418685913\n",
            "Batch Training Loss =  0.43714919686317444\n",
            "Batch Training Loss =  0.3793469965457916\n",
            "Batch Training Loss =  0.4203093349933624\n",
            "Batch Training Loss =  0.26289671659469604\n",
            "Batch Training Loss =  0.3373226523399353\n",
            "Batch Training Loss =  0.4635910391807556\n",
            "Batch Training Loss =  0.5271933674812317\n",
            "Batch Training Loss =  0.3797576427459717\n",
            "Batch Training Loss =  0.32052695751190186\n",
            "Batch Training Loss =  0.3267655670642853\n",
            "Batch Training Loss =  0.2961735129356384\n",
            "Batch Training Loss =  0.2605465054512024\n",
            "Batch Training Loss =  0.25204527378082275\n",
            "Batch Training Loss =  0.6929470896720886\n",
            "Batch Training Loss =  0.30828580260276794\n",
            "Batch Training Loss =  0.3972015678882599\n",
            "Batch Training Loss =  0.2533508837223053\n",
            "Batch Training Loss =  0.2542703449726105\n",
            "Batch Training Loss =  0.3379603624343872\n",
            "Batch Training Loss =  0.29735273122787476\n",
            "Batch Training Loss =  0.38993388414382935\n",
            "Batch Training Loss =  0.23069503903388977\n",
            "Batch Training Loss =  0.396546870470047\n",
            "Batch Training Loss =  0.36269474029541016\n",
            "Validation Loss in this epoch is 0.337\n",
            "This is  79 th epoch\n",
            "Batch Training Loss =  0.2555816173553467\n",
            "Batch Training Loss =  0.34682995080947876\n",
            "Batch Training Loss =  0.2785739600658417\n",
            "Batch Training Loss =  0.2753726840019226\n",
            "Batch Training Loss =  0.3360162079334259\n",
            "Batch Training Loss =  0.48292526602745056\n",
            "Batch Training Loss =  0.7801747918128967\n",
            "Batch Training Loss =  0.4249131679534912\n",
            "Batch Training Loss =  0.5034422278404236\n",
            "Batch Training Loss =  0.43585753440856934\n",
            "Batch Training Loss =  0.3760204017162323\n",
            "Batch Training Loss =  0.3165750205516815\n",
            "Batch Training Loss =  0.21233464777469635\n",
            "Batch Training Loss =  0.38559287786483765\n",
            "Batch Training Loss =  0.3324989378452301\n",
            "Batch Training Loss =  0.7261731624603271\n",
            "Batch Training Loss =  0.4095536470413208\n",
            "Batch Training Loss =  0.4098471701145172\n",
            "Batch Training Loss =  0.38097208738327026\n",
            "Batch Training Loss =  0.29882439970970154\n",
            "Batch Training Loss =  0.3168781101703644\n",
            "Batch Training Loss =  0.3022719621658325\n",
            "Batch Training Loss =  0.3206108510494232\n",
            "Batch Training Loss =  0.32656726241111755\n",
            "Batch Training Loss =  0.36249157786369324\n",
            "Batch Training Loss =  0.9999216198921204\n",
            "Batch Training Loss =  0.619586169719696\n",
            "Batch Training Loss =  0.6553298234939575\n",
            "Batch Training Loss =  0.538600742816925\n",
            "Batch Training Loss =  0.49598297476768494\n",
            "Batch Training Loss =  0.3860987424850464\n",
            "Batch Training Loss =  0.46221569180488586\n",
            "Validation Loss in this epoch is 0.453\n",
            "This is  80 th epoch\n",
            "Batch Training Loss =  0.35806259512901306\n",
            "Batch Training Loss =  0.3257022500038147\n",
            "Batch Training Loss =  0.3433902859687805\n",
            "Batch Training Loss =  0.41589000821113586\n",
            "Batch Training Loss =  0.22698287665843964\n",
            "Batch Training Loss =  0.4386848211288452\n",
            "Batch Training Loss =  0.5046159625053406\n",
            "Batch Training Loss =  0.6627460718154907\n",
            "Batch Training Loss =  0.46469220519065857\n",
            "Batch Training Loss =  0.4048871099948883\n",
            "Batch Training Loss =  0.2616038918495178\n",
            "Batch Training Loss =  0.4611353576183319\n",
            "Batch Training Loss =  0.37467992305755615\n",
            "Batch Training Loss =  0.20241889357566833\n",
            "Batch Training Loss =  0.3756956458091736\n",
            "Batch Training Loss =  0.3378083407878876\n",
            "Batch Training Loss =  0.2829762399196625\n",
            "Batch Training Loss =  0.1528734266757965\n",
            "Batch Training Loss =  0.2783680260181427\n",
            "Batch Training Loss =  0.3688301742076874\n",
            "Batch Training Loss =  0.41870224475860596\n",
            "Batch Training Loss =  0.335656076669693\n",
            "Batch Training Loss =  0.4084595739841461\n",
            "Batch Training Loss =  0.3124958574771881\n",
            "Batch Training Loss =  0.2024375945329666\n",
            "Batch Training Loss =  0.3838177025318146\n",
            "Batch Training Loss =  0.4047921895980835\n",
            "Batch Training Loss =  0.2996610403060913\n",
            "Batch Training Loss =  0.31306666135787964\n",
            "Batch Training Loss =  0.47107017040252686\n",
            "Batch Training Loss =  0.29380351305007935\n",
            "Batch Training Loss =  0.3770277500152588\n",
            "Validation Loss in this epoch is 0.344\n",
            "This is  81 th epoch\n",
            "Batch Training Loss =  0.3243108093738556\n",
            "Batch Training Loss =  0.24274133145809174\n",
            "Batch Training Loss =  0.3331085443496704\n",
            "Batch Training Loss =  0.5007879734039307\n",
            "Batch Training Loss =  0.6815637946128845\n",
            "Batch Training Loss =  0.6568005084991455\n",
            "Batch Training Loss =  0.43963727355003357\n",
            "Batch Training Loss =  0.32121604681015015\n",
            "Batch Training Loss =  0.22602659463882446\n",
            "Batch Training Loss =  0.2116299271583557\n",
            "Batch Training Loss =  0.21067871153354645\n",
            "Batch Training Loss =  0.17726850509643555\n",
            "Batch Training Loss =  0.38184380531311035\n",
            "Batch Training Loss =  0.2511846423149109\n",
            "Batch Training Loss =  0.33202236890792847\n",
            "Batch Training Loss =  1.002817153930664\n",
            "Batch Training Loss =  1.1872780323028564\n",
            "Batch Training Loss =  0.6868202686309814\n",
            "Batch Training Loss =  0.6593462228775024\n",
            "Batch Training Loss =  0.6651387810707092\n",
            "Batch Training Loss =  0.6254589557647705\n",
            "Batch Training Loss =  0.5529487133026123\n",
            "Batch Training Loss =  0.5155236721038818\n",
            "Batch Training Loss =  0.37369751930236816\n",
            "Batch Training Loss =  0.3776840269565582\n",
            "Batch Training Loss =  0.3492593765258789\n",
            "Batch Training Loss =  0.3822311758995056\n",
            "Batch Training Loss =  0.4260066747665405\n",
            "Batch Training Loss =  0.29347851872444153\n",
            "Batch Training Loss =  0.3867599070072174\n",
            "Batch Training Loss =  0.5516178607940674\n",
            "Batch Training Loss =  0.7524021863937378\n",
            "Validation Loss in this epoch is 0.432\n",
            "This is  82 th epoch\n",
            "Batch Training Loss =  0.46337682008743286\n",
            "Batch Training Loss =  0.4204685688018799\n",
            "Batch Training Loss =  0.28614097833633423\n",
            "Batch Training Loss =  0.28983232378959656\n",
            "Batch Training Loss =  0.4397418200969696\n",
            "Batch Training Loss =  0.5829120874404907\n",
            "Batch Training Loss =  0.5950281023979187\n",
            "Batch Training Loss =  0.43903228640556335\n",
            "Batch Training Loss =  0.34083816409111023\n",
            "Batch Training Loss =  0.2526494264602661\n",
            "Batch Training Loss =  0.2527599632740021\n",
            "Batch Training Loss =  0.2734054625034332\n",
            "Batch Training Loss =  0.6197037696838379\n",
            "Batch Training Loss =  0.9511405229568481\n",
            "Batch Training Loss =  0.5136421322822571\n",
            "Batch Training Loss =  0.49331772327423096\n",
            "Batch Training Loss =  0.40861862897872925\n",
            "Batch Training Loss =  0.405681848526001\n",
            "Batch Training Loss =  0.437852680683136\n",
            "Batch Training Loss =  0.31574177742004395\n",
            "Batch Training Loss =  0.34306439757347107\n",
            "Batch Training Loss =  0.42495405673980713\n",
            "Batch Training Loss =  1.1607284545898438\n",
            "Batch Training Loss =  0.65309739112854\n",
            "Batch Training Loss =  0.6738849878311157\n",
            "Batch Training Loss =  0.5752131938934326\n",
            "Batch Training Loss =  0.5420671701431274\n",
            "Batch Training Loss =  0.5385425686836243\n",
            "Batch Training Loss =  0.43245455622673035\n",
            "Batch Training Loss =  0.3135926127433777\n",
            "Batch Training Loss =  0.3447604775428772\n",
            "Batch Training Loss =  0.2767627537250519\n",
            "Validation Loss in this epoch is 0.421\n",
            "This is  83 th epoch\n",
            "Batch Training Loss =  0.47602954506874084\n",
            "Batch Training Loss =  0.3787580728530884\n",
            "Batch Training Loss =  0.3046097755432129\n",
            "Batch Training Loss =  0.4877626299858093\n",
            "Batch Training Loss =  0.386667400598526\n",
            "Batch Training Loss =  0.3386869728565216\n",
            "Batch Training Loss =  0.3204531967639923\n",
            "Batch Training Loss =  0.24960418045520782\n",
            "Batch Training Loss =  0.39338168501853943\n",
            "Batch Training Loss =  0.7061284780502319\n",
            "Batch Training Loss =  0.4207547903060913\n",
            "Batch Training Loss =  0.3977077901363373\n",
            "Batch Training Loss =  0.37605589628219604\n",
            "Batch Training Loss =  0.33534497022628784\n",
            "Batch Training Loss =  0.25365734100341797\n",
            "Batch Training Loss =  0.23005710542201996\n",
            "Batch Training Loss =  0.583218514919281\n",
            "Batch Training Loss =  1.0131398439407349\n",
            "Batch Training Loss =  0.4665718376636505\n",
            "Batch Training Loss =  0.5007598996162415\n",
            "Batch Training Loss =  0.5250942707061768\n",
            "Batch Training Loss =  0.4670753479003906\n",
            "Batch Training Loss =  0.5152719020843506\n",
            "Batch Training Loss =  0.4313000738620758\n",
            "Batch Training Loss =  0.36731404066085815\n",
            "Batch Training Loss =  0.3453214764595032\n",
            "Batch Training Loss =  0.3823849558830261\n",
            "Batch Training Loss =  0.27905014157295227\n",
            "Batch Training Loss =  0.36768439412117004\n",
            "Batch Training Loss =  0.2841276228427887\n",
            "Batch Training Loss =  0.3516196012496948\n",
            "Batch Training Loss =  0.31195926666259766\n",
            "Validation Loss in this epoch is 0.348\n",
            "This is  84 th epoch\n",
            "Batch Training Loss =  0.31547996401786804\n",
            "Batch Training Loss =  0.34220942854881287\n",
            "Batch Training Loss =  0.45012879371643066\n",
            "Batch Training Loss =  0.3115553557872772\n",
            "Batch Training Loss =  0.2956922650337219\n",
            "Batch Training Loss =  0.24417084455490112\n",
            "Batch Training Loss =  0.42211562395095825\n",
            "Batch Training Loss =  0.42843106389045715\n",
            "Batch Training Loss =  0.40677720308303833\n",
            "Batch Training Loss =  0.38850125670433044\n",
            "Batch Training Loss =  0.46732497215270996\n",
            "Batch Training Loss =  0.3627893030643463\n",
            "Batch Training Loss =  0.29126110672950745\n",
            "Batch Training Loss =  0.2589399218559265\n",
            "Batch Training Loss =  0.23528487980365753\n",
            "Batch Training Loss =  0.24061447381973267\n",
            "Batch Training Loss =  0.21355316042900085\n",
            "Batch Training Loss =  0.4118388295173645\n",
            "Batch Training Loss =  0.2783389389514923\n",
            "Batch Training Loss =  0.3724653124809265\n",
            "Batch Training Loss =  0.41943198442459106\n",
            "Batch Training Loss =  0.3063645362854004\n",
            "Batch Training Loss =  0.29736247658729553\n",
            "Batch Training Loss =  0.43660831451416016\n",
            "Batch Training Loss =  0.36945900321006775\n",
            "Batch Training Loss =  0.325215607881546\n",
            "Batch Training Loss =  0.23900726437568665\n",
            "Batch Training Loss =  0.31802332401275635\n",
            "Batch Training Loss =  0.43613961338996887\n",
            "Batch Training Loss =  0.4367588758468628\n",
            "Batch Training Loss =  0.5523192882537842\n",
            "Batch Training Loss =  0.5375902652740479\n",
            "Validation Loss in this epoch is 0.410\n",
            "This is  85 th epoch\n",
            "Batch Training Loss =  0.4151463806629181\n",
            "Batch Training Loss =  0.4141373336315155\n",
            "Batch Training Loss =  0.3251526951789856\n",
            "Batch Training Loss =  0.25347059965133667\n",
            "Batch Training Loss =  0.260841965675354\n",
            "Batch Training Loss =  0.4714019000530243\n",
            "Batch Training Loss =  0.34826141595840454\n",
            "Batch Training Loss =  0.32775813341140747\n",
            "Batch Training Loss =  0.22942176461219788\n",
            "Batch Training Loss =  0.3312208652496338\n",
            "Batch Training Loss =  0.4178221523761749\n",
            "Batch Training Loss =  0.29661446809768677\n",
            "Batch Training Loss =  0.2727513909339905\n",
            "Batch Training Loss =  0.24319396913051605\n",
            "Batch Training Loss =  0.3621465563774109\n",
            "Batch Training Loss =  0.32648563385009766\n",
            "Batch Training Loss =  0.25247111916542053\n",
            "Batch Training Loss =  0.34854888916015625\n",
            "Batch Training Loss =  0.9017152190208435\n",
            "Batch Training Loss =  0.8147847652435303\n",
            "Batch Training Loss =  0.594205915927887\n",
            "Batch Training Loss =  0.5182281732559204\n",
            "Batch Training Loss =  0.5142972469329834\n",
            "Batch Training Loss =  0.3480619788169861\n",
            "Batch Training Loss =  0.2475532591342926\n",
            "Batch Training Loss =  0.21556995809078217\n",
            "Batch Training Loss =  0.340745210647583\n",
            "Batch Training Loss =  0.23974643647670746\n",
            "Batch Training Loss =  0.2494247555732727\n",
            "Batch Training Loss =  0.21207304298877716\n",
            "Batch Training Loss =  0.1952066272497177\n",
            "Batch Training Loss =  0.6276404857635498\n",
            "Validation Loss in this epoch is 0.809\n",
            "This is  86 th epoch\n",
            "Batch Training Loss =  0.8476568460464478\n",
            "Batch Training Loss =  0.601128339767456\n",
            "Batch Training Loss =  0.5178272724151611\n",
            "Batch Training Loss =  0.38027453422546387\n",
            "Batch Training Loss =  0.4857068359851837\n",
            "Batch Training Loss =  0.2994794547557831\n",
            "Batch Training Loss =  0.19606521725654602\n",
            "Batch Training Loss =  0.30931368470191956\n",
            "Batch Training Loss =  0.2054998278617859\n",
            "Batch Training Loss =  0.3260541558265686\n",
            "Batch Training Loss =  0.31706225872039795\n",
            "Batch Training Loss =  0.4217644929885864\n",
            "Batch Training Loss =  0.3397798538208008\n",
            "Batch Training Loss =  0.3459922671318054\n",
            "Batch Training Loss =  0.5523518323898315\n",
            "Batch Training Loss =  0.44294679164886475\n",
            "Batch Training Loss =  0.36263003945350647\n",
            "Batch Training Loss =  0.34110260009765625\n",
            "Batch Training Loss =  0.4110512435436249\n",
            "Batch Training Loss =  0.3134879469871521\n",
            "Batch Training Loss =  0.28338396549224854\n",
            "Batch Training Loss =  0.20883861184120178\n",
            "Batch Training Loss =  0.2700764238834381\n",
            "Batch Training Loss =  0.339049369096756\n",
            "Batch Training Loss =  0.3395344018936157\n",
            "Batch Training Loss =  0.2565806806087494\n",
            "Batch Training Loss =  0.4664349853992462\n",
            "Batch Training Loss =  0.38729941844940186\n",
            "Batch Training Loss =  0.3305400013923645\n",
            "Batch Training Loss =  0.30652767419815063\n",
            "Batch Training Loss =  0.48053812980651855\n",
            "Batch Training Loss =  0.36516472697257996\n",
            "Validation Loss in this epoch is 0.357\n",
            "This is  87 th epoch\n",
            "Batch Training Loss =  0.2803966999053955\n",
            "Batch Training Loss =  0.24157367646694183\n",
            "Batch Training Loss =  0.39784255623817444\n",
            "Batch Training Loss =  0.36110132932662964\n",
            "Batch Training Loss =  0.25629723072052\n",
            "Batch Training Loss =  0.32999932765960693\n",
            "Batch Training Loss =  0.21603895723819733\n",
            "Batch Training Loss =  0.3351902365684509\n",
            "Batch Training Loss =  0.45412999391555786\n",
            "Batch Training Loss =  0.5912229418754578\n",
            "Batch Training Loss =  0.44964316487312317\n",
            "Batch Training Loss =  0.32477033138275146\n",
            "Batch Training Loss =  0.2509092390537262\n",
            "Batch Training Loss =  0.5058988332748413\n",
            "Batch Training Loss =  0.36609768867492676\n",
            "Batch Training Loss =  0.26756221055984497\n",
            "Batch Training Loss =  0.46380093693733215\n",
            "Batch Training Loss =  0.5557429790496826\n",
            "Batch Training Loss =  0.37332332134246826\n",
            "Batch Training Loss =  0.3353620767593384\n",
            "Batch Training Loss =  0.31809327006340027\n",
            "Batch Training Loss =  0.20623888075351715\n",
            "Batch Training Loss =  0.43748342990875244\n",
            "Batch Training Loss =  1.0017800331115723\n",
            "Batch Training Loss =  0.9626827239990234\n",
            "Batch Training Loss =  0.6480278968811035\n",
            "Batch Training Loss =  0.6299131512641907\n",
            "Batch Training Loss =  0.5935860872268677\n",
            "Batch Training Loss =  0.5668543577194214\n",
            "Batch Training Loss =  0.5562583804130554\n",
            "Batch Training Loss =  0.5675228238105774\n",
            "Batch Training Loss =  0.44738146662712097\n",
            "Validation Loss in this epoch is 0.404\n",
            "This is  88 th epoch\n",
            "Batch Training Loss =  0.43390658497810364\n",
            "Batch Training Loss =  0.28479287028312683\n",
            "Batch Training Loss =  0.19655637443065643\n",
            "Batch Training Loss =  0.3366442918777466\n",
            "Batch Training Loss =  0.23856912553310394\n",
            "Batch Training Loss =  0.33772650361061096\n",
            "Batch Training Loss =  0.43000298738479614\n",
            "Batch Training Loss =  0.5087224841117859\n",
            "Batch Training Loss =  0.6176629066467285\n",
            "Batch Training Loss =  0.3943832516670227\n",
            "Batch Training Loss =  0.28919699788093567\n",
            "Batch Training Loss =  0.43346694111824036\n",
            "Batch Training Loss =  0.5456483960151672\n",
            "Batch Training Loss =  0.475063294172287\n",
            "Batch Training Loss =  0.36440321803092957\n",
            "Batch Training Loss =  0.3685106933116913\n",
            "Batch Training Loss =  0.43835943937301636\n",
            "Batch Training Loss =  0.2420811802148819\n",
            "Batch Training Loss =  0.2952684462070465\n",
            "Batch Training Loss =  0.43542808294296265\n",
            "Batch Training Loss =  0.3019896447658539\n",
            "Batch Training Loss =  0.3055458962917328\n",
            "Batch Training Loss =  0.3902178704738617\n",
            "Batch Training Loss =  0.4562956690788269\n",
            "Batch Training Loss =  0.36002981662750244\n",
            "Batch Training Loss =  0.3523424565792084\n",
            "Batch Training Loss =  0.26671940088272095\n",
            "Batch Training Loss =  0.3828621804714203\n",
            "Batch Training Loss =  0.31460633873939514\n",
            "Batch Training Loss =  0.2105552852153778\n",
            "Batch Training Loss =  0.5213028192520142\n",
            "Batch Training Loss =  0.34424930810928345\n",
            "Validation Loss in this epoch is 0.348\n",
            "This is  89 th epoch\n",
            "Batch Training Loss =  0.42391616106033325\n",
            "Batch Training Loss =  0.31051957607269287\n",
            "Batch Training Loss =  0.6643276214599609\n",
            "Batch Training Loss =  1.9514083862304688\n",
            "Batch Training Loss =  0.6368524432182312\n",
            "Batch Training Loss =  0.5763953328132629\n",
            "Batch Training Loss =  0.5171380043029785\n",
            "Batch Training Loss =  0.4761478900909424\n",
            "Batch Training Loss =  0.596085786819458\n",
            "Batch Training Loss =  0.5240139961242676\n",
            "Batch Training Loss =  0.40649810433387756\n",
            "Batch Training Loss =  0.5421813726425171\n",
            "Batch Training Loss =  0.6047092080116272\n",
            "Batch Training Loss =  0.5115059614181519\n",
            "Batch Training Loss =  0.3945561349391937\n",
            "Batch Training Loss =  0.49448636174201965\n",
            "Batch Training Loss =  0.513207197189331\n",
            "Batch Training Loss =  0.4218926727771759\n",
            "Batch Training Loss =  0.39953452348709106\n",
            "Batch Training Loss =  0.38157111406326294\n",
            "Batch Training Loss =  0.3607097864151001\n",
            "Batch Training Loss =  0.3336790204048157\n",
            "Batch Training Loss =  0.4144650995731354\n",
            "Batch Training Loss =  0.5535649061203003\n",
            "Batch Training Loss =  0.6175229549407959\n",
            "Batch Training Loss =  0.589066207408905\n",
            "Batch Training Loss =  0.4133756160736084\n",
            "Batch Training Loss =  0.41461846232414246\n",
            "Batch Training Loss =  0.4640212953090668\n",
            "Batch Training Loss =  0.4891992211341858\n",
            "Batch Training Loss =  0.40402427315711975\n",
            "Batch Training Loss =  0.39025944471359253\n",
            "Validation Loss in this epoch is 0.441\n",
            "This is  90 th epoch\n",
            "Batch Training Loss =  0.4674874544143677\n",
            "Batch Training Loss =  0.6106365323066711\n",
            "Batch Training Loss =  0.47396427392959595\n",
            "Batch Training Loss =  0.4709431827068329\n",
            "Batch Training Loss =  0.6311086416244507\n",
            "Batch Training Loss =  0.47175711393356323\n",
            "Batch Training Loss =  0.5172722339630127\n",
            "Batch Training Loss =  0.7374122142791748\n",
            "Batch Training Loss =  0.7499275207519531\n",
            "Batch Training Loss =  0.5589288473129272\n",
            "Batch Training Loss =  0.3878895044326782\n",
            "Batch Training Loss =  0.4298587143421173\n",
            "Batch Training Loss =  0.485774427652359\n",
            "Batch Training Loss =  0.489957720041275\n",
            "Batch Training Loss =  0.5141385793685913\n",
            "Batch Training Loss =  0.3827124834060669\n",
            "Batch Training Loss =  0.4606858491897583\n",
            "Batch Training Loss =  0.647880494594574\n",
            "Batch Training Loss =  0.3274621069431305\n",
            "Batch Training Loss =  0.4364577531814575\n",
            "Batch Training Loss =  0.3785213530063629\n",
            "Batch Training Loss =  0.3673253357410431\n",
            "Batch Training Loss =  0.4883752763271332\n",
            "Batch Training Loss =  0.4882180094718933\n",
            "Batch Training Loss =  0.7744255065917969\n",
            "Batch Training Loss =  0.4985787272453308\n",
            "Batch Training Loss =  0.3222217261791229\n",
            "Batch Training Loss =  0.38616320490837097\n",
            "Batch Training Loss =  0.3110986053943634\n",
            "Batch Training Loss =  0.3493293821811676\n",
            "Batch Training Loss =  0.49414026737213135\n",
            "Batch Training Loss =  0.46563011407852173\n",
            "Validation Loss in this epoch is 0.676\n",
            "This is  91 th epoch\n",
            "Batch Training Loss =  0.9020881056785583\n",
            "Batch Training Loss =  0.5477778911590576\n",
            "Batch Training Loss =  0.37173786759376526\n",
            "Batch Training Loss =  0.447013258934021\n",
            "Batch Training Loss =  0.3458651602268219\n",
            "Batch Training Loss =  0.3911402225494385\n",
            "Batch Training Loss =  0.4776768982410431\n",
            "Batch Training Loss =  0.5052348375320435\n",
            "Batch Training Loss =  0.4283291697502136\n",
            "Batch Training Loss =  0.4063010513782501\n",
            "Batch Training Loss =  0.4661945700645447\n",
            "Batch Training Loss =  0.4707891345024109\n",
            "Batch Training Loss =  0.5436965823173523\n",
            "Batch Training Loss =  0.5690178871154785\n",
            "Batch Training Loss =  0.38283419609069824\n",
            "Batch Training Loss =  0.35298970341682434\n",
            "Batch Training Loss =  0.446930855512619\n",
            "Batch Training Loss =  0.38787394762039185\n",
            "Batch Training Loss =  0.34592679142951965\n",
            "Batch Training Loss =  0.5744847655296326\n",
            "Batch Training Loss =  0.7531081438064575\n",
            "Batch Training Loss =  0.49887171387672424\n",
            "Batch Training Loss =  0.40355750918388367\n",
            "Batch Training Loss =  0.4525466561317444\n",
            "Batch Training Loss =  0.24061726033687592\n",
            "Batch Training Loss =  0.34837105870246887\n",
            "Batch Training Loss =  0.38900643587112427\n",
            "Batch Training Loss =  0.38589465618133545\n",
            "Batch Training Loss =  0.42334091663360596\n",
            "Batch Training Loss =  0.772899329662323\n",
            "Batch Training Loss =  0.7622162699699402\n",
            "Batch Training Loss =  0.8255482316017151\n",
            "Validation Loss in this epoch is 0.737\n",
            "This is  92 th epoch\n",
            "Batch Training Loss =  0.9386520385742188\n",
            "Batch Training Loss =  0.8296784162521362\n",
            "Batch Training Loss =  0.7326470017433167\n",
            "Batch Training Loss =  0.6959021091461182\n",
            "Batch Training Loss =  0.7165191173553467\n",
            "Batch Training Loss =  0.7194835543632507\n",
            "Batch Training Loss =  0.6825580596923828\n",
            "Batch Training Loss =  0.6309196352958679\n",
            "Batch Training Loss =  0.5652878284454346\n",
            "Batch Training Loss =  0.4859454035758972\n",
            "Batch Training Loss =  0.46720269322395325\n",
            "Batch Training Loss =  0.36730149388313293\n",
            "Batch Training Loss =  0.32565584778785706\n",
            "Batch Training Loss =  0.4268592596054077\n",
            "Batch Training Loss =  0.40341195464134216\n",
            "Batch Training Loss =  0.269057959318161\n",
            "Batch Training Loss =  0.3186773955821991\n",
            "Batch Training Loss =  0.29531189799308777\n",
            "Batch Training Loss =  0.2688923478126526\n",
            "Batch Training Loss =  0.45647019147872925\n",
            "Batch Training Loss =  1.6830885410308838\n",
            "Batch Training Loss =  0.8678739666938782\n",
            "Batch Training Loss =  0.7010124325752258\n",
            "Batch Training Loss =  0.6807169914245605\n",
            "Batch Training Loss =  0.7545477151870728\n",
            "Batch Training Loss =  0.7432090044021606\n",
            "Batch Training Loss =  0.632112443447113\n",
            "Batch Training Loss =  0.7590324878692627\n",
            "Batch Training Loss =  0.6739807724952698\n",
            "Batch Training Loss =  0.7003031373023987\n",
            "Batch Training Loss =  0.661083459854126\n",
            "Batch Training Loss =  0.6412296295166016\n",
            "Validation Loss in this epoch is 0.643\n",
            "This is  93 th epoch\n",
            "Batch Training Loss =  0.7187093496322632\n",
            "Batch Training Loss =  0.7002877593040466\n",
            "Batch Training Loss =  0.6807945370674133\n",
            "Batch Training Loss =  0.6528835892677307\n",
            "Batch Training Loss =  0.6174461841583252\n",
            "Batch Training Loss =  0.6595028042793274\n",
            "Batch Training Loss =  0.5903178453445435\n",
            "Batch Training Loss =  0.5798951983451843\n",
            "Batch Training Loss =  0.7185330986976624\n",
            "Batch Training Loss =  0.6832021474838257\n",
            "Batch Training Loss =  0.6625366806983948\n",
            "Batch Training Loss =  0.6829099059104919\n",
            "Batch Training Loss =  0.6689221262931824\n",
            "Batch Training Loss =  0.621652364730835\n",
            "Batch Training Loss =  0.616533100605011\n",
            "Batch Training Loss =  0.6164236664772034\n",
            "Batch Training Loss =  0.6447076201438904\n",
            "Batch Training Loss =  0.6334132552146912\n",
            "Batch Training Loss =  0.6362847089767456\n",
            "Batch Training Loss =  0.6660998463630676\n",
            "Batch Training Loss =  0.6198444962501526\n",
            "Batch Training Loss =  0.6124593615531921\n",
            "Batch Training Loss =  0.6013069748878479\n",
            "Batch Training Loss =  0.5872924327850342\n",
            "Batch Training Loss =  0.5632980465888977\n",
            "Batch Training Loss =  0.5688623785972595\n",
            "Batch Training Loss =  0.5973485708236694\n",
            "Batch Training Loss =  0.5852558016777039\n",
            "Batch Training Loss =  0.5500059723854065\n",
            "Batch Training Loss =  0.5882706046104431\n",
            "Batch Training Loss =  0.6265343427658081\n",
            "Batch Training Loss =  0.5976510047912598\n",
            "Validation Loss in this epoch is 0.571\n",
            "This is  94 th epoch\n",
            "Batch Training Loss =  0.6178194284439087\n",
            "Batch Training Loss =  0.6522026062011719\n",
            "Batch Training Loss =  0.6658323407173157\n",
            "Batch Training Loss =  0.5722201466560364\n",
            "Batch Training Loss =  0.4963611364364624\n",
            "Batch Training Loss =  0.6556905508041382\n",
            "Batch Training Loss =  0.610017716884613\n",
            "Batch Training Loss =  0.6074851751327515\n",
            "Batch Training Loss =  0.6087836623191833\n",
            "Batch Training Loss =  0.5229329466819763\n",
            "Batch Training Loss =  0.5869429707527161\n",
            "Batch Training Loss =  0.8073054552078247\n",
            "Batch Training Loss =  0.6646841168403625\n",
            "Batch Training Loss =  0.67460036277771\n",
            "Batch Training Loss =  0.6912834048271179\n",
            "Batch Training Loss =  0.6569277048110962\n",
            "Batch Training Loss =  0.6481481790542603\n",
            "Batch Training Loss =  0.5870824456214905\n",
            "Batch Training Loss =  0.6243252754211426\n",
            "Batch Training Loss =  0.616106390953064\n",
            "Batch Training Loss =  0.6287402510643005\n",
            "Batch Training Loss =  0.6539450287818909\n",
            "Batch Training Loss =  0.5908752679824829\n",
            "Batch Training Loss =  0.6979498267173767\n",
            "Batch Training Loss =  0.6290265917778015\n",
            "Batch Training Loss =  0.5190514922142029\n",
            "Batch Training Loss =  0.5691862106323242\n",
            "Batch Training Loss =  0.5893490314483643\n",
            "Batch Training Loss =  0.5331854224205017\n",
            "Batch Training Loss =  0.45747411251068115\n",
            "Batch Training Loss =  0.5841391682624817\n",
            "Batch Training Loss =  0.6861381530761719\n",
            "Validation Loss in this epoch is 0.725\n",
            "This is  95 th epoch\n",
            "Batch Training Loss =  0.7122750282287598\n",
            "Batch Training Loss =  0.7076210975646973\n",
            "Batch Training Loss =  0.7417764067649841\n",
            "Batch Training Loss =  0.6750093102455139\n",
            "Batch Training Loss =  0.6519917845726013\n",
            "Batch Training Loss =  0.5232555866241455\n",
            "Batch Training Loss =  0.553825855255127\n",
            "Batch Training Loss =  0.5842990279197693\n",
            "Batch Training Loss =  0.4572664499282837\n",
            "Batch Training Loss =  0.4381638765335083\n",
            "Batch Training Loss =  0.6064655780792236\n",
            "Batch Training Loss =  0.635236382484436\n",
            "Batch Training Loss =  0.5767861604690552\n",
            "Batch Training Loss =  0.6264493465423584\n",
            "Batch Training Loss =  0.7517310380935669\n",
            "Batch Training Loss =  0.7528069019317627\n",
            "Batch Training Loss =  0.7106611728668213\n",
            "Batch Training Loss =  0.6686166524887085\n",
            "Batch Training Loss =  0.6038918495178223\n",
            "Batch Training Loss =  0.5110780596733093\n",
            "Batch Training Loss =  0.5730874538421631\n",
            "Batch Training Loss =  0.46440479159355164\n",
            "Batch Training Loss =  0.4597117304801941\n",
            "Batch Training Loss =  0.44891220331192017\n",
            "Batch Training Loss =  0.8616752624511719\n",
            "Batch Training Loss =  0.658341646194458\n",
            "Batch Training Loss =  0.7311959862709045\n",
            "Batch Training Loss =  0.7364020347595215\n",
            "Batch Training Loss =  0.6199255585670471\n",
            "Batch Training Loss =  0.6434524655342102\n",
            "Batch Training Loss =  0.6408549547195435\n",
            "Batch Training Loss =  0.5736867189407349\n",
            "Validation Loss in this epoch is 0.709\n",
            "This is  96 th epoch\n",
            "Batch Training Loss =  0.9243946671485901\n",
            "Batch Training Loss =  0.7217267751693726\n",
            "Batch Training Loss =  0.7943591475486755\n",
            "Batch Training Loss =  0.7026053071022034\n",
            "Batch Training Loss =  0.7301931977272034\n",
            "Batch Training Loss =  0.733954906463623\n",
            "Batch Training Loss =  0.7361166477203369\n",
            "Batch Training Loss =  0.7418087124824524\n",
            "Batch Training Loss =  0.6998549103736877\n",
            "Batch Training Loss =  0.6501483917236328\n",
            "Batch Training Loss =  0.705710768699646\n",
            "Batch Training Loss =  0.7267394065856934\n",
            "Batch Training Loss =  0.6766605973243713\n",
            "Batch Training Loss =  0.7044606804847717\n",
            "Batch Training Loss =  0.7080323100090027\n",
            "Batch Training Loss =  0.7082080841064453\n",
            "Batch Training Loss =  0.6971175670623779\n",
            "Batch Training Loss =  0.6874328851699829\n",
            "Batch Training Loss =  0.6635262370109558\n",
            "Batch Training Loss =  0.6541203260421753\n",
            "Batch Training Loss =  0.6712756156921387\n",
            "Batch Training Loss =  0.6798188090324402\n",
            "Batch Training Loss =  0.6340373754501343\n",
            "Batch Training Loss =  0.6258664131164551\n",
            "Batch Training Loss =  0.6184330582618713\n",
            "Batch Training Loss =  0.5956202745437622\n",
            "Batch Training Loss =  0.6084212064743042\n",
            "Batch Training Loss =  0.6178153157234192\n",
            "Batch Training Loss =  0.6039940118789673\n",
            "Batch Training Loss =  0.5441080331802368\n",
            "Batch Training Loss =  0.5652795433998108\n",
            "Batch Training Loss =  0.6018011569976807\n",
            "Validation Loss in this epoch is 0.573\n",
            "This is  97 th epoch\n",
            "Batch Training Loss =  0.5637729167938232\n",
            "Batch Training Loss =  0.7248736023902893\n",
            "Batch Training Loss =  0.7001957893371582\n",
            "Batch Training Loss =  0.7241992950439453\n",
            "Batch Training Loss =  0.6781401634216309\n",
            "Batch Training Loss =  0.6595007181167603\n",
            "Batch Training Loss =  0.667029857635498\n",
            "Batch Training Loss =  0.7369194626808167\n",
            "Batch Training Loss =  0.7093634605407715\n",
            "Batch Training Loss =  0.658237099647522\n",
            "Batch Training Loss =  0.6190250515937805\n",
            "Batch Training Loss =  0.6064302325248718\n",
            "Batch Training Loss =  0.5680873394012451\n",
            "Batch Training Loss =  0.7024038434028625\n",
            "Batch Training Loss =  0.6033008098602295\n",
            "Batch Training Loss =  0.533095121383667\n",
            "Batch Training Loss =  0.5338162779808044\n",
            "Batch Training Loss =  0.533536970615387\n",
            "Batch Training Loss =  0.5412742495536804\n",
            "Batch Training Loss =  0.5746738910675049\n",
            "Batch Training Loss =  0.4458168148994446\n",
            "Batch Training Loss =  0.44344019889831543\n",
            "Batch Training Loss =  1.3446346521377563\n",
            "Batch Training Loss =  0.7507501244544983\n",
            "Batch Training Loss =  0.740763247013092\n",
            "Batch Training Loss =  0.749087929725647\n",
            "Batch Training Loss =  0.7129935622215271\n",
            "Batch Training Loss =  0.7006518840789795\n",
            "Batch Training Loss =  0.6797968149185181\n",
            "Batch Training Loss =  0.6733112335205078\n",
            "Batch Training Loss =  0.6471141576766968\n",
            "Batch Training Loss =  0.6764885783195496\n",
            "Validation Loss in this epoch is 0.646\n",
            "This is  98 th epoch\n",
            "Batch Training Loss =  0.6547443866729736\n",
            "Batch Training Loss =  0.630202054977417\n",
            "Batch Training Loss =  0.6174132823944092\n",
            "Batch Training Loss =  0.6061765551567078\n",
            "Batch Training Loss =  0.4935799539089203\n",
            "Batch Training Loss =  0.566384494304657\n",
            "Batch Training Loss =  0.7179060578346252\n",
            "Batch Training Loss =  0.5243226289749146\n",
            "Batch Training Loss =  0.4877251386642456\n",
            "Batch Training Loss =  0.48380768299102783\n",
            "Batch Training Loss =  0.7186328768730164\n",
            "Batch Training Loss =  1.250697135925293\n",
            "Batch Training Loss =  0.69334876537323\n",
            "Batch Training Loss =  0.7027370929718018\n",
            "Batch Training Loss =  0.6864321827888489\n",
            "Batch Training Loss =  0.7009178996086121\n",
            "Batch Training Loss =  0.6239190697669983\n",
            "Batch Training Loss =  0.7171703577041626\n",
            "Batch Training Loss =  0.6508410573005676\n",
            "Batch Training Loss =  0.6667850017547607\n",
            "Batch Training Loss =  0.6406640410423279\n",
            "Batch Training Loss =  0.6259772181510925\n",
            "Batch Training Loss =  0.613685667514801\n",
            "Batch Training Loss =  0.6503231525421143\n",
            "Batch Training Loss =  0.6100780367851257\n",
            "Batch Training Loss =  0.5439584255218506\n",
            "Batch Training Loss =  0.5921409726142883\n",
            "Batch Training Loss =  0.6007744073867798\n",
            "Batch Training Loss =  0.5881993770599365\n",
            "Batch Training Loss =  0.6264786124229431\n",
            "Batch Training Loss =  0.6883029341697693\n",
            "Batch Training Loss =  0.6592017412185669\n",
            "Validation Loss in this epoch is 0.590\n",
            "This is  99 th epoch\n",
            "Batch Training Loss =  0.5457251071929932\n",
            "Batch Training Loss =  0.6414732336997986\n",
            "Batch Training Loss =  0.6604255437850952\n",
            "Batch Training Loss =  0.6066209673881531\n",
            "Batch Training Loss =  0.5955243110656738\n",
            "Batch Training Loss =  0.7204647660255432\n",
            "Batch Training Loss =  0.7021250128746033\n",
            "Batch Training Loss =  0.6704415678977966\n",
            "Batch Training Loss =  0.6899519562721252\n",
            "Batch Training Loss =  0.6752021312713623\n",
            "Batch Training Loss =  0.5746174454689026\n",
            "Batch Training Loss =  0.49070248007774353\n",
            "Batch Training Loss =  0.5421087145805359\n",
            "Batch Training Loss =  0.6728476881980896\n",
            "Batch Training Loss =  0.5950181484222412\n",
            "Batch Training Loss =  0.5225121378898621\n",
            "Batch Training Loss =  0.6344121694564819\n",
            "Batch Training Loss =  0.5636580586433411\n",
            "Batch Training Loss =  0.5640941858291626\n",
            "Batch Training Loss =  0.6309738755226135\n",
            "Batch Training Loss =  0.5140681266784668\n",
            "Batch Training Loss =  0.558029294013977\n",
            "Batch Training Loss =  0.6536282896995544\n",
            "Batch Training Loss =  0.7148075103759766\n",
            "Batch Training Loss =  0.6719844937324524\n",
            "Batch Training Loss =  0.6932477355003357\n",
            "Batch Training Loss =  0.7132105827331543\n",
            "Batch Training Loss =  0.6793153882026672\n",
            "Batch Training Loss =  0.6961755156517029\n",
            "Batch Training Loss =  0.7203091979026794\n",
            "Batch Training Loss =  0.7289519309997559\n",
            "Batch Training Loss =  0.7046475410461426\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  100 th epoch\n",
            "Batch Training Loss =  0.7065678238868713\n",
            "Batch Training Loss =  0.6871288418769836\n",
            "Batch Training Loss =  0.6742317080497742\n",
            "Batch Training Loss =  0.7018100619316101\n",
            "Batch Training Loss =  0.728258490562439\n",
            "Batch Training Loss =  0.6937684416770935\n",
            "Batch Training Loss =  0.6977127194404602\n",
            "Batch Training Loss =  0.6874362230300903\n",
            "Batch Training Loss =  0.6847886443138123\n",
            "Batch Training Loss =  0.6983590722084045\n",
            "Batch Training Loss =  0.6896892786026001\n",
            "Batch Training Loss =  0.6802390813827515\n",
            "Batch Training Loss =  0.6958245635032654\n",
            "Batch Training Loss =  0.6946619153022766\n",
            "Batch Training Loss =  0.6840578317642212\n",
            "Batch Training Loss =  0.6993818879127502\n",
            "Batch Training Loss =  0.6999475955963135\n",
            "Batch Training Loss =  0.6916765570640564\n",
            "Batch Training Loss =  0.6834371089935303\n",
            "Batch Training Loss =  0.6951994299888611\n",
            "Batch Training Loss =  0.6986754536628723\n",
            "Batch Training Loss =  0.6855424642562866\n",
            "Batch Training Loss =  0.6812463402748108\n",
            "Batch Training Loss =  0.685178279876709\n",
            "Batch Training Loss =  0.6813223361968994\n",
            "Batch Training Loss =  0.6814746260643005\n",
            "Batch Training Loss =  0.679212212562561\n",
            "Batch Training Loss =  0.6738324761390686\n",
            "Batch Training Loss =  0.6812156438827515\n",
            "Batch Training Loss =  0.6689425706863403\n",
            "Batch Training Loss =  0.656914472579956\n",
            "Batch Training Loss =  0.6532936096191406\n",
            "Validation Loss in this epoch is 0.647\n"
          ]
        }
      ],
      "source": [
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "weight_decays = [0., 0.01]\n",
        "batch_size = 50\n",
        "n_epochs = 100\n",
        "n_folds = 5\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        val_accs = []  # store validation accuracy for each fold\n",
        "        train_accs = []  # store training accuracy for each fold\n",
        "\n",
        "        # TODO: iterate over folds, remember to use \"shuffle=True\", as datapoints are not shuffled\n",
        "\n",
        "        myKFold = KFold(n_splits = n_folds, shuffle = True, random_state = 0)\n",
        "\n",
        "        # TODO: Split data into train and validation\n",
        "        for trainIndex, valIndex in myKFold.split(X):\n",
        "\n",
        "                # TODO: Create data loaders to pass to training loop\n",
        "                XTrain, XVal = X[trainIndex], X[valIndex]\n",
        "                yTrain, yVal = y[trainIndex], y[valIndex]\n",
        "\n",
        "\n",
        "                trainLoader = DataLoader(TensorDataset(XTrain, yTrain), batch_size = batch_size,shuffle = True)\n",
        "                valLoader = DataLoader(TensorDataset(XVal, yVal), batch_size = len(XVal))\n",
        "\n",
        "                # TODO: Initialize model, criterion (Cross entropy loss), and optimizer (SGD with various hyperparameters)\n",
        "                model = MyMLP(1000, 2)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = torch.optim.SGD(model.parameters(), lr = lr, weight_decay=wd)\n",
        "\n",
        "                # Call your training function\n",
        "                train(model, trainLoader, valLoader, n_epochs, optimizer, criterion, verbose=True)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # TODO: Use the trained model to estimate train/val accuracy\n",
        "                    # (Hint: our model outputs logits, argmax is good to get the class prediction corresponding to max logit)\n",
        "                    yPredTrain = model(XTrain).argmax(dim = 1)\n",
        "                    yPredVal = model(XVal).argmax(dim = 1)\n",
        "\n",
        "                    train_acc = accuracy_score(yPredTrain, yTrain)\n",
        "                    train_accs.append(train_acc)\n",
        "\n",
        "                    val_acc = accuracy_score(yPredVal, yVal)\n",
        "                    val_accs.append(val_acc)\n",
        "\n",
        "        # For each hyper-parameter, I'm storing the parameter values and the mean and standard error of accuracy in a list in \"results\".\n",
        "        train_std, train_mean = torch.std_mean(torch.tensor(train_accs))\n",
        "        val_std, val_mean = torch.std_mean(torch.tensor(val_accs))\n",
        "        rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds\n",
        "        train_se, val_se = train_std / rootn, val_std / rootn\n",
        "        # Storing learning rate, weight decay value, train mean accuracy, standard error, val mean accuracy, standard error\n",
        "        results.append((lr, wd, train_mean.item(), train_se.item(), val_mean.item(), val_se.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "ohNPYQQwzp3i",
        "outputId": "6e4f6a57-8f0e-4b63-c8e1-6cd8063aa02f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training results\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007071067811865475,\n        \"min\": 0.0,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.871 +/- 0.002\",\n          \"0.869 +/- 0.004\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.924 +/- 0.014\",\n          \"0.917 +/- 0.017\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.719 +/- 0.045\",\n          \"0.871 +/- 0.007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "pivot_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-6513fe93-58f1-4d2b-81a0-0e09a6d2a693\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.869 +/- 0.004</td>\n",
              "      <td>0.917 +/- 0.017</td>\n",
              "      <td>0.871 +/- 0.007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>0.871 +/- 0.002</td>\n",
              "      <td>0.924 +/- 0.014</td>\n",
              "      <td>0.719 +/- 0.045</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6513fe93-58f1-4d2b-81a0-0e09a6d2a693')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6513fe93-58f1-4d2b-81a0-0e09a6d2a693 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6513fe93-58f1-4d2b-81a0-0e09a6d2a693');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-56b9914a-edc9-43ef-bdd3-8930f4cc115f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-56b9914a-edc9-43ef-bdd3-8930f4cc115f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-56b9914a-edc9-43ef-bdd3-8930f4cc115f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ef03fbb7-eb17-481c-8d5a-f1dcf17fb92c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ef03fbb7-eb17-481c-8d5a-f1dcf17fb92c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           0.869 +/- 0.004  0.917 +/- 0.017  0.871 +/- 0.007\n",
              "0.01           0.871 +/- 0.002  0.924 +/- 0.014  0.719 +/- 0.045"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation results\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007071067811865475,\n        \"min\": 0.0,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.856 +/- 0.007\",\n          \"0.855 +/- 0.004\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.886 +/- 0.008\",\n          \"0.875 +/- 0.015\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.719 +/- 0.043\",\n          \"0.854 +/- 0.009\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "pivot_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-25fc8786-dacf-4b3a-b0fd-3b84fe2fd281\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.855 +/- 0.004</td>\n",
              "      <td>0.875 +/- 0.015</td>\n",
              "      <td>0.854 +/- 0.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>0.856 +/- 0.007</td>\n",
              "      <td>0.886 +/- 0.008</td>\n",
              "      <td>0.719 +/- 0.043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25fc8786-dacf-4b3a-b0fd-3b84fe2fd281')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25fc8786-dacf-4b3a-b0fd-3b84fe2fd281 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25fc8786-dacf-4b3a-b0fd-3b84fe2fd281');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8d345eea-0205-4468-af8f-dba639cac160\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d345eea-0205-4468-af8f-dba639cac160')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8d345eea-0205-4468-af8f-dba639cac160 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b0b71b83-4440-48bf-ae4a-e7a4731959e7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b0b71b83-4440-48bf-ae4a-e7a4731959e7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           0.855 +/- 0.004  0.875 +/- 0.015  0.854 +/- 0.009\n",
              "0.01           0.856 +/- 0.007  0.886 +/- 0.008  0.719 +/- 0.043"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO [3 points]. Print the final result (should be no need to modify code)\n",
        "# You should be able to see a best train acc > 95% , and a best val acc > 80%\n",
        "\n",
        "# Create a DataFrame from the list of tuples, with labeled columns\n",
        "column_names = ['learning_rate', 'weight_decay', 'train_mean', 'train_se','val_mean', 'val_se']\n",
        "df = pd.DataFrame(results, columns=column_names)\n",
        "\n",
        "# Make pretty printable strings, with standard error bars\n",
        "df['train_output'] = df.apply(lambda row: f\"{row['train_mean']:.3f} +/- {row['train_se']:.3f}\", axis=1)\n",
        "df['val_output'] = df.apply(lambda row: f\"{row['val_mean']:.3f} +/- {row['val_se']:.3f}\", axis=1)\n",
        "\n",
        "print('Training results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='train_output')\n",
        "display(pivot_df)\n",
        "\n",
        "print('Validation results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='val_output')\n",
        "display(pivot_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRXlGUrJcHSg"
      },
      "source": [
        "***Using ResNet18 embeddings for MLP***\n",
        "\n",
        "As can be seen above, although training accuracies are lower than earlier, but the validation accuracies have improved. And the range of improvement is between 7% - 9% which is actually pretty good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEtd8LuMLrid"
      },
      "source": [
        "## Extra Credit 3 - Train using ResNet instead of MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC-EsrCf1fEL",
        "outputId": "c99941d1-5e0a-4089-9c7a-75d76be9f4ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Jul 10 22:51:25 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   74C    P0              33W /  70W |  14399MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az-o7LA-L9oc",
        "outputId": "c4ca134d-b407-48b3-be0b-2d497094db13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X before transformation was  torch.Size([2000, 3, 32, 32])\n",
            "Shape of X after transformation is  torch.Size([1000, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "resNetModel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained = True)\n",
        "\n",
        "X, y = torch.load('/content/drive/MyDrive/CS224-FunadamentalsOfMachineLearning/HW2-DeepFakeCatDetector/hw2_data.pt')\n",
        "print (\"Shape of X before transformation was \", X.shape)\n",
        "X = transformImgTensorForResnet(X)\n",
        "X = X[1000:,]\n",
        "print (\"Shape of X after transformation is \", X.shape)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    X = X.to('cuda')\n",
        "    resNetModel.to('cuda')\n",
        "    y = y.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnNJVN4y0cpJ",
        "outputId": "a9e503e5-2383-4878-cec9-7a87fbadc057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch Training Loss =  0.004150249529629946\n",
            "Batch Training Loss =  0.0038616019301116467\n",
            "Batch Training Loss =  0.004534918814897537\n",
            "Batch Training Loss =  0.003911461681127548\n",
            "Batch Training Loss =  0.004111708607524633\n",
            "Batch Training Loss =  0.004181941505521536\n",
            "Batch Training Loss =  0.003981165122240782\n",
            "Batch Training Loss =  0.00420107739046216\n",
            "Batch Training Loss =  0.003931490238755941\n",
            "Batch Training Loss =  0.004143808968365192\n",
            "Batch Training Loss =  0.0035789282992482185\n",
            "Batch Training Loss =  0.003943626303225756\n",
            "Batch Training Loss =  0.0036112547386437654\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.004152604844421148\n",
            "Batch Training Loss =  0.003898163791745901\n",
            "Batch Training Loss =  0.004022727720439434\n",
            "Batch Training Loss =  0.0037328845355659723\n",
            "Batch Training Loss =  0.0039606173522770405\n",
            "Batch Training Loss =  0.003829400287941098\n",
            "Batch Training Loss =  0.00378497876226902\n",
            "Batch Training Loss =  0.00410122238099575\n",
            "Batch Training Loss =  0.003758037928491831\n",
            "Batch Training Loss =  0.004006094299256802\n",
            "Batch Training Loss =  0.003808746114373207\n",
            "Batch Training Loss =  0.003994053229689598\n",
            "Batch Training Loss =  0.003725280985236168\n",
            "Batch Training Loss =  0.0038665086030960083\n",
            "Batch Training Loss =  0.0038580375257879496\n",
            "Batch Training Loss =  0.0036801763344556093\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.003698688466101885\n",
            "Batch Training Loss =  0.003765695495530963\n",
            "Batch Training Loss =  0.003635300090536475\n",
            "Batch Training Loss =  0.0037590963765978813\n",
            "Batch Training Loss =  0.003608032362535596\n",
            "Batch Training Loss =  0.003908178769052029\n",
            "Batch Training Loss =  0.0035557090304791927\n",
            "Batch Training Loss =  0.0038973542395979166\n",
            "Batch Training Loss =  0.00419926131144166\n",
            "Batch Training Loss =  0.003747657872736454\n",
            "Batch Training Loss =  0.0036495591048151255\n",
            "Batch Training Loss =  0.0041570719331502914\n",
            "Batch Training Loss =  0.003974482417106628\n",
            "Batch Training Loss =  0.0038493468891829252\n",
            "Batch Training Loss =  0.0035300725139677525\n",
            "Batch Training Loss =  0.003691778751090169\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0038834004662930965\n",
            "Batch Training Loss =  0.00397228030487895\n",
            "Batch Training Loss =  0.003612130880355835\n",
            "Batch Training Loss =  0.004078606143593788\n",
            "Batch Training Loss =  0.003443013411015272\n",
            "Batch Training Loss =  0.0038418611511588097\n",
            "Batch Training Loss =  0.003732798621058464\n",
            "Batch Training Loss =  0.00358049594797194\n",
            "Batch Training Loss =  0.003751823678612709\n",
            "Batch Training Loss =  0.003741747234016657\n",
            "Batch Training Loss =  0.0036615810822695494\n",
            "Batch Training Loss =  0.0033630114048719406\n",
            "Batch Training Loss =  0.0037650770973414183\n",
            "Batch Training Loss =  0.0037058955058455467\n",
            "Batch Training Loss =  0.0036606821231544018\n",
            "Batch Training Loss =  0.0035435522440820932\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0033893389627337456\n",
            "Batch Training Loss =  0.0037061041221022606\n",
            "Batch Training Loss =  0.0035706839989870787\n",
            "Batch Training Loss =  0.0032644697930663824\n",
            "Batch Training Loss =  0.0034324387088418007\n",
            "Batch Training Loss =  0.003726975293830037\n",
            "Batch Training Loss =  0.003427780233323574\n",
            "Batch Training Loss =  0.0041131251491606236\n",
            "Batch Training Loss =  0.0033139672596007586\n",
            "Batch Training Loss =  0.003893681103363633\n",
            "Batch Training Loss =  0.0033454359509050846\n",
            "Batch Training Loss =  0.00346619775518775\n",
            "Batch Training Loss =  0.0032265749759972095\n",
            "Batch Training Loss =  0.003976848907768726\n",
            "Batch Training Loss =  0.0034083020873367786\n",
            "Batch Training Loss =  0.0036893521901220083\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0036563295871019363\n",
            "Batch Training Loss =  0.0032691145315766335\n",
            "Batch Training Loss =  0.0035772197879850864\n",
            "Batch Training Loss =  0.003160976804792881\n",
            "Batch Training Loss =  0.0033371702302247286\n",
            "Batch Training Loss =  0.003434879705309868\n",
            "Batch Training Loss =  0.003435205901041627\n",
            "Batch Training Loss =  0.003353248815983534\n",
            "Batch Training Loss =  0.003866516286507249\n",
            "Batch Training Loss =  0.0035346781369298697\n",
            "Batch Training Loss =  0.0032655391842126846\n",
            "Batch Training Loss =  0.003170503070577979\n",
            "Batch Training Loss =  0.0032498668879270554\n",
            "Batch Training Loss =  0.003506647888571024\n",
            "Batch Training Loss =  0.003589050844311714\n",
            "Batch Training Loss =  0.003362631890922785\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.003664920339360833\n",
            "Batch Training Loss =  0.003522770944982767\n",
            "Batch Training Loss =  0.0032771388068795204\n",
            "Batch Training Loss =  0.0033464154694229364\n",
            "Batch Training Loss =  0.003271915949881077\n",
            "Batch Training Loss =  0.0035287661012262106\n",
            "Batch Training Loss =  0.0032866375986486673\n",
            "Batch Training Loss =  0.0034899278543889523\n",
            "Batch Training Loss =  0.003325808560475707\n",
            "Batch Training Loss =  0.003479385981336236\n",
            "Batch Training Loss =  0.0031123259104788303\n",
            "Batch Training Loss =  0.003314355621114373\n",
            "Batch Training Loss =  0.0033887254539877176\n",
            "Batch Training Loss =  0.003201655810698867\n",
            "Batch Training Loss =  0.0031967812683433294\n",
            "Batch Training Loss =  0.0033179318998008966\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.003337098052725196\n",
            "Batch Training Loss =  0.003274105256423354\n",
            "Batch Training Loss =  0.0033476685639470816\n",
            "Batch Training Loss =  0.0032640453428030014\n",
            "Batch Training Loss =  0.0033481589052826166\n",
            "Batch Training Loss =  0.003117130370810628\n",
            "Batch Training Loss =  0.003643212839961052\n",
            "Batch Training Loss =  0.003182534361258149\n",
            "Batch Training Loss =  0.003228308167308569\n",
            "Batch Training Loss =  0.003248471301048994\n",
            "Batch Training Loss =  0.003610198851674795\n",
            "Batch Training Loss =  0.0030583508778363466\n",
            "Batch Training Loss =  0.0030466958414763212\n",
            "Batch Training Loss =  0.0031562468502670527\n",
            "Batch Training Loss =  0.003045970806851983\n",
            "Batch Training Loss =  0.003267904743552208\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.003297302173450589\n",
            "Batch Training Loss =  0.0031749247573316097\n",
            "Batch Training Loss =  0.0033024444710463285\n",
            "Batch Training Loss =  0.0031164404936134815\n",
            "Batch Training Loss =  0.003227829933166504\n",
            "Batch Training Loss =  0.003364205127581954\n",
            "Batch Training Loss =  0.0034786416217684746\n",
            "Batch Training Loss =  0.0031741971615701914\n",
            "Batch Training Loss =  0.003104203613474965\n",
            "Batch Training Loss =  0.003053610911592841\n",
            "Batch Training Loss =  0.003272133180871606\n",
            "Batch Training Loss =  0.0029712205287069082\n",
            "Batch Training Loss =  0.0033029362093657255\n",
            "Batch Training Loss =  0.0031631686724722385\n",
            "Batch Training Loss =  0.003005352569743991\n",
            "Batch Training Loss =  0.003209856804460287\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0031980734784156084\n",
            "Batch Training Loss =  0.0030512369703501463\n",
            "Batch Training Loss =  0.0031012368854135275\n",
            "Batch Training Loss =  0.003266957588493824\n",
            "Batch Training Loss =  0.0030946857295930386\n",
            "Batch Training Loss =  0.0032696446869522333\n",
            "Batch Training Loss =  0.0028007226064801216\n",
            "Batch Training Loss =  0.0034331895876675844\n",
            "Batch Training Loss =  0.0032030586153268814\n",
            "Batch Training Loss =  0.0031027549412101507\n",
            "Batch Training Loss =  0.002802932169288397\n",
            "Batch Training Loss =  0.00318437279202044\n",
            "Batch Training Loss =  0.002886016620323062\n",
            "Batch Training Loss =  0.002994011854752898\n",
            "Batch Training Loss =  0.003403604030609131\n",
            "Batch Training Loss =  0.002965388586744666\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0030182485934346914\n",
            "Batch Training Loss =  0.0030647534877061844\n",
            "Batch Training Loss =  0.003158885519951582\n",
            "Batch Training Loss =  0.003176129423081875\n",
            "Batch Training Loss =  0.003061160445213318\n",
            "Batch Training Loss =  0.0029285335913300514\n",
            "Batch Training Loss =  0.0030654349830001593\n",
            "Batch Training Loss =  0.003010500455275178\n",
            "Batch Training Loss =  0.0032002634834498167\n",
            "Batch Training Loss =  0.0030220758635550737\n",
            "Batch Training Loss =  0.003110715188086033\n",
            "Batch Training Loss =  0.0029858567286282778\n",
            "Batch Training Loss =  0.00286777107976377\n",
            "Batch Training Loss =  0.0029847014229744673\n",
            "Batch Training Loss =  0.002897772006690502\n",
            "Batch Training Loss =  0.0028971850406378508\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.002912261290475726\n",
            "Batch Training Loss =  0.0031039335299283266\n",
            "Batch Training Loss =  0.003145261202007532\n",
            "Batch Training Loss =  0.0027511375956237316\n",
            "Batch Training Loss =  0.0030654678121209145\n",
            "Batch Training Loss =  0.0029713434632867575\n",
            "Batch Training Loss =  0.0031078646425157785\n",
            "Batch Training Loss =  0.002923048799857497\n",
            "Batch Training Loss =  0.003088470781221986\n",
            "Batch Training Loss =  0.0027706408873200417\n",
            "Batch Training Loss =  0.0029868842102587223\n",
            "Batch Training Loss =  0.0031199767254292965\n",
            "Batch Training Loss =  0.0027189524844288826\n",
            "Batch Training Loss =  0.002965761348605156\n",
            "Batch Training Loss =  0.0028379184659570456\n",
            "Batch Training Loss =  0.0028605139814317226\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0029660274740308523\n",
            "Batch Training Loss =  0.0030501370783895254\n",
            "Batch Training Loss =  0.0028878659941256046\n",
            "Batch Training Loss =  0.00266707013361156\n",
            "Batch Training Loss =  0.0029940807726234198\n",
            "Batch Training Loss =  0.003100304864346981\n",
            "Batch Training Loss =  0.0027013865765184164\n",
            "Batch Training Loss =  0.0030795179773122072\n",
            "Batch Training Loss =  0.002794921165332198\n",
            "Batch Training Loss =  0.002684482838958502\n",
            "Batch Training Loss =  0.0030715870670974255\n",
            "Batch Training Loss =  0.0031208836007863283\n",
            "Batch Training Loss =  0.002670606365427375\n",
            "Batch Training Loss =  0.00297001120634377\n",
            "Batch Training Loss =  0.002680382924154401\n",
            "Batch Training Loss =  0.0029708745423704386\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.002969993744045496\n",
            "Batch Training Loss =  0.0028220731765031815\n",
            "Batch Training Loss =  0.002913626143708825\n",
            "Batch Training Loss =  0.0028643107507377863\n",
            "Batch Training Loss =  0.002788715297356248\n",
            "Batch Training Loss =  0.002727288519963622\n",
            "Batch Training Loss =  0.002594749443233013\n",
            "Batch Training Loss =  0.002733920468017459\n",
            "Batch Training Loss =  0.0030452702194452286\n",
            "Batch Training Loss =  0.0026924191042780876\n",
            "Batch Training Loss =  0.002430042950436473\n",
            "Batch Training Loss =  0.0026758320163935423\n",
            "Batch Training Loss =  0.0027698546182364225\n",
            "Batch Training Loss =  0.0029750012326985598\n",
            "Batch Training Loss =  0.002831664402037859\n",
            "Batch Training Loss =  0.003088760655373335\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.002960611367598176\n",
            "Batch Training Loss =  0.0027401901315897703\n",
            "Batch Training Loss =  0.002665775828063488\n",
            "Batch Training Loss =  0.002861072076484561\n",
            "Batch Training Loss =  0.002604387467727065\n",
            "Batch Training Loss =  0.002650116104632616\n",
            "Batch Training Loss =  0.0025794869288802147\n",
            "Batch Training Loss =  0.002721291733905673\n",
            "Batch Training Loss =  0.002617122605443001\n",
            "Batch Training Loss =  0.0025964498054236174\n",
            "Batch Training Loss =  0.0032884832471609116\n",
            "Batch Training Loss =  0.002872432814911008\n",
            "Batch Training Loss =  0.0029401180800050497\n",
            "Batch Training Loss =  0.002742898417636752\n",
            "Batch Training Loss =  0.002610651310533285\n",
            "Batch Training Loss =  0.0027749142609536648\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0025732663925737143\n",
            "Batch Training Loss =  0.0025804415345191956\n",
            "Batch Training Loss =  0.002724721096456051\n",
            "Batch Training Loss =  0.002933966228738427\n",
            "Batch Training Loss =  0.0027048385236412287\n",
            "Batch Training Loss =  0.002791257109493017\n",
            "Batch Training Loss =  0.002650471869856119\n",
            "Batch Training Loss =  0.0026456075720489025\n",
            "Batch Training Loss =  0.002790805185213685\n",
            "Batch Training Loss =  0.002848646603524685\n",
            "Batch Training Loss =  0.0028245917055755854\n",
            "Batch Training Loss =  0.002592615783214569\n",
            "Batch Training Loss =  0.0025347708724439144\n",
            "Batch Training Loss =  0.002643910003826022\n",
            "Batch Training Loss =  0.0027065458707511425\n",
            "Batch Training Loss =  0.0025906201917678118\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0024882510770112276\n",
            "Batch Training Loss =  0.0024519257713109255\n",
            "Batch Training Loss =  0.0026938575319945812\n",
            "Batch Training Loss =  0.002696669427677989\n",
            "Batch Training Loss =  0.0026224125176668167\n",
            "Batch Training Loss =  0.0027499126736074686\n",
            "Batch Training Loss =  0.0025502743665128946\n",
            "Batch Training Loss =  0.0025442647747695446\n",
            "Batch Training Loss =  0.002641937229782343\n",
            "Batch Training Loss =  0.0027603546623140574\n",
            "Batch Training Loss =  0.0028312820941209793\n",
            "Batch Training Loss =  0.002503864234313369\n",
            "Batch Training Loss =  0.002588015515357256\n",
            "Batch Training Loss =  0.0026493368204683065\n",
            "Batch Training Loss =  0.002585415728390217\n",
            "Batch Training Loss =  0.002566160634160042\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.002456527901813388\n",
            "Batch Training Loss =  0.0025492452550679445\n",
            "Batch Training Loss =  0.0023528842721134424\n",
            "Batch Training Loss =  0.0026611769571900368\n",
            "Batch Training Loss =  0.002360014012083411\n",
            "Batch Training Loss =  0.00270354887470603\n",
            "Batch Training Loss =  0.002979898126795888\n",
            "Batch Training Loss =  0.0026681292802095413\n",
            "Batch Training Loss =  0.002408555243164301\n",
            "Batch Training Loss =  0.0024035198148339987\n",
            "Batch Training Loss =  0.002687462605535984\n",
            "Batch Training Loss =  0.002869217423722148\n",
            "Batch Training Loss =  0.0024530552327632904\n",
            "Batch Training Loss =  0.0026575636584311724\n",
            "Batch Training Loss =  0.0026969395112246275\n",
            "Batch Training Loss =  0.0024711538571864367\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0025461260229349136\n",
            "Batch Training Loss =  0.0023262994363904\n",
            "Batch Training Loss =  0.002814658684656024\n",
            "Batch Training Loss =  0.002387078944593668\n",
            "Batch Training Loss =  0.002377414610236883\n",
            "Batch Training Loss =  0.0025516063906252384\n",
            "Batch Training Loss =  0.0026021399535238743\n",
            "Batch Training Loss =  0.00268432404845953\n",
            "Batch Training Loss =  0.0026289699599146843\n",
            "Batch Training Loss =  0.0024472635705024004\n",
            "Batch Training Loss =  0.0025560918729752302\n",
            "Batch Training Loss =  0.0028162437956780195\n",
            "Batch Training Loss =  0.0024987896904349327\n",
            "Batch Training Loss =  0.0024107962381094694\n",
            "Batch Training Loss =  0.0026009792927652597\n",
            "Batch Training Loss =  0.002534868661314249\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0025464289356023073\n",
            "Batch Training Loss =  0.0025749034248292446\n",
            "Batch Training Loss =  0.0024839432444423437\n",
            "Batch Training Loss =  0.0025580411311239004\n",
            "Batch Training Loss =  0.002461985219269991\n",
            "Batch Training Loss =  0.002406017854809761\n",
            "Batch Training Loss =  0.0024900375865399837\n",
            "Batch Training Loss =  0.0025565579999238253\n",
            "Batch Training Loss =  0.0024594203568995\n",
            "Batch Training Loss =  0.0025065357331186533\n",
            "Batch Training Loss =  0.002364066196605563\n",
            "Batch Training Loss =  0.00270697264932096\n",
            "Batch Training Loss =  0.0023317818995565176\n",
            "Batch Training Loss =  0.00250942911952734\n",
            "Batch Training Loss =  0.0023159903939813375\n",
            "Batch Training Loss =  0.0026609953492879868\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0024956963025033474\n",
            "Batch Training Loss =  0.002367574954405427\n",
            "Batch Training Loss =  0.002502692397683859\n",
            "Batch Training Loss =  0.0023421263322234154\n",
            "Batch Training Loss =  0.002517229411751032\n",
            "Batch Training Loss =  0.0024066511541604996\n",
            "Batch Training Loss =  0.0023818928748369217\n",
            "Batch Training Loss =  0.0025487495586276054\n",
            "Batch Training Loss =  0.002375752432271838\n",
            "Batch Training Loss =  0.0024897176772356033\n",
            "Batch Training Loss =  0.0024662355426698923\n",
            "Batch Training Loss =  0.0021069725044071674\n",
            "Batch Training Loss =  0.0026159060653299093\n",
            "Batch Training Loss =  0.002321254927664995\n",
            "Batch Training Loss =  0.002381961327046156\n",
            "Batch Training Loss =  0.0023134583607316017\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0023906396236270666\n",
            "Batch Training Loss =  0.002200524089857936\n",
            "Batch Training Loss =  0.002366046654060483\n",
            "Batch Training Loss =  0.0026843855157494545\n",
            "Batch Training Loss =  0.0021944351028651\n",
            "Batch Training Loss =  0.0024107436183840036\n",
            "Batch Training Loss =  0.002626257250085473\n",
            "Batch Training Loss =  0.002312871627509594\n",
            "Batch Training Loss =  0.002309548668563366\n",
            "Batch Training Loss =  0.0024227346293628216\n",
            "Batch Training Loss =  0.0024090891238301992\n",
            "Batch Training Loss =  0.002492772415280342\n",
            "Batch Training Loss =  0.002181505784392357\n",
            "Batch Training Loss =  0.0026008940767496824\n",
            "Batch Training Loss =  0.0023232456296682358\n",
            "Batch Training Loss =  0.0023908717557787895\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0022616684436798096\n",
            "Batch Training Loss =  0.002425438491627574\n",
            "Batch Training Loss =  0.0021626814268529415\n",
            "Batch Training Loss =  0.0023569476325064898\n",
            "Batch Training Loss =  0.0021916059777140617\n",
            "Batch Training Loss =  0.0022393488325178623\n",
            "Batch Training Loss =  0.0025319335982203484\n",
            "Batch Training Loss =  0.0024041240103542805\n",
            "Batch Training Loss =  0.0023153137881308794\n",
            "Batch Training Loss =  0.002200630959123373\n",
            "Batch Training Loss =  0.0024241087958216667\n",
            "Batch Training Loss =  0.002711310749873519\n",
            "Batch Training Loss =  0.0022105693351477385\n",
            "Batch Training Loss =  0.0022298425901681185\n",
            "Batch Training Loss =  0.0024417941458523273\n",
            "Batch Training Loss =  0.0021789181046187878\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.002357675926759839\n",
            "Batch Training Loss =  0.0021247221156954765\n",
            "Batch Training Loss =  0.002242992166429758\n",
            "Batch Training Loss =  0.002190468367189169\n",
            "Batch Training Loss =  0.0024708041455596685\n",
            "Batch Training Loss =  0.0022358696442097425\n",
            "Batch Training Loss =  0.0023299697786569595\n",
            "Batch Training Loss =  0.0022709022741764784\n",
            "Batch Training Loss =  0.0022645331919193268\n",
            "Batch Training Loss =  0.0023429470602422953\n",
            "Batch Training Loss =  0.0021810787729918957\n",
            "Batch Training Loss =  0.002502826740965247\n",
            "Batch Training Loss =  0.002292411867529154\n",
            "Batch Training Loss =  0.002419780706986785\n",
            "Batch Training Loss =  0.002221335656940937\n",
            "Batch Training Loss =  0.0023720646277070045\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.002452553017064929\n",
            "Batch Training Loss =  0.0024681189097464085\n",
            "Batch Training Loss =  0.0022110275458544493\n",
            "Batch Training Loss =  0.0023656091652810574\n",
            "Batch Training Loss =  0.0021534895058721304\n",
            "Batch Training Loss =  0.0022938326001167297\n",
            "Batch Training Loss =  0.002180037321522832\n",
            "Batch Training Loss =  0.0022867033258080482\n",
            "Batch Training Loss =  0.002208438003435731\n",
            "Batch Training Loss =  0.0023115144576877356\n",
            "Batch Training Loss =  0.0023092732299119234\n",
            "Batch Training Loss =  0.002310326788574457\n",
            "Batch Training Loss =  0.002118878299370408\n",
            "Batch Training Loss =  0.002201314549893141\n",
            "Batch Training Loss =  0.0021482245065271854\n",
            "Batch Training Loss =  0.002218436449766159\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0022218460217118263\n",
            "Batch Training Loss =  0.0021398502867668867\n",
            "Batch Training Loss =  0.0022992659360170364\n",
            "Batch Training Loss =  0.0023932342883199453\n",
            "Batch Training Loss =  0.0023053816985338926\n",
            "Batch Training Loss =  0.0021205260418355465\n",
            "Batch Training Loss =  0.002179184928536415\n",
            "Batch Training Loss =  0.0020455019548535347\n",
            "Batch Training Loss =  0.002366353292018175\n",
            "Batch Training Loss =  0.002046985784545541\n",
            "Batch Training Loss =  0.0023009430151432753\n",
            "Batch Training Loss =  0.002162144985049963\n",
            "Batch Training Loss =  0.0022331876680254936\n",
            "Batch Training Loss =  0.0020290822722017765\n",
            "Batch Training Loss =  0.0022940419148653746\n",
            "Batch Training Loss =  0.0022968892008066177\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0021817521192133427\n",
            "Batch Training Loss =  0.002122241072356701\n",
            "Batch Training Loss =  0.0023546265438199043\n",
            "Batch Training Loss =  0.002162996446713805\n",
            "Batch Training Loss =  0.0021608525421470404\n",
            "Batch Training Loss =  0.0020772682037204504\n",
            "Batch Training Loss =  0.0019958042539656162\n",
            "Batch Training Loss =  0.002289482858031988\n",
            "Batch Training Loss =  0.0021049126517027617\n",
            "Batch Training Loss =  0.002062119310721755\n",
            "Batch Training Loss =  0.0022595003247261047\n",
            "Batch Training Loss =  0.0023107656743377447\n",
            "Batch Training Loss =  0.0021076614502817392\n",
            "Batch Training Loss =  0.002198325004428625\n",
            "Batch Training Loss =  0.002114236820489168\n",
            "Batch Training Loss =  0.0022594761103391647\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.002239363035187125\n",
            "Batch Training Loss =  0.002326963236555457\n",
            "Batch Training Loss =  0.0023072243202477694\n",
            "Batch Training Loss =  0.002165229059755802\n",
            "Batch Training Loss =  0.0021731415763497353\n",
            "Batch Training Loss =  0.002338741673156619\n",
            "Batch Training Loss =  0.0021286732517182827\n",
            "Batch Training Loss =  0.00203905813395977\n",
            "Batch Training Loss =  0.0020279965829104185\n",
            "Batch Training Loss =  0.0019809294026345015\n",
            "Batch Training Loss =  0.002124484395608306\n",
            "Batch Training Loss =  0.0020612177904695272\n",
            "Batch Training Loss =  0.0021249176934361458\n",
            "Batch Training Loss =  0.0021552403923124075\n",
            "Batch Training Loss =  0.0019685730803757906\n",
            "Batch Training Loss =  0.0021609023679047823\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.002068981295451522\n",
            "Batch Training Loss =  0.0022348640486598015\n",
            "Batch Training Loss =  0.002294735284522176\n",
            "Batch Training Loss =  0.002055272227153182\n",
            "Batch Training Loss =  0.002030200557783246\n",
            "Batch Training Loss =  0.001986286137253046\n",
            "Batch Training Loss =  0.001918601687066257\n",
            "Batch Training Loss =  0.0019820104353129864\n",
            "Batch Training Loss =  0.0020636001136153936\n",
            "Batch Training Loss =  0.0020577958784997463\n",
            "Batch Training Loss =  0.002055620076134801\n",
            "Batch Training Loss =  0.001996561186388135\n",
            "Batch Training Loss =  0.002037803642451763\n",
            "Batch Training Loss =  0.00199877773411572\n",
            "Batch Training Loss =  0.0021301580127328634\n",
            "Batch Training Loss =  0.0020385663956403732\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0020772519055753946\n",
            "Batch Training Loss =  0.0021028604824095964\n",
            "Batch Training Loss =  0.0021008553449064493\n",
            "Batch Training Loss =  0.0020077594090253115\n",
            "Batch Training Loss =  0.002068326808512211\n",
            "Batch Training Loss =  0.0019318427657708526\n",
            "Batch Training Loss =  0.0019330106442794204\n",
            "Batch Training Loss =  0.0018845417071133852\n",
            "Batch Training Loss =  0.0021491521038115025\n",
            "Batch Training Loss =  0.0022306572645902634\n",
            "Batch Training Loss =  0.0019554365426301956\n",
            "Batch Training Loss =  0.0019698822870850563\n",
            "Batch Training Loss =  0.0020175997633486986\n",
            "Batch Training Loss =  0.001979054184630513\n",
            "Batch Training Loss =  0.0019687642343342304\n",
            "Batch Training Loss =  0.0020139559637755156\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0019210841273888946\n",
            "Batch Training Loss =  0.002169598126783967\n",
            "Batch Training Loss =  0.002204817021265626\n",
            "Batch Training Loss =  0.0020283563062548637\n",
            "Batch Training Loss =  0.0019935390446335077\n",
            "Batch Training Loss =  0.0019397992873564363\n",
            "Batch Training Loss =  0.0019434489076957107\n",
            "Batch Training Loss =  0.002108893124386668\n",
            "Batch Training Loss =  0.0021064248867332935\n",
            "Batch Training Loss =  0.0019341680454090238\n",
            "Batch Training Loss =  0.0019075045129284263\n",
            "Batch Training Loss =  0.0019048195099458098\n",
            "Batch Training Loss =  0.0021109450608491898\n",
            "Batch Training Loss =  0.001931732171215117\n",
            "Batch Training Loss =  0.0020340955816209316\n",
            "Batch Training Loss =  0.0019950871355831623\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0019501537317410111\n",
            "Batch Training Loss =  0.001950811012648046\n",
            "Batch Training Loss =  0.0019125716062262654\n",
            "Batch Training Loss =  0.002081008395180106\n",
            "Batch Training Loss =  0.0019347390625625849\n",
            "Batch Training Loss =  0.00216754712164402\n",
            "Batch Training Loss =  0.0020891493186354637\n",
            "Batch Training Loss =  0.0019188784062862396\n",
            "Batch Training Loss =  0.001928151585161686\n",
            "Batch Training Loss =  0.0021769965533167124\n",
            "Batch Training Loss =  0.001938707660883665\n",
            "Batch Training Loss =  0.002090966096147895\n",
            "Batch Training Loss =  0.0020415307953953743\n",
            "Batch Training Loss =  0.0020473250187933445\n",
            "Batch Training Loss =  0.0019858386367559433\n",
            "Batch Training Loss =  0.0019266802119091153\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0018385393777862191\n",
            "Batch Training Loss =  0.0019351131049916148\n",
            "Batch Training Loss =  0.001943350420333445\n",
            "Batch Training Loss =  0.0019850130192935467\n",
            "Batch Training Loss =  0.0018094098195433617\n",
            "Batch Training Loss =  0.0019378719152882695\n",
            "Batch Training Loss =  0.0020803757943212986\n",
            "Batch Training Loss =  0.0019647052977234125\n",
            "Batch Training Loss =  0.0020336315501481295\n",
            "Batch Training Loss =  0.0019104385282844305\n",
            "Batch Training Loss =  0.0020556054078042507\n",
            "Batch Training Loss =  0.0019465209916234016\n",
            "Batch Training Loss =  0.0020645514596253633\n",
            "Batch Training Loss =  0.0019372999668121338\n",
            "Batch Training Loss =  0.002110400004312396\n",
            "Batch Training Loss =  0.002007385017350316\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.002086296444758773\n",
            "Batch Training Loss =  0.0019062800565734506\n",
            "Batch Training Loss =  0.0020262422040104866\n",
            "Batch Training Loss =  0.0019877401646226645\n",
            "Batch Training Loss =  0.001907111145555973\n",
            "Batch Training Loss =  0.001985982758924365\n",
            "Batch Training Loss =  0.0018899253336712718\n",
            "Batch Training Loss =  0.0020007567945867777\n",
            "Batch Training Loss =  0.0020335568115115166\n",
            "Batch Training Loss =  0.0019094061572104692\n",
            "Batch Training Loss =  0.0018903822638094425\n",
            "Batch Training Loss =  0.0018352745100855827\n",
            "Batch Training Loss =  0.001855150330811739\n",
            "Batch Training Loss =  0.0017824267270043492\n",
            "Batch Training Loss =  0.0018970895325765014\n",
            "Batch Training Loss =  0.0019024864304810762\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0019784087780863047\n",
            "Batch Training Loss =  0.0019763000309467316\n",
            "Batch Training Loss =  0.0019849950913339853\n",
            "Batch Training Loss =  0.0018261490622535348\n",
            "Batch Training Loss =  0.0018709483556449413\n",
            "Batch Training Loss =  0.001846578437834978\n",
            "Batch Training Loss =  0.0020040557719767094\n",
            "Batch Training Loss =  0.0018280890071764588\n",
            "Batch Training Loss =  0.0018154692370444536\n",
            "Batch Training Loss =  0.0018513650866225362\n",
            "Batch Training Loss =  0.0021644621156156063\n",
            "Batch Training Loss =  0.0019093247829005122\n",
            "Batch Training Loss =  0.001979439752176404\n",
            "Batch Training Loss =  0.0018721474334597588\n",
            "Batch Training Loss =  0.0018997355364263058\n",
            "Batch Training Loss =  0.0017123041907325387\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0019454711582511663\n",
            "Batch Training Loss =  0.0017453401815146208\n",
            "Batch Training Loss =  0.0019887015223503113\n",
            "Batch Training Loss =  0.001941596157848835\n",
            "Batch Training Loss =  0.001956385327503085\n",
            "Batch Training Loss =  0.0018531529931351542\n",
            "Batch Training Loss =  0.0019335884135216475\n",
            "Batch Training Loss =  0.0019377500284463167\n",
            "Batch Training Loss =  0.0017916997894644737\n",
            "Batch Training Loss =  0.0018922154558822513\n",
            "Batch Training Loss =  0.0018864813027903438\n",
            "Batch Training Loss =  0.0018664507661014795\n",
            "Batch Training Loss =  0.001849485095590353\n",
            "Batch Training Loss =  0.0019380757585167885\n",
            "Batch Training Loss =  0.0018712054006755352\n",
            "Batch Training Loss =  0.0019833820406347513\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0017845281399786472\n",
            "Batch Training Loss =  0.0019096443429589272\n",
            "Batch Training Loss =  0.0016928635304793715\n",
            "Batch Training Loss =  0.0019365064799785614\n",
            "Batch Training Loss =  0.001777017256245017\n",
            "Batch Training Loss =  0.0018064598552882671\n",
            "Batch Training Loss =  0.002070317044854164\n",
            "Batch Training Loss =  0.0018743900582194328\n",
            "Batch Training Loss =  0.0019529553828760982\n",
            "Batch Training Loss =  0.002110375789925456\n",
            "Batch Training Loss =  0.0019287790637463331\n",
            "Batch Training Loss =  0.0018012278014793992\n",
            "Batch Training Loss =  0.0017100650584325194\n",
            "Batch Training Loss =  0.0018857377581298351\n",
            "Batch Training Loss =  0.001860147574916482\n",
            "Batch Training Loss =  0.0018576941220089793\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0018126122886314988\n",
            "Batch Training Loss =  0.0018335835775360465\n",
            "Batch Training Loss =  0.0017141461139544845\n",
            "Batch Training Loss =  0.001688887132331729\n",
            "Batch Training Loss =  0.001828923705033958\n",
            "Batch Training Loss =  0.0017616194672882557\n",
            "Batch Training Loss =  0.0018198646139353514\n",
            "Batch Training Loss =  0.0019209730671718717\n",
            "Batch Training Loss =  0.0019281089771538973\n",
            "Batch Training Loss =  0.001868695137090981\n",
            "Batch Training Loss =  0.001940144575200975\n",
            "Batch Training Loss =  0.002070790622383356\n",
            "Batch Training Loss =  0.0018743169493973255\n",
            "Batch Training Loss =  0.0019586451817303896\n",
            "Batch Training Loss =  0.0018237574258819222\n",
            "Batch Training Loss =  0.0017850290751084685\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0018285386031493545\n",
            "Batch Training Loss =  0.001772958436049521\n",
            "Batch Training Loss =  0.0018113567493855953\n",
            "Batch Training Loss =  0.0018888029735535383\n",
            "Batch Training Loss =  0.0017255156999453902\n",
            "Batch Training Loss =  0.002011694712564349\n",
            "Batch Training Loss =  0.0019345716573297977\n",
            "Batch Training Loss =  0.0018920053262263536\n",
            "Batch Training Loss =  0.0017491909675300121\n",
            "Batch Training Loss =  0.0017951991176232696\n",
            "Batch Training Loss =  0.001891644555144012\n",
            "Batch Training Loss =  0.0019668748136609793\n",
            "Batch Training Loss =  0.0017180046997964382\n",
            "Batch Training Loss =  0.0018489831127226353\n",
            "Batch Training Loss =  0.0018249177373945713\n",
            "Batch Training Loss =  0.0020893379114568233\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0019716371316462755\n",
            "Batch Training Loss =  0.0019300734857097268\n",
            "Batch Training Loss =  0.0018839958356693387\n",
            "Batch Training Loss =  0.002005159156396985\n",
            "Batch Training Loss =  0.001800136175006628\n",
            "Batch Training Loss =  0.0018132750410586596\n",
            "Batch Training Loss =  0.0018784722778946161\n",
            "Batch Training Loss =  0.0017330325208604336\n",
            "Batch Training Loss =  0.001856108196079731\n",
            "Batch Training Loss =  0.0018633570289239287\n",
            "Batch Training Loss =  0.0018470027716830373\n",
            "Batch Training Loss =  0.001730980584397912\n",
            "Batch Training Loss =  0.0018052452942356467\n",
            "Batch Training Loss =  0.0018261303193867207\n",
            "Batch Training Loss =  0.0017417140770703554\n",
            "Batch Training Loss =  0.0019076063763350248\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0017309300601482391\n",
            "Batch Training Loss =  0.0017688603838905692\n",
            "Batch Training Loss =  0.0017645531333982944\n",
            "Batch Training Loss =  0.0019081449136137962\n",
            "Batch Training Loss =  0.0018800676334649324\n",
            "Batch Training Loss =  0.0017627221532166004\n",
            "Batch Training Loss =  0.00201094476506114\n",
            "Batch Training Loss =  0.0018068772042170167\n",
            "Batch Training Loss =  0.0017470610328018665\n",
            "Batch Training Loss =  0.0018616791348904371\n",
            "Batch Training Loss =  0.0017691073007881641\n",
            "Batch Training Loss =  0.0017363385995849967\n",
            "Batch Training Loss =  0.0017718918388709426\n",
            "Batch Training Loss =  0.001873220200650394\n",
            "Batch Training Loss =  0.001894502085633576\n",
            "Batch Training Loss =  0.0018708589486777782\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0017773214494809508\n",
            "Batch Training Loss =  0.0017467783764004707\n",
            "Batch Training Loss =  0.0018390010809525847\n",
            "Batch Training Loss =  0.001933957333676517\n",
            "Batch Training Loss =  0.001758698490448296\n",
            "Batch Training Loss =  0.0017864856636151671\n",
            "Batch Training Loss =  0.001761894323863089\n",
            "Batch Training Loss =  0.0017936447402462363\n",
            "Batch Training Loss =  0.0017947740852832794\n",
            "Batch Training Loss =  0.0018076661508530378\n",
            "Batch Training Loss =  0.0017667850479483604\n",
            "Batch Training Loss =  0.0018308754079043865\n",
            "Batch Training Loss =  0.0017655408009886742\n",
            "Batch Training Loss =  0.0019813848193734884\n",
            "Batch Training Loss =  0.0017810941208153963\n",
            "Batch Training Loss =  0.0017594716046005487\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0018356472719460726\n",
            "Batch Training Loss =  0.001719098654575646\n",
            "Batch Training Loss =  0.0017499156529083848\n",
            "Batch Training Loss =  0.001854170230217278\n",
            "Batch Training Loss =  0.0017231726087629795\n",
            "Batch Training Loss =  0.001825741957873106\n",
            "Batch Training Loss =  0.001797510078176856\n",
            "Batch Training Loss =  0.0017199516296386719\n",
            "Batch Training Loss =  0.0016353509854525328\n",
            "Batch Training Loss =  0.001797857228666544\n",
            "Batch Training Loss =  0.0016350377118214965\n",
            "Batch Training Loss =  0.001878594164736569\n",
            "Batch Training Loss =  0.0017334435833618045\n",
            "Batch Training Loss =  0.0018083538161590695\n",
            "Batch Training Loss =  0.001795888994820416\n",
            "Batch Training Loss =  0.001859382726252079\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0016975103644654155\n",
            "Batch Training Loss =  0.001604451215825975\n",
            "Batch Training Loss =  0.0018850895576179028\n",
            "Batch Training Loss =  0.0018517820863053203\n",
            "Batch Training Loss =  0.0017647977219894528\n",
            "Batch Training Loss =  0.0016745547764003277\n",
            "Batch Training Loss =  0.0018391545163467526\n",
            "Batch Training Loss =  0.001556570059619844\n",
            "Batch Training Loss =  0.0016194629715755582\n",
            "Batch Training Loss =  0.0018451237119734287\n",
            "Batch Training Loss =  0.001639839611016214\n",
            "Batch Training Loss =  0.0017802850343286991\n",
            "Batch Training Loss =  0.0018087311182171106\n",
            "Batch Training Loss =  0.0017352678114548326\n",
            "Batch Training Loss =  0.0017481924733147025\n",
            "Batch Training Loss =  0.0017962065758183599\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.001730491523630917\n",
            "Batch Training Loss =  0.0017096890369430184\n",
            "Batch Training Loss =  0.0018033541273325682\n",
            "Batch Training Loss =  0.0016675943043082952\n",
            "Batch Training Loss =  0.0017489100573584437\n",
            "Batch Training Loss =  0.0018844087608158588\n",
            "Batch Training Loss =  0.001711438992060721\n",
            "Batch Training Loss =  0.0015868514310568571\n",
            "Batch Training Loss =  0.0018741232343018055\n",
            "Batch Training Loss =  0.0017850996227934957\n",
            "Batch Training Loss =  0.0017581055872142315\n",
            "Batch Training Loss =  0.0017411148874089122\n",
            "Batch Training Loss =  0.0018008186016231775\n",
            "Batch Training Loss =  0.0016418354352936149\n",
            "Batch Training Loss =  0.0015733875334262848\n",
            "Batch Training Loss =  0.0017300989711657166\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0016821756726130843\n",
            "Batch Training Loss =  0.0017956978408619761\n",
            "Batch Training Loss =  0.0016805317718535662\n",
            "Batch Training Loss =  0.0017851884476840496\n",
            "Batch Training Loss =  0.0017283700872212648\n",
            "Batch Training Loss =  0.0016509767156094313\n",
            "Batch Training Loss =  0.0016213807975873351\n",
            "Batch Training Loss =  0.0016390980454161763\n",
            "Batch Training Loss =  0.001680205576121807\n",
            "Batch Training Loss =  0.0017900473903864622\n",
            "Batch Training Loss =  0.001717583741992712\n",
            "Batch Training Loss =  0.0017525869188830256\n",
            "Batch Training Loss =  0.0017079169629141688\n",
            "Batch Training Loss =  0.0017453937325626612\n",
            "Batch Training Loss =  0.0017282856861129403\n",
            "Batch Training Loss =  0.001740527804940939\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0016872368287295103\n",
            "Batch Training Loss =  0.0016699597472324967\n",
            "Batch Training Loss =  0.0018134356942027807\n",
            "Batch Training Loss =  0.0016410468379035592\n",
            "Batch Training Loss =  0.0017494028434157372\n",
            "Batch Training Loss =  0.0016374086262658238\n",
            "Batch Training Loss =  0.0016934372251853347\n",
            "Batch Training Loss =  0.0016618215013295412\n",
            "Batch Training Loss =  0.0017710422398522496\n",
            "Batch Training Loss =  0.00173653825186193\n",
            "Batch Training Loss =  0.0018216153839603066\n",
            "Batch Training Loss =  0.0017062996048480272\n",
            "Batch Training Loss =  0.0017302579944953322\n",
            "Batch Training Loss =  0.0016628177836537361\n",
            "Batch Training Loss =  0.0016381734749302268\n",
            "Batch Training Loss =  0.0016553219174966216\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0016851292457431555\n",
            "Batch Training Loss =  0.0016311014769598842\n",
            "Batch Training Loss =  0.0017194087849929929\n",
            "Batch Training Loss =  0.001601614523679018\n",
            "Batch Training Loss =  0.001712840748950839\n",
            "Batch Training Loss =  0.001693077734671533\n",
            "Batch Training Loss =  0.0016169184818863869\n",
            "Batch Training Loss =  0.0017342765349894762\n",
            "Batch Training Loss =  0.0016502438811585307\n",
            "Batch Training Loss =  0.001552859670482576\n",
            "Batch Training Loss =  0.0016476145246997476\n",
            "Batch Training Loss =  0.0017641462618485093\n",
            "Batch Training Loss =  0.0016696297097951174\n",
            "Batch Training Loss =  0.001654054969549179\n",
            "Batch Training Loss =  0.0016668487805873156\n",
            "Batch Training Loss =  0.0017643880564719439\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.001791929011233151\n",
            "Batch Training Loss =  0.0017165428726002574\n",
            "Batch Training Loss =  0.001660595997236669\n",
            "Batch Training Loss =  0.0018086560303345323\n",
            "Batch Training Loss =  0.0016775837866589427\n",
            "Batch Training Loss =  0.001604846678674221\n",
            "Batch Training Loss =  0.0015783661510795355\n",
            "Batch Training Loss =  0.0016531402943655849\n",
            "Batch Training Loss =  0.0016559261130169034\n",
            "Batch Training Loss =  0.0015828448813408613\n",
            "Batch Training Loss =  0.0017408961430191994\n",
            "Batch Training Loss =  0.0016214633360505104\n",
            "Batch Training Loss =  0.0016283247387036681\n",
            "Batch Training Loss =  0.0017103339778259397\n",
            "Batch Training Loss =  0.0016232840716838837\n",
            "Batch Training Loss =  0.0017679467564448714\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0016457155579701066\n",
            "Batch Training Loss =  0.0016142200911417603\n",
            "Batch Training Loss =  0.0016982730012387037\n",
            "Batch Training Loss =  0.0017009672010317445\n",
            "Batch Training Loss =  0.0016502471407875419\n",
            "Batch Training Loss =  0.0016627759905532002\n",
            "Batch Training Loss =  0.0016429013339802623\n",
            "Batch Training Loss =  0.001826804829761386\n",
            "Batch Training Loss =  0.001700587454251945\n",
            "Batch Training Loss =  0.0016745999455451965\n",
            "Batch Training Loss =  0.0015644453233107924\n",
            "Batch Training Loss =  0.001681083464063704\n",
            "Batch Training Loss =  0.0016033778665587306\n",
            "Batch Training Loss =  0.0015720883384346962\n",
            "Batch Training Loss =  0.0017165589379146695\n",
            "Batch Training Loss =  0.0016031895065680146\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0016918175388127565\n",
            "Batch Training Loss =  0.0016845975769683719\n",
            "Batch Training Loss =  0.001615950372070074\n",
            "Batch Training Loss =  0.0018099278677254915\n",
            "Batch Training Loss =  0.001510347705334425\n",
            "Batch Training Loss =  0.0017916178330779076\n",
            "Batch Training Loss =  0.0016540155047550797\n",
            "Batch Training Loss =  0.0015548040391877294\n",
            "Batch Training Loss =  0.0017191063379868865\n",
            "Batch Training Loss =  0.0016428283415734768\n",
            "Batch Training Loss =  0.0015398419927805662\n",
            "Batch Training Loss =  0.0015888095367699862\n",
            "Batch Training Loss =  0.0017319165635854006\n",
            "Batch Training Loss =  0.0016027773963287473\n",
            "Batch Training Loss =  0.001590702566318214\n",
            "Batch Training Loss =  0.0016359447035938501\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0016396022401750088\n",
            "Batch Training Loss =  0.0015902769519016147\n",
            "Batch Training Loss =  0.0016064879018813372\n",
            "Batch Training Loss =  0.001617579604499042\n",
            "Batch Training Loss =  0.0014888456789776683\n",
            "Batch Training Loss =  0.0017649702494964004\n",
            "Batch Training Loss =  0.0016909155528992414\n",
            "Batch Training Loss =  0.0016496297903358936\n",
            "Batch Training Loss =  0.0017192543018609285\n",
            "Batch Training Loss =  0.0017807716503739357\n",
            "Batch Training Loss =  0.0016728452173992991\n",
            "Batch Training Loss =  0.0015256244223564863\n",
            "Batch Training Loss =  0.0015643809456378222\n",
            "Batch Training Loss =  0.0015552421100437641\n",
            "Batch Training Loss =  0.0016482076607644558\n",
            "Batch Training Loss =  0.0015255919424816966\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0016409519594162703\n",
            "Batch Training Loss =  0.0016421760665252805\n",
            "Batch Training Loss =  0.001539116376079619\n",
            "Batch Training Loss =  0.0015517325373366475\n",
            "Batch Training Loss =  0.0016221599653363228\n",
            "Batch Training Loss =  0.00156165671069175\n",
            "Batch Training Loss =  0.0015605537919327617\n",
            "Batch Training Loss =  0.0016755452379584312\n",
            "Batch Training Loss =  0.001586166792549193\n",
            "Batch Training Loss =  0.0016642393311485648\n",
            "Batch Training Loss =  0.0017166405450552702\n",
            "Batch Training Loss =  0.001728659262880683\n",
            "Batch Training Loss =  0.0015546053182333708\n",
            "Batch Training Loss =  0.0016637103399261832\n",
            "Batch Training Loss =  0.001611950108781457\n",
            "Batch Training Loss =  0.0015276697231456637\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0015651648864150047\n",
            "Batch Training Loss =  0.0016493086004629731\n",
            "Batch Training Loss =  0.0016195493517443538\n",
            "Batch Training Loss =  0.0015511374222114682\n",
            "Batch Training Loss =  0.0015749541344121099\n",
            "Batch Training Loss =  0.0016193367773666978\n",
            "Batch Training Loss =  0.0015484098112210631\n",
            "Batch Training Loss =  0.0016395290149375796\n",
            "Batch Training Loss =  0.0016400357708334923\n",
            "Batch Training Loss =  0.0015320037491619587\n",
            "Batch Training Loss =  0.0016676468076184392\n",
            "Batch Training Loss =  0.0016255696536973119\n",
            "Batch Training Loss =  0.0015380956465378404\n",
            "Batch Training Loss =  0.0014620150905102491\n",
            "Batch Training Loss =  0.0016410585958510637\n",
            "Batch Training Loss =  0.0016265069134533405\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0015796669758856297\n",
            "Batch Training Loss =  0.0015959418378770351\n",
            "Batch Training Loss =  0.0015535014681518078\n",
            "Batch Training Loss =  0.0015821839915588498\n",
            "Batch Training Loss =  0.001554805669002235\n",
            "Batch Training Loss =  0.001553392969071865\n",
            "Batch Training Loss =  0.001576044363901019\n",
            "Batch Training Loss =  0.0016206236323341727\n",
            "Batch Training Loss =  0.0017369664274156094\n",
            "Batch Training Loss =  0.0016022693598642945\n",
            "Batch Training Loss =  0.0017389042768627405\n",
            "Batch Training Loss =  0.0015688057756051421\n",
            "Batch Training Loss =  0.0015666810795664787\n",
            "Batch Training Loss =  0.0015161680057644844\n",
            "Batch Training Loss =  0.001496893004514277\n",
            "Batch Training Loss =  0.001586552825756371\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0015419807750731707\n",
            "Batch Training Loss =  0.0015714078908786178\n",
            "Batch Training Loss =  0.0016042968491092324\n",
            "Batch Training Loss =  0.0014844394754618406\n",
            "Batch Training Loss =  0.001706375740468502\n",
            "Batch Training Loss =  0.0015773441409692168\n",
            "Batch Training Loss =  0.0015064654871821404\n",
            "Batch Training Loss =  0.0014730525435879827\n",
            "Batch Training Loss =  0.0015221460489556193\n",
            "Batch Training Loss =  0.0016971784643828869\n",
            "Batch Training Loss =  0.0016773265087977052\n",
            "Batch Training Loss =  0.0016178510850295424\n",
            "Batch Training Loss =  0.0014490026514977217\n",
            "Batch Training Loss =  0.0015517696738243103\n",
            "Batch Training Loss =  0.001604005810804665\n",
            "Batch Training Loss =  0.001534200506284833\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0015402828576043248\n",
            "Batch Training Loss =  0.0016498740296810865\n",
            "Batch Training Loss =  0.0016002410557121038\n",
            "Batch Training Loss =  0.001517202123068273\n",
            "Batch Training Loss =  0.001496194745413959\n",
            "Batch Training Loss =  0.001578300492838025\n",
            "Batch Training Loss =  0.0018474818207323551\n",
            "Batch Training Loss =  0.0014647101052105427\n",
            "Batch Training Loss =  0.001549880369566381\n",
            "Batch Training Loss =  0.0015723004471510649\n",
            "Batch Training Loss =  0.001492558978497982\n",
            "Batch Training Loss =  0.0016225686995312572\n",
            "Batch Training Loss =  0.001548243802972138\n",
            "Batch Training Loss =  0.0015300549566745758\n",
            "Batch Training Loss =  0.0014268241357058287\n",
            "Batch Training Loss =  0.0014782031066715717\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0016663911519572139\n",
            "Batch Training Loss =  0.001429133233614266\n",
            "Batch Training Loss =  0.001563199795782566\n",
            "Batch Training Loss =  0.001578362425789237\n",
            "Batch Training Loss =  0.0014830769505351782\n",
            "Batch Training Loss =  0.0015488079516217113\n",
            "Batch Training Loss =  0.0015992525732144713\n",
            "Batch Training Loss =  0.0016299649141728878\n",
            "Batch Training Loss =  0.0014881809474900365\n",
            "Batch Training Loss =  0.001410933444276452\n",
            "Batch Training Loss =  0.00137366633862257\n",
            "Batch Training Loss =  0.0015003978041931987\n",
            "Batch Training Loss =  0.0015035244869068265\n",
            "Batch Training Loss =  0.0016094375168904662\n",
            "Batch Training Loss =  0.001530896290205419\n",
            "Batch Training Loss =  0.0015176762826740742\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.001495651202276349\n",
            "Batch Training Loss =  0.0014612325467169285\n",
            "Batch Training Loss =  0.0019003812922164798\n",
            "Batch Training Loss =  0.0014845357509329915\n",
            "Batch Training Loss =  0.0014868632424622774\n",
            "Batch Training Loss =  0.0015374780632555485\n",
            "Batch Training Loss =  0.0016736551187932491\n",
            "Batch Training Loss =  0.001455729128792882\n",
            "Batch Training Loss =  0.0015214118175208569\n",
            "Batch Training Loss =  0.0015206580283120275\n",
            "Batch Training Loss =  0.0014987674076110125\n",
            "Batch Training Loss =  0.001487779663875699\n",
            "Batch Training Loss =  0.0014925781870260835\n",
            "Batch Training Loss =  0.00150795909576118\n",
            "Batch Training Loss =  0.0015937000280246139\n",
            "Batch Training Loss =  0.0014164781896397471\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0015129272360354662\n",
            "Batch Training Loss =  0.0015178424073383212\n",
            "Batch Training Loss =  0.001518165459856391\n",
            "Batch Training Loss =  0.0015161558985710144\n",
            "Batch Training Loss =  0.001512702670879662\n",
            "Batch Training Loss =  0.0014900382375344634\n",
            "Batch Training Loss =  0.0014720078324899077\n",
            "Batch Training Loss =  0.001477590762078762\n",
            "Batch Training Loss =  0.0014989526243880391\n",
            "Batch Training Loss =  0.0016754255630075932\n",
            "Batch Training Loss =  0.0015003271400928497\n",
            "Batch Training Loss =  0.0015141176991164684\n",
            "Batch Training Loss =  0.0015543097397312522\n",
            "Batch Training Loss =  0.001466688932850957\n",
            "Batch Training Loss =  0.0014401051448658109\n",
            "Batch Training Loss =  0.0014938577078282833\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0015411102212965488\n",
            "Batch Training Loss =  0.001467490685172379\n",
            "Batch Training Loss =  0.0015271349111571908\n",
            "Batch Training Loss =  0.0016320194117724895\n",
            "Batch Training Loss =  0.001456560450606048\n",
            "Batch Training Loss =  0.001574003603309393\n",
            "Batch Training Loss =  0.001426094677299261\n",
            "Batch Training Loss =  0.0014694492565467954\n",
            "Batch Training Loss =  0.0016794551629573107\n",
            "Batch Training Loss =  0.0015662738587707281\n",
            "Batch Training Loss =  0.001470734947361052\n",
            "Batch Training Loss =  0.0014688132796436548\n",
            "Batch Training Loss =  0.0014044031267985702\n",
            "Batch Training Loss =  0.0015027285553514957\n",
            "Batch Training Loss =  0.0014511026674881577\n",
            "Batch Training Loss =  0.0015216011088341475\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0015324181877076626\n",
            "Batch Training Loss =  0.0013949029380455613\n",
            "Batch Training Loss =  0.0016742432489991188\n",
            "Batch Training Loss =  0.0014831562293693423\n",
            "Batch Training Loss =  0.001477559213526547\n",
            "Batch Training Loss =  0.0015468447236344218\n",
            "Batch Training Loss =  0.0014471608446910977\n",
            "Batch Training Loss =  0.0014811258297413588\n",
            "Batch Training Loss =  0.001529892673715949\n",
            "Batch Training Loss =  0.00154436519369483\n",
            "Batch Training Loss =  0.001378000364638865\n",
            "Batch Training Loss =  0.0014587922487407923\n",
            "Batch Training Loss =  0.0014763781800866127\n",
            "Batch Training Loss =  0.0014163581654429436\n",
            "Batch Training Loss =  0.0014669012743979692\n",
            "Batch Training Loss =  0.0015916924457997084\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.001498248428106308\n",
            "Batch Training Loss =  0.0015710197621956468\n",
            "Batch Training Loss =  0.0015538958832621574\n",
            "Batch Training Loss =  0.0014860364608466625\n",
            "Batch Training Loss =  0.0014713321579620242\n",
            "Batch Training Loss =  0.0015227720141410828\n",
            "Batch Training Loss =  0.0014308670070022345\n",
            "Batch Training Loss =  0.0013894152361899614\n",
            "Batch Training Loss =  0.0014333976432681084\n",
            "Batch Training Loss =  0.001434614066965878\n",
            "Batch Training Loss =  0.0016149102011695504\n",
            "Batch Training Loss =  0.001565160695463419\n",
            "Batch Training Loss =  0.001402537920512259\n",
            "Batch Training Loss =  0.0015017460100352764\n",
            "Batch Training Loss =  0.001470749732106924\n",
            "Batch Training Loss =  0.0014840748626738787\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0013761412119492888\n",
            "Batch Training Loss =  0.0014736856101080775\n",
            "Batch Training Loss =  0.0013865693472325802\n",
            "Batch Training Loss =  0.001438898267224431\n",
            "Batch Training Loss =  0.0014751136768609285\n",
            "Batch Training Loss =  0.001485210843384266\n",
            "Batch Training Loss =  0.0013859610771760345\n",
            "Batch Training Loss =  0.0014852252788841724\n",
            "Batch Training Loss =  0.0013715213863179088\n",
            "Batch Training Loss =  0.0015831456985324621\n",
            "Batch Training Loss =  0.0014300565235316753\n",
            "Batch Training Loss =  0.0014508157037198544\n",
            "Batch Training Loss =  0.0014888164587318897\n",
            "Batch Training Loss =  0.0014000997180119157\n",
            "Batch Training Loss =  0.0015859146369621158\n",
            "Batch Training Loss =  0.0015215394087135792\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0015499410219490528\n",
            "Batch Training Loss =  0.0014187784399837255\n",
            "Batch Training Loss =  0.0014399125939235091\n",
            "Batch Training Loss =  0.0014405031688511372\n",
            "Batch Training Loss =  0.0015779086388647556\n",
            "Batch Training Loss =  0.0014664827613160014\n",
            "Batch Training Loss =  0.001442297943867743\n",
            "Batch Training Loss =  0.001543689868412912\n",
            "Batch Training Loss =  0.001452775439247489\n",
            "Batch Training Loss =  0.0014700588071718812\n",
            "Batch Training Loss =  0.0013664972502738237\n",
            "Batch Training Loss =  0.001389246084727347\n",
            "Batch Training Loss =  0.0015122853219509125\n",
            "Batch Training Loss =  0.0014985838206484914\n",
            "Batch Training Loss =  0.001424081507138908\n",
            "Batch Training Loss =  0.0014018395449966192\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0015113065019249916\n",
            "Batch Training Loss =  0.001436571590602398\n",
            "Batch Training Loss =  0.0014312603743746877\n",
            "Batch Training Loss =  0.0015532639808952808\n",
            "Batch Training Loss =  0.0014912160113453865\n",
            "Batch Training Loss =  0.0014651338569819927\n",
            "Batch Training Loss =  0.0014501629630103707\n",
            "Batch Training Loss =  0.0014606164768338203\n",
            "Batch Training Loss =  0.0014405834954231977\n",
            "Batch Training Loss =  0.001550538232550025\n",
            "Batch Training Loss =  0.0014054833445698023\n",
            "Batch Training Loss =  0.0014428503345698118\n",
            "Batch Training Loss =  0.0014045529533177614\n",
            "Batch Training Loss =  0.0014289261307567358\n",
            "Batch Training Loss =  0.0014739875914528966\n",
            "Batch Training Loss =  0.0013178098015487194\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0014440701343119144\n",
            "Batch Training Loss =  0.0014028154546394944\n",
            "Batch Training Loss =  0.001366345677524805\n",
            "Batch Training Loss =  0.0013169314479455352\n",
            "Batch Training Loss =  0.0014650259399786592\n",
            "Batch Training Loss =  0.0015573742566630244\n",
            "Batch Training Loss =  0.001403822097927332\n",
            "Batch Training Loss =  0.0014773751609027386\n",
            "Batch Training Loss =  0.0014053396880626678\n",
            "Batch Training Loss =  0.001504878280684352\n",
            "Batch Training Loss =  0.001542019541375339\n",
            "Batch Training Loss =  0.0014259181916713715\n",
            "Batch Training Loss =  0.0013839767780154943\n",
            "Batch Training Loss =  0.0014964998699724674\n",
            "Batch Training Loss =  0.0014209016226232052\n",
            "Batch Training Loss =  0.0013581013772636652\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0013988969149067998\n",
            "Batch Training Loss =  0.0013061739737167954\n",
            "Batch Training Loss =  0.0014812189619988203\n",
            "Batch Training Loss =  0.0015511283418163657\n",
            "Batch Training Loss =  0.001502970582805574\n",
            "Batch Training Loss =  0.0014161935541778803\n",
            "Batch Training Loss =  0.001355431624688208\n",
            "Batch Training Loss =  0.0013995143817737699\n",
            "Batch Training Loss =  0.0014039995148777962\n",
            "Batch Training Loss =  0.001456031808629632\n",
            "Batch Training Loss =  0.0013827139046043158\n",
            "Batch Training Loss =  0.001344504184089601\n",
            "Batch Training Loss =  0.00146306527312845\n",
            "Batch Training Loss =  0.0013741501607000828\n",
            "Batch Training Loss =  0.0015032205265015364\n",
            "Batch Training Loss =  0.0014299434842541814\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0015429502818733454\n",
            "Batch Training Loss =  0.001391273457556963\n",
            "Batch Training Loss =  0.001501415390521288\n",
            "Batch Training Loss =  0.0014369538985192776\n",
            "Batch Training Loss =  0.0013717738911509514\n",
            "Batch Training Loss =  0.0014096501981839538\n",
            "Batch Training Loss =  0.0013274141820147634\n",
            "Batch Training Loss =  0.0014448459260165691\n",
            "Batch Training Loss =  0.0013315859250724316\n",
            "Batch Training Loss =  0.0013472221326082945\n",
            "Batch Training Loss =  0.0013978674542158842\n",
            "Batch Training Loss =  0.0014898990048095584\n",
            "Batch Training Loss =  0.0013311532093212008\n",
            "Batch Training Loss =  0.0014741846825927496\n",
            "Batch Training Loss =  0.001380890142172575\n",
            "Batch Training Loss =  0.00146595505066216\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0014160351129248738\n",
            "Batch Training Loss =  0.001429104944691062\n",
            "Batch Training Loss =  0.0013600561069324613\n",
            "Batch Training Loss =  0.0014980988344177604\n",
            "Batch Training Loss =  0.0013079094933345914\n",
            "Batch Training Loss =  0.0013525730464607477\n",
            "Batch Training Loss =  0.0014568507904186845\n",
            "Batch Training Loss =  0.001464178552851081\n",
            "Batch Training Loss =  0.0013004629872739315\n",
            "Batch Training Loss =  0.0013394023990258574\n",
            "Batch Training Loss =  0.0014278644230216742\n",
            "Batch Training Loss =  0.0013713055523112416\n",
            "Batch Training Loss =  0.0014136722311377525\n",
            "Batch Training Loss =  0.0013774203835055232\n",
            "Batch Training Loss =  0.00159933150280267\n",
            "Batch Training Loss =  0.0014148263726383448\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.001268977764993906\n",
            "Batch Training Loss =  0.00130648713093251\n",
            "Batch Training Loss =  0.0014061612309888005\n",
            "Batch Training Loss =  0.00134030741173774\n",
            "Batch Training Loss =  0.0013706408208236098\n",
            "Batch Training Loss =  0.0014210666995495558\n",
            "Batch Training Loss =  0.0013489004923030734\n",
            "Batch Training Loss =  0.0014051274629309773\n",
            "Batch Training Loss =  0.0013549518771469593\n",
            "Batch Training Loss =  0.0014219218865036964\n",
            "Batch Training Loss =  0.0015422034775838256\n",
            "Batch Training Loss =  0.0015039151767268777\n",
            "Batch Training Loss =  0.0014978398103266954\n",
            "Batch Training Loss =  0.0014556007226929069\n",
            "Batch Training Loss =  0.0014361734502017498\n",
            "Batch Training Loss =  0.0012938001891598105\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0014720602193847299\n",
            "Batch Training Loss =  0.0013263828586786985\n",
            "Batch Training Loss =  0.0013907323591411114\n",
            "Batch Training Loss =  0.0014607745688408613\n",
            "Batch Training Loss =  0.0013506024843081832\n",
            "Batch Training Loss =  0.0013526898110285401\n",
            "Batch Training Loss =  0.001445758156478405\n",
            "Batch Training Loss =  0.0014863376272842288\n",
            "Batch Training Loss =  0.001379003282636404\n",
            "Batch Training Loss =  0.0013795895501971245\n",
            "Batch Training Loss =  0.0013521703658625484\n",
            "Batch Training Loss =  0.0015503097092732787\n",
            "Batch Training Loss =  0.0013803814072161913\n",
            "Batch Training Loss =  0.001337694120593369\n",
            "Batch Training Loss =  0.0012862251605838537\n",
            "Batch Training Loss =  0.0012750237947329879\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0012849379563704133\n",
            "Batch Training Loss =  0.001358244102448225\n",
            "Batch Training Loss =  0.0013759274734184146\n",
            "Batch Training Loss =  0.0014040690148249269\n",
            "Batch Training Loss =  0.0014378735795617104\n",
            "Batch Training Loss =  0.001322627766057849\n",
            "Batch Training Loss =  0.0013768428470939398\n",
            "Batch Training Loss =  0.0014124722220003605\n",
            "Batch Training Loss =  0.0013084172969684005\n",
            "Batch Training Loss =  0.0014347892720252275\n",
            "Batch Training Loss =  0.001431971206329763\n",
            "Batch Training Loss =  0.0012819957919418812\n",
            "Batch Training Loss =  0.0014451919123530388\n",
            "Batch Training Loss =  0.0015040428843349218\n",
            "Batch Training Loss =  0.0014110376359894872\n",
            "Batch Training Loss =  0.0012867446057498455\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.001462101936340332\n",
            "Batch Training Loss =  0.001325137447565794\n",
            "Batch Training Loss =  0.0015161691699177027\n",
            "Batch Training Loss =  0.0013452093116939068\n",
            "Batch Training Loss =  0.0014083321439102292\n",
            "Batch Training Loss =  0.0012409990886226296\n",
            "Batch Training Loss =  0.001565817161463201\n",
            "Batch Training Loss =  0.0013876798329874873\n",
            "Batch Training Loss =  0.0013625718420371413\n",
            "Batch Training Loss =  0.0013975657057017088\n",
            "Batch Training Loss =  0.0014963023131713271\n",
            "Batch Training Loss =  0.0013220779364928603\n",
            "Batch Training Loss =  0.0013125851983204484\n",
            "Batch Training Loss =  0.001277546281926334\n",
            "Batch Training Loss =  0.001272008754312992\n",
            "Batch Training Loss =  0.0013422343181446195\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0014257138827815652\n",
            "Batch Training Loss =  0.0013457678724080324\n",
            "Batch Training Loss =  0.0013512552250176668\n",
            "Batch Training Loss =  0.0013422257034108043\n",
            "Batch Training Loss =  0.0013630088651552796\n",
            "Batch Training Loss =  0.0015097435098141432\n",
            "Batch Training Loss =  0.0015080067096278071\n",
            "Batch Training Loss =  0.00132310064509511\n",
            "Batch Training Loss =  0.0013716749381273985\n",
            "Batch Training Loss =  0.0014611906372010708\n",
            "Batch Training Loss =  0.001299991738051176\n",
            "Batch Training Loss =  0.0013432343257591128\n",
            "Batch Training Loss =  0.001391543191857636\n",
            "Batch Training Loss =  0.0012419030535966158\n",
            "Batch Training Loss =  0.001308084581978619\n",
            "Batch Training Loss =  0.0013780088629573584\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0013721826253458858\n",
            "Batch Training Loss =  0.0013212391640990973\n",
            "Batch Training Loss =  0.0014074232894927263\n",
            "Batch Training Loss =  0.001490869210101664\n",
            "Batch Training Loss =  0.0013187980512157083\n",
            "Batch Training Loss =  0.0014047259464859962\n",
            "Batch Training Loss =  0.0013978795614093542\n",
            "Batch Training Loss =  0.001310131512582302\n",
            "Batch Training Loss =  0.0013217857340350747\n",
            "Batch Training Loss =  0.0012576384469866753\n",
            "Batch Training Loss =  0.0013498147018253803\n",
            "Batch Training Loss =  0.0013275030069053173\n",
            "Batch Training Loss =  0.0012307236902415752\n",
            "Batch Training Loss =  0.0014638497959822416\n",
            "Batch Training Loss =  0.0013355878181755543\n",
            "Batch Training Loss =  0.0013902140781283379\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0013110440922901034\n",
            "Batch Training Loss =  0.001300141797401011\n",
            "Batch Training Loss =  0.0013491861755028367\n",
            "Batch Training Loss =  0.0012493529357016087\n",
            "Batch Training Loss =  0.0013377508148550987\n",
            "Batch Training Loss =  0.0013164626434445381\n",
            "Batch Training Loss =  0.0013871241826564074\n",
            "Batch Training Loss =  0.0012819123221561313\n",
            "Batch Training Loss =  0.0014462278923019767\n",
            "Batch Training Loss =  0.001310323947109282\n",
            "Batch Training Loss =  0.0013650141190737486\n",
            "Batch Training Loss =  0.0014386118855327368\n",
            "Batch Training Loss =  0.0013049693079665303\n",
            "Batch Training Loss =  0.0014067352749407291\n",
            "Batch Training Loss =  0.00129565445240587\n",
            "Batch Training Loss =  0.0013792914105579257\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.001312806154601276\n",
            "Batch Training Loss =  0.0014277969021350145\n",
            "Batch Training Loss =  0.0012227242114022374\n",
            "Batch Training Loss =  0.001328697893768549\n",
            "Batch Training Loss =  0.0013025375083088875\n",
            "Batch Training Loss =  0.0014707644004374743\n",
            "Batch Training Loss =  0.0013634369242936373\n",
            "Batch Training Loss =  0.0013682459248229861\n",
            "Batch Training Loss =  0.0013465705560520291\n",
            "Batch Training Loss =  0.0014339881017804146\n",
            "Batch Training Loss =  0.0013993706088513136\n",
            "Batch Training Loss =  0.0012590814149007201\n",
            "Batch Training Loss =  0.0012823460856452584\n",
            "Batch Training Loss =  0.0012787991436198354\n",
            "Batch Training Loss =  0.0012847803300246596\n",
            "Batch Training Loss =  0.001297305221669376\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0013198101660236716\n",
            "Batch Training Loss =  0.0012083820765838027\n",
            "Batch Training Loss =  0.0013137954520061612\n",
            "Batch Training Loss =  0.0012837140820920467\n",
            "Batch Training Loss =  0.0011885004350915551\n",
            "Batch Training Loss =  0.0012067352654412389\n",
            "Batch Training Loss =  0.0013279473641887307\n",
            "Batch Training Loss =  0.0012337175430729985\n",
            "Batch Training Loss =  0.0012382633285596967\n",
            "Batch Training Loss =  0.0011725141666829586\n",
            "Batch Training Loss =  0.0012378026731312275\n",
            "Batch Training Loss =  0.0012209380511194468\n",
            "Batch Training Loss =  0.0013084405800327659\n",
            "Batch Training Loss =  0.001219464000314474\n",
            "Batch Training Loss =  0.001119604567065835\n",
            "Batch Training Loss =  0.0012602650094777346\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.00109911581967026\n",
            "Batch Training Loss =  0.0011804078239947557\n",
            "Batch Training Loss =  0.0011379877105355263\n",
            "Batch Training Loss =  0.0011822428787127137\n",
            "Batch Training Loss =  0.0011990005150437355\n",
            "Batch Training Loss =  0.0012846440076828003\n",
            "Batch Training Loss =  0.0011438813526183367\n",
            "Batch Training Loss =  0.0011685955105349422\n",
            "Batch Training Loss =  0.0010797808645293117\n",
            "Batch Training Loss =  0.0011455080239102244\n",
            "Batch Training Loss =  0.0010984722757712007\n",
            "Batch Training Loss =  0.0011367418337613344\n",
            "Batch Training Loss =  0.0011347262188792229\n",
            "Batch Training Loss =  0.001077962340787053\n",
            "Batch Training Loss =  0.0010081124491989613\n",
            "Batch Training Loss =  0.0010407507652416825\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0010352986864745617\n",
            "Batch Training Loss =  0.0010009348625317216\n",
            "Batch Training Loss =  0.0010627913288772106\n",
            "Batch Training Loss =  0.0010727520566433668\n",
            "Batch Training Loss =  0.0011388841085135937\n",
            "Batch Training Loss =  0.0010401325998827815\n",
            "Batch Training Loss =  0.0010593627812340856\n",
            "Batch Training Loss =  0.0010817660950124264\n",
            "Batch Training Loss =  0.0010354366386309266\n",
            "Batch Training Loss =  0.0009968915255740285\n",
            "Batch Training Loss =  0.0009921503951773047\n",
            "Batch Training Loss =  0.0009881596779450774\n",
            "Batch Training Loss =  0.0010582843096926808\n",
            "Batch Training Loss =  0.0010693419026210904\n",
            "Batch Training Loss =  0.0010352024110034108\n",
            "Batch Training Loss =  0.0009660732466727495\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0011155781103298068\n",
            "Batch Training Loss =  0.000944145314861089\n",
            "Batch Training Loss =  0.0009672429878264666\n",
            "Batch Training Loss =  0.0009331378387287259\n",
            "Batch Training Loss =  0.000922306498978287\n",
            "Batch Training Loss =  0.0009129020036198199\n",
            "Batch Training Loss =  0.0009032453526742756\n",
            "Batch Training Loss =  0.0008826061966829002\n",
            "Batch Training Loss =  0.0010099569335579872\n",
            "Batch Training Loss =  0.001025641686283052\n",
            "Batch Training Loss =  0.0009275356424041092\n",
            "Batch Training Loss =  0.0010442548664286733\n",
            "Batch Training Loss =  0.0009906162740662694\n",
            "Batch Training Loss =  0.0009552994160912931\n",
            "Batch Training Loss =  0.0009923920733854175\n",
            "Batch Training Loss =  0.0009403547155670822\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.000878729741089046\n",
            "Batch Training Loss =  0.0008990205824375153\n",
            "Batch Training Loss =  0.0009114681161008775\n",
            "Batch Training Loss =  0.0008956927922554314\n",
            "Batch Training Loss =  0.0008956106030382216\n",
            "Batch Training Loss =  0.00095852225786075\n",
            "Batch Training Loss =  0.000839540793094784\n",
            "Batch Training Loss =  0.0009071051608771086\n",
            "Batch Training Loss =  0.0009531211690045893\n",
            "Batch Training Loss =  0.0009107307414524257\n",
            "Batch Training Loss =  0.000837908242829144\n",
            "Batch Training Loss =  0.0008983681327663362\n",
            "Batch Training Loss =  0.000819411885458976\n",
            "Batch Training Loss =  0.0008984101586975157\n",
            "Batch Training Loss =  0.0008600695291534066\n",
            "Batch Training Loss =  0.0008313184953294694\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0008784790989011526\n",
            "Batch Training Loss =  0.000813754799310118\n",
            "Batch Training Loss =  0.0008009542361833155\n",
            "Batch Training Loss =  0.0008234276901930571\n",
            "Batch Training Loss =  0.0008441017707809806\n",
            "Batch Training Loss =  0.0008683504420332611\n",
            "Batch Training Loss =  0.0008824554970487952\n",
            "Batch Training Loss =  0.000823049049358815\n",
            "Batch Training Loss =  0.0007768853683955967\n",
            "Batch Training Loss =  0.0008322542998939753\n",
            "Batch Training Loss =  0.0008286075317300856\n",
            "Batch Training Loss =  0.0008900188840925694\n",
            "Batch Training Loss =  0.0008133478695526719\n",
            "Batch Training Loss =  0.0007906399550847709\n",
            "Batch Training Loss =  0.0008447332074865699\n",
            "Batch Training Loss =  0.0007832287810742855\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0008299363544210792\n",
            "Batch Training Loss =  0.0007643340504728258\n",
            "Batch Training Loss =  0.0007746131741441786\n",
            "Batch Training Loss =  0.000782186456490308\n",
            "Batch Training Loss =  0.000772909028455615\n",
            "Batch Training Loss =  0.0008131287177093327\n",
            "Batch Training Loss =  0.0008294454892165959\n",
            "Batch Training Loss =  0.0008100076229311526\n",
            "Batch Training Loss =  0.0007476751343347132\n",
            "Batch Training Loss =  0.0007142861722968519\n",
            "Batch Training Loss =  0.0007846035878174007\n",
            "Batch Training Loss =  0.0008246312499977648\n",
            "Batch Training Loss =  0.0007416892913170159\n",
            "Batch Training Loss =  0.0007723582675680518\n",
            "Batch Training Loss =  0.000716172915417701\n",
            "Batch Training Loss =  0.0007432576385326684\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0007385907811112702\n",
            "Batch Training Loss =  0.0007838697638362646\n",
            "Batch Training Loss =  0.0006859095301479101\n",
            "Batch Training Loss =  0.0007275007083080709\n",
            "Batch Training Loss =  0.0007025734521448612\n",
            "Batch Training Loss =  0.0007831586990505457\n",
            "Batch Training Loss =  0.0007427488453686237\n",
            "Batch Training Loss =  0.000763032934628427\n",
            "Batch Training Loss =  0.0007635833462700248\n",
            "Batch Training Loss =  0.0007451634737662971\n",
            "Batch Training Loss =  0.0007935123285278678\n",
            "Batch Training Loss =  0.0007252120412886143\n",
            "Batch Training Loss =  0.000684559578076005\n",
            "Batch Training Loss =  0.0006686608539894223\n",
            "Batch Training Loss =  0.0007174360798671842\n",
            "Batch Training Loss =  0.0007311031804420054\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0007370801758952439\n",
            "Batch Training Loss =  0.0007426086813211441\n",
            "Batch Training Loss =  0.0006904212641529739\n",
            "Batch Training Loss =  0.000678601092658937\n",
            "Batch Training Loss =  0.0006914418772794306\n",
            "Batch Training Loss =  0.0007097478373907506\n",
            "Batch Training Loss =  0.0007409549434669316\n",
            "Batch Training Loss =  0.0007227565511129797\n",
            "Batch Training Loss =  0.0006514825508929789\n",
            "Batch Training Loss =  0.000653195776976645\n",
            "Batch Training Loss =  0.0006816258537583053\n",
            "Batch Training Loss =  0.0007099673384800553\n",
            "Batch Training Loss =  0.000675398507155478\n",
            "Batch Training Loss =  0.0006528070080094039\n",
            "Batch Training Loss =  0.0006670680013485253\n",
            "Batch Training Loss =  0.000693231588229537\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0006831953069195151\n",
            "Batch Training Loss =  0.0007096807821653783\n",
            "Batch Training Loss =  0.0006717076757922769\n",
            "Batch Training Loss =  0.0006794477230869234\n",
            "Batch Training Loss =  0.0006138223689049482\n",
            "Batch Training Loss =  0.0006362331914715469\n",
            "Batch Training Loss =  0.0006200666539371014\n",
            "Batch Training Loss =  0.0006600338383577764\n",
            "Batch Training Loss =  0.0006600306369364262\n",
            "Batch Training Loss =  0.0006716004572808743\n",
            "Batch Training Loss =  0.0006415654788725078\n",
            "Batch Training Loss =  0.0006087085348553956\n",
            "Batch Training Loss =  0.0006694024195894599\n",
            "Batch Training Loss =  0.0006409084890037775\n",
            "Batch Training Loss =  0.0006696865893900394\n",
            "Batch Training Loss =  0.0006279825465753675\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0006263455143198371\n",
            "Batch Training Loss =  0.0006001999136060476\n",
            "Batch Training Loss =  0.0006364128203131258\n",
            "Batch Training Loss =  0.0006093234405852854\n",
            "Batch Training Loss =  0.0006527365185320377\n",
            "Batch Training Loss =  0.0006329366005957127\n",
            "Batch Training Loss =  0.0006504157208837569\n",
            "Batch Training Loss =  0.0007018524338491261\n",
            "Batch Training Loss =  0.0006523680058307946\n",
            "Batch Training Loss =  0.0006149579421617091\n",
            "Batch Training Loss =  0.0006449252832680941\n",
            "Batch Training Loss =  0.0006083602784201503\n",
            "Batch Training Loss =  0.0006417201366275549\n",
            "Batch Training Loss =  0.0006370565970428288\n",
            "Batch Training Loss =  0.0006081366445869207\n",
            "Batch Training Loss =  0.0006144613143987954\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0006371221970766783\n",
            "Batch Training Loss =  0.0005859534139744937\n",
            "Batch Training Loss =  0.0005930489278398454\n",
            "Batch Training Loss =  0.0006528337835334241\n",
            "Batch Training Loss =  0.0006017737905494869\n",
            "Batch Training Loss =  0.0006313308840617537\n",
            "Batch Training Loss =  0.0005844352417625487\n",
            "Batch Training Loss =  0.0005639293813146651\n",
            "Batch Training Loss =  0.0005820286460220814\n",
            "Batch Training Loss =  0.0006067752256058156\n",
            "Batch Training Loss =  0.0005816830089315772\n",
            "Batch Training Loss =  0.0006007053889334202\n",
            "Batch Training Loss =  0.0006012145895510912\n",
            "Batch Training Loss =  0.0005975017556920648\n",
            "Batch Training Loss =  0.0005665652570314705\n",
            "Batch Training Loss =  0.0006180238560773432\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0005445567658171058\n",
            "Batch Training Loss =  0.0006278473301790655\n",
            "Batch Training Loss =  0.0005894678179174662\n",
            "Batch Training Loss =  0.0005520628183148801\n",
            "Batch Training Loss =  0.000570963486097753\n",
            "Batch Training Loss =  0.0005695264553651214\n",
            "Batch Training Loss =  0.0005955692613497376\n",
            "Batch Training Loss =  0.0005761292413808405\n",
            "Batch Training Loss =  0.0005694095161743462\n",
            "Batch Training Loss =  0.0005805045948363841\n",
            "Batch Training Loss =  0.0005587025661952794\n",
            "Batch Training Loss =  0.0006100163445807993\n",
            "Batch Training Loss =  0.000533908954821527\n",
            "Batch Training Loss =  0.0005640870076604187\n",
            "Batch Training Loss =  0.0005734458682127297\n",
            "Batch Training Loss =  0.000557050050701946\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.00055350037291646\n",
            "Batch Training Loss =  0.0005781482323072851\n",
            "Batch Training Loss =  0.0005684444331564009\n",
            "Batch Training Loss =  0.0005455283680930734\n",
            "Batch Training Loss =  0.0005199139704927802\n",
            "Batch Training Loss =  0.0005140310386195779\n",
            "Batch Training Loss =  0.0005119770648889244\n",
            "Batch Training Loss =  0.0005096452077850699\n",
            "Batch Training Loss =  0.0005631214589811862\n",
            "Batch Training Loss =  0.0005292633431963623\n",
            "Batch Training Loss =  0.0005618600989691913\n",
            "Batch Training Loss =  0.0005344917881302536\n",
            "Batch Training Loss =  0.0005732736317440867\n",
            "Batch Training Loss =  0.0005428640288300812\n",
            "Batch Training Loss =  0.000566386675927788\n",
            "Batch Training Loss =  0.0005989093333482742\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0005458371597342193\n",
            "Batch Training Loss =  0.0005464794812723994\n",
            "Batch Training Loss =  0.000518087821546942\n",
            "Batch Training Loss =  0.0004971028538420796\n",
            "Batch Training Loss =  0.0005050310282967985\n",
            "Batch Training Loss =  0.0005397810600697994\n",
            "Batch Training Loss =  0.0005263544735498726\n",
            "Batch Training Loss =  0.0005065894802100956\n",
            "Batch Training Loss =  0.0005219967570155859\n",
            "Batch Training Loss =  0.0005046403966844082\n",
            "Batch Training Loss =  0.0005066214362159371\n",
            "Batch Training Loss =  0.0005461615510284901\n",
            "Batch Training Loss =  0.0005165216862224042\n",
            "Batch Training Loss =  0.0005531238275580108\n",
            "Batch Training Loss =  0.00053250981727615\n",
            "Batch Training Loss =  0.0005249439273029566\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0005262325867079198\n",
            "Batch Training Loss =  0.0005075435619801283\n",
            "Batch Training Loss =  0.0005204366752877831\n",
            "Batch Training Loss =  0.0005229494418017566\n",
            "Batch Training Loss =  0.00048563737072981894\n",
            "Batch Training Loss =  0.0005199180450290442\n",
            "Batch Training Loss =  0.00046945526264607906\n",
            "Batch Training Loss =  0.0004795474815182388\n",
            "Batch Training Loss =  0.00050557148642838\n",
            "Batch Training Loss =  0.0005079378024674952\n",
            "Batch Training Loss =  0.0004816607397515327\n",
            "Batch Training Loss =  0.00046580962953157723\n",
            "Batch Training Loss =  0.0005563389277085662\n",
            "Batch Training Loss =  0.00046626373659819365\n",
            "Batch Training Loss =  0.0005193763063289225\n",
            "Batch Training Loss =  0.00048511993372812867\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0004996843636035919\n",
            "Batch Training Loss =  0.0005161116132512689\n",
            "Batch Training Loss =  0.0004899321356788278\n",
            "Batch Training Loss =  0.00047932923189364374\n",
            "Batch Training Loss =  0.00047957044444046915\n",
            "Batch Training Loss =  0.0004912668955512345\n",
            "Batch Training Loss =  0.0004732736269943416\n",
            "Batch Training Loss =  0.0004933945601806045\n",
            "Batch Training Loss =  0.00048805889673531055\n",
            "Batch Training Loss =  0.00045836076606065035\n",
            "Batch Training Loss =  0.00046244877739809453\n",
            "Batch Training Loss =  0.0004911577561870217\n",
            "Batch Training Loss =  0.0004956097691319883\n",
            "Batch Training Loss =  0.0004603401175700128\n",
            "Batch Training Loss =  0.0004557414213195443\n",
            "Batch Training Loss =  0.0004615452780853957\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0004832890990655869\n",
            "Batch Training Loss =  0.00044548316509462893\n",
            "Batch Training Loss =  0.00047724321484565735\n",
            "Batch Training Loss =  0.00047312487731687725\n",
            "Batch Training Loss =  0.0004585968272294849\n",
            "Batch Training Loss =  0.0004554776824079454\n",
            "Batch Training Loss =  0.0004202114068903029\n",
            "Batch Training Loss =  0.00047832506243139505\n",
            "Batch Training Loss =  0.00045535474782809615\n",
            "Batch Training Loss =  0.0004389095411170274\n",
            "Batch Training Loss =  0.0005070626502856612\n",
            "Batch Training Loss =  0.000455709348898381\n",
            "Batch Training Loss =  0.00042059607221744955\n",
            "Batch Training Loss =  0.0004623514832928777\n",
            "Batch Training Loss =  0.00047493184683844447\n",
            "Batch Training Loss =  0.00045093471999280155\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0004478124901652336\n",
            "Batch Training Loss =  0.0004653768555726856\n",
            "Batch Training Loss =  0.0004157809598837048\n",
            "Batch Training Loss =  0.0004497037152759731\n",
            "Batch Training Loss =  0.00045437042717821896\n",
            "Batch Training Loss =  0.00041175642400048673\n",
            "Batch Training Loss =  0.00047503295354545116\n",
            "Batch Training Loss =  0.00044377471203915775\n",
            "Batch Training Loss =  0.00045358960051089525\n",
            "Batch Training Loss =  0.00043755368096753955\n",
            "Batch Training Loss =  0.000420036434661597\n",
            "Batch Training Loss =  0.00045018293894827366\n",
            "Batch Training Loss =  0.00045722807408310473\n",
            "Batch Training Loss =  0.0005092360079288483\n",
            "Batch Training Loss =  0.0004352955147624016\n",
            "Batch Training Loss =  0.0004397050943225622\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.00043823226587846875\n",
            "Batch Training Loss =  0.0004481577198021114\n",
            "Batch Training Loss =  0.00047051944420672953\n",
            "Batch Training Loss =  0.0004210268089082092\n",
            "Batch Training Loss =  0.0004099836223758757\n",
            "Batch Training Loss =  0.0004080650978721678\n",
            "Batch Training Loss =  0.00043829387868754566\n",
            "Batch Training Loss =  0.00043457731953822076\n",
            "Batch Training Loss =  0.00045470931218005717\n",
            "Batch Training Loss =  0.00044586238800548017\n",
            "Batch Training Loss =  0.000444652367150411\n",
            "Batch Training Loss =  0.00039702444337308407\n",
            "Batch Training Loss =  0.000426915823481977\n",
            "Batch Training Loss =  0.0004149690212216228\n",
            "Batch Training Loss =  0.00043938218732364476\n",
            "Batch Training Loss =  0.00042357546044513583\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0004274830862414092\n",
            "Batch Training Loss =  0.00047398274182341993\n",
            "Batch Training Loss =  0.0004281486617401242\n",
            "Batch Training Loss =  0.0004204344004392624\n",
            "Batch Training Loss =  0.0004150537424720824\n",
            "Batch Training Loss =  0.0004065851971972734\n",
            "Batch Training Loss =  0.0003921183233615011\n",
            "Batch Training Loss =  0.0004171293112449348\n",
            "Batch Training Loss =  0.0004271691432222724\n",
            "Batch Training Loss =  0.0004131021269131452\n",
            "Batch Training Loss =  0.0004078797937836498\n",
            "Batch Training Loss =  0.0004306230985093862\n",
            "Batch Training Loss =  0.0004137950309086591\n",
            "Batch Training Loss =  0.0004129138251300901\n",
            "Batch Training Loss =  0.00041144833085127175\n",
            "Batch Training Loss =  0.0004083448147866875\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.00039627085789106786\n",
            "Batch Training Loss =  0.000426983431680128\n",
            "Batch Training Loss =  0.0004369549569673836\n",
            "Batch Training Loss =  0.0003860282595269382\n",
            "Batch Training Loss =  0.00041948474245145917\n",
            "Batch Training Loss =  0.0004038393381051719\n",
            "Batch Training Loss =  0.00039127946365624666\n",
            "Batch Training Loss =  0.0004132297763135284\n",
            "Batch Training Loss =  0.00039423012640327215\n",
            "Batch Training Loss =  0.00041148829041048884\n",
            "Batch Training Loss =  0.000381873338483274\n",
            "Batch Training Loss =  0.00038382216007448733\n",
            "Batch Training Loss =  0.0003901418240275234\n",
            "Batch Training Loss =  0.0003881137818098068\n",
            "Batch Training Loss =  0.00041769014205783606\n",
            "Batch Training Loss =  0.00039304353413172066\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.00039158191066235304\n",
            "Batch Training Loss =  0.00040865811752155423\n",
            "Batch Training Loss =  0.0004116837808396667\n",
            "Batch Training Loss =  0.00039194748387672007\n",
            "Batch Training Loss =  0.00041336085996590555\n",
            "Batch Training Loss =  0.0003921364259440452\n",
            "Batch Training Loss =  0.00040429667569696903\n",
            "Batch Training Loss =  0.00037320173578336835\n",
            "Batch Training Loss =  0.00039358672802336514\n",
            "Batch Training Loss =  0.0004139128723181784\n",
            "Batch Training Loss =  0.0003757863596547395\n",
            "Batch Training Loss =  0.0003885541809722781\n",
            "Batch Training Loss =  0.00039811793249100447\n",
            "Batch Training Loss =  0.0003765359288081527\n",
            "Batch Training Loss =  0.000384602346457541\n",
            "Batch Training Loss =  0.0003831534122582525\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0003760128456633538\n",
            "Batch Training Loss =  0.00040026777423918247\n",
            "Batch Training Loss =  0.0003805881424341351\n",
            "Batch Training Loss =  0.0003704613191075623\n",
            "Batch Training Loss =  0.00039121127338148654\n",
            "Batch Training Loss =  0.00035781561746262014\n",
            "Batch Training Loss =  0.00037636165507137775\n",
            "Batch Training Loss =  0.0004097115888725966\n",
            "Batch Training Loss =  0.0003972098638769239\n",
            "Batch Training Loss =  0.00038643262814730406\n",
            "Batch Training Loss =  0.00036859800457023084\n",
            "Batch Training Loss =  0.00039481837302446365\n",
            "Batch Training Loss =  0.0003513162664603442\n",
            "Batch Training Loss =  0.0003707490104716271\n",
            "Batch Training Loss =  0.00036741659278050065\n",
            "Batch Training Loss =  0.00038289278745651245\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0003608645929489285\n",
            "Batch Training Loss =  0.0003473280230537057\n",
            "Batch Training Loss =  0.0003866494807880372\n",
            "Batch Training Loss =  0.0003889140207320452\n",
            "Batch Training Loss =  0.00037209532456472516\n",
            "Batch Training Loss =  0.00038176565431058407\n",
            "Batch Training Loss =  0.00037968833930790424\n",
            "Batch Training Loss =  0.0003633064916357398\n",
            "Batch Training Loss =  0.0004015148733742535\n",
            "Batch Training Loss =  0.0003645295510068536\n",
            "Batch Training Loss =  0.0003661718510556966\n",
            "Batch Training Loss =  0.00036971838562749326\n",
            "Batch Training Loss =  0.00031884823692962527\n",
            "Batch Training Loss =  0.0003724402922671288\n",
            "Batch Training Loss =  0.0003689252189360559\n",
            "Batch Training Loss =  0.000346977001754567\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.00037784138112328947\n",
            "Batch Training Loss =  0.00035324683994986117\n",
            "Batch Training Loss =  0.00035349701647646725\n",
            "Batch Training Loss =  0.0003776277299039066\n",
            "Batch Training Loss =  0.0003892398381140083\n",
            "Batch Training Loss =  0.0003840770514216274\n",
            "Batch Training Loss =  0.00034812770900316536\n",
            "Batch Training Loss =  0.0003603009390644729\n",
            "Batch Training Loss =  0.00039098429260775447\n",
            "Batch Training Loss =  0.0003351001359988004\n",
            "Batch Training Loss =  0.00036091834772378206\n",
            "Batch Training Loss =  0.00035479009966365993\n",
            "Batch Training Loss =  0.0003354906802996993\n",
            "Batch Training Loss =  0.0003465718764346093\n",
            "Batch Training Loss =  0.00033910994534380734\n",
            "Batch Training Loss =  0.0003506709763314575\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.000361837592208758\n",
            "Batch Training Loss =  0.00035245809704065323\n",
            "Batch Training Loss =  0.00033652427373453975\n",
            "Batch Training Loss =  0.0003341359260957688\n",
            "Batch Training Loss =  0.00034486662480048835\n",
            "Batch Training Loss =  0.0003445613256189972\n",
            "Batch Training Loss =  0.0003334607172291726\n",
            "Batch Training Loss =  0.0003239937941543758\n",
            "Batch Training Loss =  0.0003616304602473974\n",
            "Batch Training Loss =  0.00038068380672484636\n",
            "Batch Training Loss =  0.00035125960130244493\n",
            "Batch Training Loss =  0.0003453415702097118\n",
            "Batch Training Loss =  0.0003337873495183885\n",
            "Batch Training Loss =  0.00034803111338987947\n",
            "Batch Training Loss =  0.0003709027078002691\n",
            "Batch Training Loss =  0.00033065848401747644\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0003511959803290665\n",
            "Batch Training Loss =  0.00033997982973232865\n",
            "Batch Training Loss =  0.0003403552691452205\n",
            "Batch Training Loss =  0.0003445297188591212\n",
            "Batch Training Loss =  0.0003273150941822678\n",
            "Batch Training Loss =  0.000360956066288054\n",
            "Batch Training Loss =  0.00032707329955883324\n",
            "Batch Training Loss =  0.0003346254234202206\n",
            "Batch Training Loss =  0.0003339987015351653\n",
            "Batch Training Loss =  0.00035401323111727834\n",
            "Batch Training Loss =  0.00032950565218925476\n",
            "Batch Training Loss =  0.0003408208431210369\n",
            "Batch Training Loss =  0.00032472319435328245\n",
            "Batch Training Loss =  0.000337395555106923\n",
            "Batch Training Loss =  0.00035597840906120837\n",
            "Batch Training Loss =  0.000346872431691736\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0003329525934532285\n",
            "Batch Training Loss =  0.00033816107315942645\n",
            "Batch Training Loss =  0.00032018867204897106\n",
            "Batch Training Loss =  0.00035119641688652337\n",
            "Batch Training Loss =  0.00035764925996772945\n",
            "Batch Training Loss =  0.00033048554905690253\n",
            "Batch Training Loss =  0.000310588045977056\n",
            "Batch Training Loss =  0.00031716807279735804\n",
            "Batch Training Loss =  0.00031417858554050326\n",
            "Batch Training Loss =  0.00033744395477697253\n",
            "Batch Training Loss =  0.0003307504812255502\n",
            "Batch Training Loss =  0.0003140540502499789\n",
            "Batch Training Loss =  0.00035560320247896016\n",
            "Batch Training Loss =  0.0003477565187495202\n",
            "Batch Training Loss =  0.0003407795447856188\n",
            "Batch Training Loss =  0.00031924794893711805\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.00031828598002903163\n",
            "Batch Training Loss =  0.00035269808722659945\n",
            "Batch Training Loss =  0.0003433000238146633\n",
            "Batch Training Loss =  0.00030382268596440554\n",
            "Batch Training Loss =  0.0003299976815469563\n",
            "Batch Training Loss =  0.0003282467951066792\n",
            "Batch Training Loss =  0.0003185584500897676\n",
            "Batch Training Loss =  0.00034231573226861656\n",
            "Batch Training Loss =  0.0003198921331204474\n",
            "Batch Training Loss =  0.00032960649696178734\n",
            "Batch Training Loss =  0.00030054693343117833\n",
            "Batch Training Loss =  0.0003090697282459587\n",
            "Batch Training Loss =  0.0003053333202842623\n",
            "Batch Training Loss =  0.0002897988888435066\n",
            "Batch Training Loss =  0.0003166285459883511\n",
            "Batch Training Loss =  0.0003272262110840529\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.00030972345848567784\n",
            "Batch Training Loss =  0.0003300583048257977\n",
            "Batch Training Loss =  0.00030727090779691935\n",
            "Batch Training Loss =  0.00033528890344314277\n",
            "Batch Training Loss =  0.00031205741106532514\n",
            "Batch Training Loss =  0.0003074418636970222\n",
            "Batch Training Loss =  0.0003379157860763371\n",
            "Batch Training Loss =  0.0003020330041181296\n",
            "Batch Training Loss =  0.0003280954551883042\n",
            "Batch Training Loss =  0.00029529910534620285\n",
            "Batch Training Loss =  0.0003095820138696581\n",
            "Batch Training Loss =  0.0003098040178883821\n",
            "Batch Training Loss =  0.00031592269078828394\n",
            "Batch Training Loss =  0.00032643534359522164\n",
            "Batch Training Loss =  0.0003075191634707153\n",
            "Batch Training Loss =  0.00029538015951402485\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0003053295658901334\n",
            "Batch Training Loss =  0.00030760717345401645\n",
            "Batch Training Loss =  0.00031491112895309925\n",
            "Batch Training Loss =  0.00030782094108872116\n",
            "Batch Training Loss =  0.0003220741346012801\n",
            "Batch Training Loss =  0.0003068677324336022\n",
            "Batch Training Loss =  0.0003252277383580804\n",
            "Batch Training Loss =  0.00031049715471453965\n",
            "Batch Training Loss =  0.0003045783087145537\n",
            "Batch Training Loss =  0.0003071032406296581\n",
            "Batch Training Loss =  0.000315503915771842\n",
            "Batch Training Loss =  0.0002939862897619605\n",
            "Batch Training Loss =  0.00028596597258001566\n",
            "Batch Training Loss =  0.0002801957307383418\n",
            "Batch Training Loss =  0.0003006394545082003\n",
            "Batch Training Loss =  0.0002813126193359494\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0003247575368732214\n",
            "Batch Training Loss =  0.00031815512920729816\n",
            "Batch Training Loss =  0.00029726006323471665\n",
            "Batch Training Loss =  0.00029833585722371936\n",
            "Batch Training Loss =  0.00030295451870188117\n",
            "Batch Training Loss =  0.0002908945025410503\n",
            "Batch Training Loss =  0.0002799137437250465\n",
            "Batch Training Loss =  0.0002762043150141835\n",
            "Batch Training Loss =  0.00030046291067264974\n",
            "Batch Training Loss =  0.00034173508174717426\n",
            "Batch Training Loss =  0.00029001961229369044\n",
            "Batch Training Loss =  0.00031062596826814115\n",
            "Batch Training Loss =  0.00029412293224595487\n",
            "Batch Training Loss =  0.00028927490347996354\n",
            "Batch Training Loss =  0.00028255331562832\n",
            "Batch Training Loss =  0.0002881340042222291\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0002961555146612227\n",
            "Batch Training Loss =  0.000281406391877681\n",
            "Batch Training Loss =  0.0002788832352962345\n",
            "Batch Training Loss =  0.00028674444183707237\n",
            "Batch Training Loss =  0.0003013884706888348\n",
            "Batch Training Loss =  0.0002993829839397222\n",
            "Batch Training Loss =  0.0002889480674639344\n",
            "Batch Training Loss =  0.0002951428177766502\n",
            "Batch Training Loss =  0.0002880985557567328\n",
            "Batch Training Loss =  0.0002924160799011588\n",
            "Batch Training Loss =  0.00027708758716471493\n",
            "Batch Training Loss =  0.0002939806436188519\n",
            "Batch Training Loss =  0.0002923093852587044\n",
            "Batch Training Loss =  0.00030165593489073217\n",
            "Batch Training Loss =  0.0002741699863690883\n",
            "Batch Training Loss =  0.00028886430663987994\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0002983951708301902\n",
            "Batch Training Loss =  0.00030089091160334647\n",
            "Batch Training Loss =  0.00029521630494855344\n",
            "Batch Training Loss =  0.00026743198395706713\n",
            "Batch Training Loss =  0.0002803914248943329\n",
            "Batch Training Loss =  0.0002864723792299628\n",
            "Batch Training Loss =  0.0002874686324503273\n",
            "Batch Training Loss =  0.0002836752391885966\n",
            "Batch Training Loss =  0.00026414592866785824\n",
            "Batch Training Loss =  0.00029842936783097684\n",
            "Batch Training Loss =  0.0002729968982748687\n",
            "Batch Training Loss =  0.00027163303457200527\n",
            "Batch Training Loss =  0.00026080667157657444\n",
            "Batch Training Loss =  0.00028886948712170124\n",
            "Batch Training Loss =  0.0002887207374442369\n",
            "Batch Training Loss =  0.00028895269497297704\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0002869689778890461\n",
            "Batch Training Loss =  0.0002886054280679673\n",
            "Batch Training Loss =  0.00028094559093005955\n",
            "Batch Training Loss =  0.0002626250497996807\n",
            "Batch Training Loss =  0.0002612950629554689\n",
            "Batch Training Loss =  0.0003019684227183461\n",
            "Batch Training Loss =  0.00028464870410971344\n",
            "Batch Training Loss =  0.00028945179656147957\n",
            "Batch Training Loss =  0.00029344952781684697\n",
            "Batch Training Loss =  0.0002718541363719851\n",
            "Batch Training Loss =  0.00028180726803839207\n",
            "Batch Training Loss =  0.00027667960966937244\n",
            "Batch Training Loss =  0.0002826797717716545\n",
            "Batch Training Loss =  0.00027223999495618045\n",
            "Batch Training Loss =  0.00025694220676086843\n",
            "Batch Training Loss =  0.0002599197905510664\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0002820545923896134\n",
            "Batch Training Loss =  0.0002604651963338256\n",
            "Batch Training Loss =  0.0002575850230641663\n",
            "Batch Training Loss =  0.000263709865976125\n",
            "Batch Training Loss =  0.00027321718516759574\n",
            "Batch Training Loss =  0.000272061355644837\n",
            "Batch Training Loss =  0.0002835328341461718\n",
            "Batch Training Loss =  0.0002672558184713125\n",
            "Batch Training Loss =  0.0002708826505113393\n",
            "Batch Training Loss =  0.00027637777384370565\n",
            "Batch Training Loss =  0.00026443981914781034\n",
            "Batch Training Loss =  0.0002810934092849493\n",
            "Batch Training Loss =  0.0002648871159180999\n",
            "Batch Training Loss =  0.0002845242852345109\n",
            "Batch Training Loss =  0.00026161977439187467\n",
            "Batch Training Loss =  0.000270785327302292\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0002668417291715741\n",
            "Batch Training Loss =  0.00026497815269976854\n",
            "Batch Training Loss =  0.00025868244119919837\n",
            "Batch Training Loss =  0.0002441057440591976\n",
            "Batch Training Loss =  0.0002643111802171916\n",
            "Batch Training Loss =  0.00024949171347543597\n",
            "Batch Training Loss =  0.00026815381716005504\n",
            "Batch Training Loss =  0.0002933196956291795\n",
            "Batch Training Loss =  0.00025813368847593665\n",
            "Batch Training Loss =  0.0002741913194768131\n",
            "Batch Training Loss =  0.0002693077549338341\n",
            "Batch Training Loss =  0.0002806376724038273\n",
            "Batch Training Loss =  0.0002770062710624188\n",
            "Batch Training Loss =  0.00025893605197779834\n",
            "Batch Training Loss =  0.0002553423692006618\n",
            "Batch Training Loss =  0.0002999338321387768\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0002557002007961273\n",
            "Batch Training Loss =  0.0002606178750284016\n",
            "Batch Training Loss =  0.00025349410134367645\n",
            "Batch Training Loss =  0.00026032867026515305\n",
            "Batch Training Loss =  0.00026612519286572933\n",
            "Batch Training Loss =  0.00024172030680347234\n",
            "Batch Training Loss =  0.0002615925041027367\n",
            "Batch Training Loss =  0.0002670857065822929\n",
            "Batch Training Loss =  0.00026462788810022175\n",
            "Batch Training Loss =  0.0002666311338543892\n",
            "Batch Training Loss =  0.0002868721494451165\n",
            "Batch Training Loss =  0.00025461832410655916\n",
            "Batch Training Loss =  0.0002659551682882011\n",
            "Batch Training Loss =  0.00024698380730114877\n",
            "Batch Training Loss =  0.0002790443832054734\n",
            "Batch Training Loss =  0.0002641252940520644\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.00025569743593223393\n",
            "Batch Training Loss =  0.00024383742129430175\n",
            "Batch Training Loss =  0.0002541418361943215\n",
            "Batch Training Loss =  0.00025420906604267657\n",
            "Batch Training Loss =  0.00024861565907485783\n",
            "Batch Training Loss =  0.0002566325420048088\n",
            "Batch Training Loss =  0.00026155359228141606\n",
            "Batch Training Loss =  0.0002600938605610281\n",
            "Batch Training Loss =  0.0002803016977850348\n",
            "Batch Training Loss =  0.0002599522704258561\n",
            "Batch Training Loss =  0.0002488335594534874\n",
            "Batch Training Loss =  0.00023488533042836934\n",
            "Batch Training Loss =  0.000250492274062708\n",
            "Batch Training Loss =  0.0002485720324330032\n",
            "Batch Training Loss =  0.0002366046974202618\n",
            "Batch Training Loss =  0.00025065953377634287\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.000253730162512511\n",
            "Batch Training Loss =  0.0002310019772266969\n",
            "Batch Training Loss =  0.0002510937047190964\n",
            "Batch Training Loss =  0.00026703975163400173\n",
            "Batch Training Loss =  0.0002553867525421083\n",
            "Batch Training Loss =  0.00025912790442816913\n",
            "Batch Training Loss =  0.00023045623674988747\n",
            "Batch Training Loss =  0.0002453232300467789\n",
            "Batch Training Loss =  0.0002481136179994792\n",
            "Batch Training Loss =  0.00023791134299244732\n",
            "Batch Training Loss =  0.0002536415704526007\n",
            "Batch Training Loss =  0.0002473356726113707\n",
            "Batch Training Loss =  0.00024291675072163343\n",
            "Batch Training Loss =  0.00023903651162981987\n",
            "Batch Training Loss =  0.0002587436465546489\n",
            "Batch Training Loss =  0.00025766732869669795\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.00022842416365165263\n",
            "Batch Training Loss =  0.0002464090648572892\n",
            "Batch Training Loss =  0.00024787188158370554\n",
            "Batch Training Loss =  0.00024269566347356886\n",
            "Batch Training Loss =  0.00023792398860678077\n",
            "Batch Training Loss =  0.0002358051569899544\n",
            "Batch Training Loss =  0.000248878204729408\n",
            "Batch Training Loss =  0.00023375735327135772\n",
            "Batch Training Loss =  0.00024772071628831327\n",
            "Batch Training Loss =  0.00025654392084106803\n",
            "Batch Training Loss =  0.000255661754636094\n",
            "Batch Training Loss =  0.00024281728838104755\n",
            "Batch Training Loss =  0.00023814548330847174\n",
            "Batch Training Loss =  0.0002406552666798234\n",
            "Batch Training Loss =  0.00024518504505977035\n",
            "Batch Training Loss =  0.00026110842009074986\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.00024440628476440907\n",
            "Batch Training Loss =  0.0002290992415510118\n",
            "Batch Training Loss =  0.0002501347044017166\n",
            "Batch Training Loss =  0.000247267191298306\n",
            "Batch Training Loss =  0.00023434852482751012\n",
            "Batch Training Loss =  0.00023095514916349202\n",
            "Batch Training Loss =  0.0002446548896841705\n",
            "Batch Training Loss =  0.00023474176123272628\n",
            "Batch Training Loss =  0.00023109930043574423\n",
            "Batch Training Loss =  0.0002480499679222703\n",
            "Batch Training Loss =  0.00025016997824423015\n",
            "Batch Training Loss =  0.0002605043991934508\n",
            "Batch Training Loss =  0.0002418394578853622\n",
            "Batch Training Loss =  0.000262747285887599\n",
            "Batch Training Loss =  0.00024044136807788163\n",
            "Batch Training Loss =  0.0002349643036723137\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.00023726056679151952\n",
            "Batch Training Loss =  0.00022309375344775617\n",
            "Batch Training Loss =  0.00024317618226632476\n",
            "Batch Training Loss =  0.00024493620730936527\n",
            "Batch Training Loss =  0.00023604408488608897\n",
            "Batch Training Loss =  0.00024229568953160197\n",
            "Batch Training Loss =  0.00022459428873844445\n",
            "Batch Training Loss =  0.00022173720935825258\n",
            "Batch Training Loss =  0.00023228628560900688\n",
            "Batch Training Loss =  0.00024543842300772667\n",
            "Batch Training Loss =  0.00023347829119302332\n",
            "Batch Training Loss =  0.00022912117128726095\n",
            "Batch Training Loss =  0.00023202542797662318\n",
            "Batch Training Loss =  0.0002564597234595567\n",
            "Batch Training Loss =  0.00023290807439479977\n",
            "Batch Training Loss =  0.00024090016086120158\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.00022899190662428737\n",
            "Batch Training Loss =  0.00023521394177805632\n",
            "Batch Training Loss =  0.00024329446023330092\n",
            "Batch Training Loss =  0.00023667822824791074\n",
            "Batch Training Loss =  0.00023893118486739695\n",
            "Batch Training Loss =  0.00022569706197828054\n",
            "Batch Training Loss =  0.00022893640561960638\n",
            "Batch Training Loss =  0.00023237834102474153\n",
            "Batch Training Loss =  0.00022422010079026222\n",
            "Batch Training Loss =  0.00024110543017741293\n",
            "Batch Training Loss =  0.0002348522248212248\n",
            "Batch Training Loss =  0.0002235386346001178\n",
            "Batch Training Loss =  0.0002348937268834561\n",
            "Batch Training Loss =  0.00023677220451645553\n",
            "Batch Training Loss =  0.0002232640254078433\n",
            "Batch Training Loss =  0.00022580295626539737\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.00023248603974934667\n",
            "Batch Training Loss =  0.0002278556057717651\n",
            "Batch Training Loss =  0.0002240839967271313\n",
            "Batch Training Loss =  0.00023043066903483123\n",
            "Batch Training Loss =  0.00022090846323408186\n",
            "Batch Training Loss =  0.0002288693212904036\n",
            "Batch Training Loss =  0.0002370803995290771\n",
            "Batch Training Loss =  0.00022661472030449659\n",
            "Batch Training Loss =  0.00024357026268262416\n",
            "Batch Training Loss =  0.00024015369126573205\n",
            "Batch Training Loss =  0.00023833914019633085\n",
            "Batch Training Loss =  0.000228147953748703\n",
            "Batch Training Loss =  0.00021646948880515993\n",
            "Batch Training Loss =  0.00021352498151827604\n",
            "Batch Training Loss =  0.00021900809952057898\n",
            "Batch Training Loss =  0.00021406944142654538\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.00021308782743290067\n",
            "Batch Training Loss =  0.00021735636983066797\n",
            "Batch Training Loss =  0.00022649513266514987\n",
            "Batch Training Loss =  0.000225243202294223\n",
            "Batch Training Loss =  0.00021881272550672293\n",
            "Batch Training Loss =  0.00021921943698544055\n",
            "Batch Training Loss =  0.00022182127577252686\n",
            "Batch Training Loss =  0.00024238062906078994\n",
            "Batch Training Loss =  0.00023027483257465065\n",
            "Batch Training Loss =  0.00021939896396361291\n",
            "Batch Training Loss =  0.00022023648489266634\n",
            "Batch Training Loss =  0.00021012956858612597\n",
            "Batch Training Loss =  0.0002187353529734537\n",
            "Batch Training Loss =  0.0002238088782178238\n",
            "Batch Training Loss =  0.00023131273337639868\n",
            "Batch Training Loss =  0.00022843634360469878\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0002149629726773128\n",
            "Batch Training Loss =  0.00021046919573564082\n",
            "Batch Training Loss =  0.00021754932822659612\n",
            "Batch Training Loss =  0.0002342331426916644\n",
            "Batch Training Loss =  0.00022119797358755022\n",
            "Batch Training Loss =  0.0002317050821147859\n",
            "Batch Training Loss =  0.00021792999177705497\n",
            "Batch Training Loss =  0.0002215670101577416\n",
            "Batch Training Loss =  0.00023468602739740163\n",
            "Batch Training Loss =  0.00021291995653882623\n",
            "Batch Training Loss =  0.00022014252317603678\n",
            "Batch Training Loss =  0.00021291790471877903\n",
            "Batch Training Loss =  0.00021127132640685886\n",
            "Batch Training Loss =  0.0002210110251326114\n",
            "Batch Training Loss =  0.0002217652217950672\n",
            "Batch Training Loss =  0.00021943150204606354\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0002184794284403324\n",
            "Batch Training Loss =  0.00022577313939109445\n",
            "Batch Training Loss =  0.0002095139934681356\n",
            "Batch Training Loss =  0.0002138037234544754\n",
            "Batch Training Loss =  0.00020150450291112065\n",
            "Batch Training Loss =  0.00021651387214660645\n",
            "Batch Training Loss =  0.0002011231699725613\n",
            "Batch Training Loss =  0.00021116607240401208\n",
            "Batch Training Loss =  0.00022496242308989167\n",
            "Batch Training Loss =  0.00020515317737590522\n",
            "Batch Training Loss =  0.00023085958673618734\n",
            "Batch Training Loss =  0.00021019736595917493\n",
            "Batch Training Loss =  0.00021783082047477365\n",
            "Batch Training Loss =  0.00022261192498262972\n",
            "Batch Training Loss =  0.00021603058848995715\n",
            "Batch Training Loss =  0.00021761124662589282\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.00020880292868241668\n",
            "Batch Training Loss =  0.00020392797887325287\n",
            "Batch Training Loss =  0.00022122221707832068\n",
            "Batch Training Loss =  0.00022386621276382357\n",
            "Batch Training Loss =  0.00020927769946865737\n",
            "Batch Training Loss =  0.00020635411783587188\n",
            "Batch Training Loss =  0.00023212999803945422\n",
            "Batch Training Loss =  0.00020145290181972086\n",
            "Batch Training Loss =  0.00021158286835998297\n",
            "Batch Training Loss =  0.00022421698668040335\n",
            "Batch Training Loss =  0.00021935086988378316\n",
            "Batch Training Loss =  0.00022270064800977707\n",
            "Batch Training Loss =  0.00019679350953083485\n",
            "Batch Training Loss =  0.0002171811502194032\n",
            "Batch Training Loss =  0.00021747432765550911\n",
            "Batch Training Loss =  0.00019938978948630393\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.00019523350056260824\n",
            "Batch Training Loss =  0.00018967823416460305\n",
            "Batch Training Loss =  0.00020531643531285226\n",
            "Batch Training Loss =  0.00021466206817422062\n",
            "Batch Training Loss =  0.0001968017459148541\n",
            "Batch Training Loss =  0.00020342573407106102\n",
            "Batch Training Loss =  0.00019824322953354567\n",
            "Batch Training Loss =  0.00020890627638436854\n",
            "Batch Training Loss =  0.0002277400199091062\n",
            "Batch Training Loss =  0.00020782591309398413\n",
            "Batch Training Loss =  0.00021869767806492746\n",
            "Batch Training Loss =  0.00020627875346690416\n",
            "Batch Training Loss =  0.0002245226496597752\n",
            "Batch Training Loss =  0.0002265754883410409\n",
            "Batch Training Loss =  0.0002513915242161602\n",
            "Batch Training Loss =  0.00020612675871234387\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.00021115462004672736\n",
            "Batch Training Loss =  0.00020540291734505445\n",
            "Batch Training Loss =  0.0002167408529203385\n",
            "Batch Training Loss =  0.0002222749899374321\n",
            "Batch Training Loss =  0.00021896499674767256\n",
            "Batch Training Loss =  0.00021266371186356992\n",
            "Batch Training Loss =  0.0002280748449265957\n",
            "Batch Training Loss =  0.0002169748768210411\n",
            "Batch Training Loss =  0.00023064957349561155\n",
            "Batch Training Loss =  0.00021447725885082036\n",
            "Batch Training Loss =  0.0002090332272928208\n",
            "Batch Training Loss =  0.0002212217077612877\n",
            "Batch Training Loss =  0.00023836058971937746\n",
            "Batch Training Loss =  0.00021197748719714582\n",
            "Batch Training Loss =  0.0002150509535567835\n",
            "Batch Training Loss =  0.0002080327831208706\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.00021881940483581275\n",
            "Batch Training Loss =  0.0002231492253486067\n",
            "Batch Training Loss =  0.00023796723689883947\n",
            "Batch Training Loss =  0.0002170492080040276\n",
            "Batch Training Loss =  0.00021996362193021923\n",
            "Batch Training Loss =  0.00021504450705833733\n",
            "Batch Training Loss =  0.00022614806948695332\n",
            "Batch Training Loss =  0.00022769736824557185\n",
            "Batch Training Loss =  0.00022543947852682322\n",
            "Batch Training Loss =  0.00021558708976954222\n",
            "Batch Training Loss =  0.0002368376008234918\n",
            "Batch Training Loss =  0.0002242056798422709\n",
            "Batch Training Loss =  0.0002245338400825858\n",
            "Batch Training Loss =  0.00022296076349448413\n",
            "Batch Training Loss =  0.00025254584033973515\n",
            "Batch Training Loss =  0.00021488616766873747\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.00022609869483858347\n",
            "Batch Training Loss =  0.00022699784312862903\n",
            "Batch Training Loss =  0.00022397648717742413\n",
            "Batch Training Loss =  0.00022131839068606496\n",
            "Batch Training Loss =  0.00022498529870063066\n",
            "Batch Training Loss =  0.00022955726308282465\n",
            "Batch Training Loss =  0.00023448053980246186\n",
            "Batch Training Loss =  0.00023517312365584075\n",
            "Batch Training Loss =  0.00022137683117762208\n",
            "Batch Training Loss =  0.00023525436699856073\n",
            "Batch Training Loss =  0.00023252681421581656\n",
            "Batch Training Loss =  0.00023593429068569094\n",
            "Batch Training Loss =  0.00022209796588867903\n",
            "Batch Training Loss =  0.00023302806948777288\n",
            "Batch Training Loss =  0.00023848304408602417\n",
            "Batch Training Loss =  0.00023372014402411878\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0002383788669249043\n",
            "Batch Training Loss =  0.000270991149591282\n",
            "Batch Training Loss =  0.0002216630382463336\n",
            "Batch Training Loss =  0.00022535529569722712\n",
            "Batch Training Loss =  0.0002429525920888409\n",
            "Batch Training Loss =  0.0002352367009734735\n",
            "Batch Training Loss =  0.00024122426111716777\n",
            "Batch Training Loss =  0.0002250678080599755\n",
            "Batch Training Loss =  0.00022919921320863068\n",
            "Batch Training Loss =  0.00023722165497019887\n",
            "Batch Training Loss =  0.00024147382646333426\n",
            "Batch Training Loss =  0.00024310231674462557\n",
            "Batch Training Loss =  0.00022325706959236413\n",
            "Batch Training Loss =  0.00023451920424122363\n",
            "Batch Training Loss =  0.00024343434779439121\n",
            "Batch Training Loss =  0.00023745326325297356\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.00024348357692360878\n",
            "Batch Training Loss =  0.0002424251870252192\n",
            "Batch Training Loss =  0.00022701213310938329\n",
            "Batch Training Loss =  0.0002422998659312725\n",
            "Batch Training Loss =  0.00023223301104735583\n",
            "Batch Training Loss =  0.0002280401240568608\n",
            "Batch Training Loss =  0.0002514199586585164\n",
            "Batch Training Loss =  0.00023338879691436887\n",
            "Batch Training Loss =  0.00023506976140197366\n",
            "Batch Training Loss =  0.000241741887293756\n",
            "Batch Training Loss =  0.00024425843730568886\n",
            "Batch Training Loss =  0.0002646643843036145\n",
            "Batch Training Loss =  0.00025435187853872776\n",
            "Batch Training Loss =  0.000267842406174168\n",
            "Batch Training Loss =  0.00024173450947273523\n",
            "Batch Training Loss =  0.00024782412219792604\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.00023341485939454287\n",
            "Batch Training Loss =  0.00024858186952769756\n",
            "Batch Training Loss =  0.0002622550819069147\n",
            "Batch Training Loss =  0.0002507077297195792\n",
            "Batch Training Loss =  0.00024535920238122344\n",
            "Batch Training Loss =  0.00024401537666562945\n",
            "Batch Training Loss =  0.0002529449702706188\n",
            "Batch Training Loss =  0.00023679282458033413\n",
            "Batch Training Loss =  0.00024558973382227123\n",
            "Batch Training Loss =  0.00025119297788478434\n",
            "Batch Training Loss =  0.0002492167113814503\n",
            "Batch Training Loss =  0.0002489264006726444\n",
            "Batch Training Loss =  0.00024978118017315865\n",
            "Batch Training Loss =  0.0002445545687805861\n",
            "Batch Training Loss =  0.00026329129468649626\n",
            "Batch Training Loss =  0.00025743714650161564\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0002506006567273289\n",
            "Batch Training Loss =  0.00023929269809741527\n",
            "Batch Training Loss =  0.00024820337421260774\n",
            "Batch Training Loss =  0.00023792586580384523\n",
            "Batch Training Loss =  0.0002501219278201461\n",
            "Batch Training Loss =  0.0002472967025823891\n",
            "Batch Training Loss =  0.0002652080438565463\n",
            "Batch Training Loss =  0.0002694874128792435\n",
            "Batch Training Loss =  0.0002555505488999188\n",
            "Batch Training Loss =  0.0002539547858759761\n",
            "Batch Training Loss =  0.0002662431506905705\n",
            "Batch Training Loss =  0.00026062942924909294\n",
            "Batch Training Loss =  0.00026347144739702344\n",
            "Batch Training Loss =  0.00025937482132576406\n",
            "Batch Training Loss =  0.0002469402679707855\n",
            "Batch Training Loss =  0.00028647531871683896\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0002517897228244692\n",
            "Batch Training Loss =  0.0002519361732993275\n",
            "Batch Training Loss =  0.0002516280801501125\n",
            "Batch Training Loss =  0.00027500043506734073\n",
            "Batch Training Loss =  0.0002652942785061896\n",
            "Batch Training Loss =  0.0002532276266720146\n",
            "Batch Training Loss =  0.00026679079746827483\n",
            "Batch Training Loss =  0.00025349805946461856\n",
            "Batch Training Loss =  0.0002680103643797338\n",
            "Batch Training Loss =  0.0002718846080824733\n",
            "Batch Training Loss =  0.0002610309747979045\n",
            "Batch Training Loss =  0.00026974803768098354\n",
            "Batch Training Loss =  0.0002666214422788471\n",
            "Batch Training Loss =  0.00025237040244974196\n",
            "Batch Training Loss =  0.0002784346288535744\n",
            "Batch Training Loss =  0.00025465863291174173\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0002655411371961236\n",
            "Batch Training Loss =  0.00027084664907306433\n",
            "Batch Training Loss =  0.0002709458058234304\n",
            "Batch Training Loss =  0.0002954217488877475\n",
            "Batch Training Loss =  0.00027543038595467806\n",
            "Batch Training Loss =  0.0002723228535614908\n",
            "Batch Training Loss =  0.0002695646253414452\n",
            "Batch Training Loss =  0.0002791580045595765\n",
            "Batch Training Loss =  0.0002516507520340383\n",
            "Batch Training Loss =  0.0002833800681401044\n",
            "Batch Training Loss =  0.0002590616059023887\n",
            "Batch Training Loss =  0.0002646033535711467\n",
            "Batch Training Loss =  0.00028471427503973246\n",
            "Batch Training Loss =  0.0002685774816200137\n",
            "Batch Training Loss =  0.0002513088984414935\n",
            "Batch Training Loss =  0.0002782510709948838\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0002614721015561372\n",
            "Batch Training Loss =  0.0002801594673655927\n",
            "Batch Training Loss =  0.00029313855338841677\n",
            "Batch Training Loss =  0.00028278451645746827\n",
            "Batch Training Loss =  0.0002730685519054532\n",
            "Batch Training Loss =  0.00027324730763211846\n",
            "Batch Training Loss =  0.0002747937978710979\n",
            "Batch Training Loss =  0.0002687335363589227\n",
            "Batch Training Loss =  0.0002598718856461346\n",
            "Batch Training Loss =  0.00026101930416189134\n",
            "Batch Training Loss =  0.00027672763098962605\n",
            "Batch Training Loss =  0.00028406549245119095\n",
            "Batch Training Loss =  0.00028947016107849777\n",
            "Batch Training Loss =  0.0003013668756466359\n",
            "Batch Training Loss =  0.00029768317472189665\n",
            "Batch Training Loss =  0.00028041136101819575\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0002745186211541295\n",
            "Batch Training Loss =  0.00030339890508912504\n",
            "Batch Training Loss =  0.0002883929992094636\n",
            "Batch Training Loss =  0.00028842309257015586\n",
            "Batch Training Loss =  0.0002771642175503075\n",
            "Batch Training Loss =  0.000277066370472312\n",
            "Batch Training Loss =  0.00028662747354246676\n",
            "Batch Training Loss =  0.00027331290766596794\n",
            "Batch Training Loss =  0.0002759421186055988\n",
            "Batch Training Loss =  0.00028784264577552676\n",
            "Batch Training Loss =  0.0002816279884427786\n",
            "Batch Training Loss =  0.00029469080618582666\n",
            "Batch Training Loss =  0.0002722704375628382\n",
            "Batch Training Loss =  0.00028773100348189473\n",
            "Batch Training Loss =  0.0002750300045590848\n",
            "Batch Training Loss =  0.00030084908939898014\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0003082356124650687\n",
            "Batch Training Loss =  0.0002845932904165238\n",
            "Batch Training Loss =  0.0002763776865322143\n",
            "Batch Training Loss =  0.0003017052367795259\n",
            "Batch Training Loss =  0.00028710023616440594\n",
            "Batch Training Loss =  0.0002807815617416054\n",
            "Batch Training Loss =  0.0002906247682403773\n",
            "Batch Training Loss =  0.00027026585303246975\n",
            "Batch Training Loss =  0.00029949593590572476\n",
            "Batch Training Loss =  0.0002918849640991539\n",
            "Batch Training Loss =  0.0003251317248214036\n",
            "Batch Training Loss =  0.0002803148818202317\n",
            "Batch Training Loss =  0.0002989688073284924\n",
            "Batch Training Loss =  0.00029897925560362637\n",
            "Batch Training Loss =  0.0002941648126579821\n",
            "Batch Training Loss =  0.0003020363219548017\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.00029106298461556435\n",
            "Batch Training Loss =  0.00028002538601867855\n",
            "Batch Training Loss =  0.0003027784696314484\n",
            "Batch Training Loss =  0.0002975689712911844\n",
            "Batch Training Loss =  0.0002930430055130273\n",
            "Batch Training Loss =  0.0002870185999199748\n",
            "Batch Training Loss =  0.0003147635725326836\n",
            "Batch Training Loss =  0.0003167787508573383\n",
            "Batch Training Loss =  0.00031991591094993055\n",
            "Batch Training Loss =  0.00029691026429645717\n",
            "Batch Training Loss =  0.0002980969147756696\n",
            "Batch Training Loss =  0.0003118396271020174\n",
            "Batch Training Loss =  0.0003150820848532021\n",
            "Batch Training Loss =  0.0002965409366879612\n",
            "Batch Training Loss =  0.00029784030630253255\n",
            "Batch Training Loss =  0.00029209459898993373\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.00030920145218260586\n",
            "Batch Training Loss =  0.00028540813946165144\n",
            "Batch Training Loss =  0.000298032711725682\n",
            "Batch Training Loss =  0.00030265937675721943\n",
            "Batch Training Loss =  0.0003090327081736177\n",
            "Batch Training Loss =  0.0003104135685134679\n",
            "Batch Training Loss =  0.00030881891143508255\n",
            "Batch Training Loss =  0.0003073070547543466\n",
            "Batch Training Loss =  0.000294272176688537\n",
            "Batch Training Loss =  0.0002988690393976867\n",
            "Batch Training Loss =  0.0002899105311371386\n",
            "Batch Training Loss =  0.0003175574529450387\n",
            "Batch Training Loss =  0.00030247471295297146\n",
            "Batch Training Loss =  0.0003221403167117387\n",
            "Batch Training Loss =  0.00030405810684897006\n",
            "Batch Training Loss =  0.0003157607570756227\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.00030525311012752354\n",
            "Batch Training Loss =  0.00030300236539915204\n",
            "Batch Training Loss =  0.00031432133982889354\n",
            "Batch Training Loss =  0.00030544085893779993\n",
            "Batch Training Loss =  0.00031376248807646334\n",
            "Batch Training Loss =  0.00033224650542251766\n",
            "Batch Training Loss =  0.00031612187740392983\n",
            "Batch Training Loss =  0.0003080818278249353\n",
            "Batch Training Loss =  0.00032053067116066813\n",
            "Batch Training Loss =  0.000326918059727177\n",
            "Batch Training Loss =  0.00030932153458707035\n",
            "Batch Training Loss =  0.0003292141482234001\n",
            "Batch Training Loss =  0.0003173239529132843\n",
            "Batch Training Loss =  0.0003112050471827388\n",
            "Batch Training Loss =  0.00029432313749566674\n",
            "Batch Training Loss =  0.0003189274575561285\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0003159394836984575\n",
            "Batch Training Loss =  0.0003051178646273911\n",
            "Batch Training Loss =  0.0003140457847621292\n",
            "Batch Training Loss =  0.0003270038578193635\n",
            "Batch Training Loss =  0.0003341595584060997\n",
            "Batch Training Loss =  0.0003303547273389995\n",
            "Batch Training Loss =  0.000344322354067117\n",
            "Batch Training Loss =  0.0003300064417999238\n",
            "Batch Training Loss =  0.0002973695518448949\n",
            "Batch Training Loss =  0.00032387845567427576\n",
            "Batch Training Loss =  0.00032119109528139234\n",
            "Batch Training Loss =  0.00032179857953451574\n",
            "Batch Training Loss =  0.0003391245845705271\n",
            "Batch Training Loss =  0.0003214099269825965\n",
            "Batch Training Loss =  0.0003019795403815806\n",
            "Batch Training Loss =  0.0003266593557782471\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0003124132053926587\n",
            "Batch Training Loss =  0.00031582496012561023\n",
            "Batch Training Loss =  0.00034639949444681406\n",
            "Batch Training Loss =  0.0003214164753444493\n",
            "Batch Training Loss =  0.0003323971468489617\n",
            "Batch Training Loss =  0.0003146103408653289\n",
            "Batch Training Loss =  0.00032874225871637464\n",
            "Batch Training Loss =  0.0003361402195878327\n",
            "Batch Training Loss =  0.0003206460678484291\n",
            "Batch Training Loss =  0.00033335990156047046\n",
            "Batch Training Loss =  0.0003380324924364686\n",
            "Batch Training Loss =  0.0003248769498895854\n",
            "Batch Training Loss =  0.00033362751128152013\n",
            "Batch Training Loss =  0.0003373758227098733\n",
            "Batch Training Loss =  0.0003214251482859254\n",
            "Batch Training Loss =  0.00031661000684835017\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0003281364915892482\n",
            "Batch Training Loss =  0.0003409952623769641\n",
            "Batch Training Loss =  0.00031400175066664815\n",
            "Batch Training Loss =  0.0003333476197440177\n",
            "Batch Training Loss =  0.0003264812985435128\n",
            "Batch Training Loss =  0.0003498857549857348\n",
            "Batch Training Loss =  0.00034876741119660437\n",
            "Batch Training Loss =  0.00032523396657779813\n",
            "Batch Training Loss =  0.0003298973897472024\n",
            "Batch Training Loss =  0.00032783980714157224\n",
            "Batch Training Loss =  0.00034361195866949856\n",
            "Batch Training Loss =  0.0003327933372929692\n",
            "Batch Training Loss =  0.0003272636968176812\n",
            "Batch Training Loss =  0.00034799997229129076\n",
            "Batch Training Loss =  0.00034208325087092817\n",
            "Batch Training Loss =  0.0003366676392033696\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.00035176362143829465\n",
            "Batch Training Loss =  0.0003231446899008006\n",
            "Batch Training Loss =  0.00033472705399617553\n",
            "Batch Training Loss =  0.0003360752307344228\n",
            "Batch Training Loss =  0.0003347932070028037\n",
            "Batch Training Loss =  0.00033551984233781695\n",
            "Batch Training Loss =  0.0003493071999400854\n",
            "Batch Training Loss =  0.0003412669466342777\n",
            "Batch Training Loss =  0.000355665193637833\n",
            "Batch Training Loss =  0.00038585197762586176\n",
            "Batch Training Loss =  0.0003404315502848476\n",
            "Batch Training Loss =  0.0003406628966331482\n",
            "Batch Training Loss =  0.0003302876721136272\n",
            "Batch Training Loss =  0.0003231627051718533\n",
            "Batch Training Loss =  0.00032798125175759196\n",
            "Batch Training Loss =  0.0003319067764095962\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0003300842363387346\n",
            "Batch Training Loss =  0.00035383019712753594\n",
            "Batch Training Loss =  0.00034034642158076167\n",
            "Batch Training Loss =  0.0003389358753338456\n",
            "Batch Training Loss =  0.00032420229399576783\n",
            "Batch Training Loss =  0.0003557966265361756\n",
            "Batch Training Loss =  0.0003526759392116219\n",
            "Batch Training Loss =  0.00035151338670402765\n",
            "Batch Training Loss =  0.00033866718877106905\n",
            "Batch Training Loss =  0.00035290716914460063\n",
            "Batch Training Loss =  0.0003597293689381331\n",
            "Batch Training Loss =  0.0003576776653062552\n",
            "Batch Training Loss =  0.000362196791684255\n",
            "Batch Training Loss =  0.0003570794651750475\n",
            "Batch Training Loss =  0.00035936274798586965\n",
            "Batch Training Loss =  0.0003558772150427103\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0003718700900208205\n",
            "Batch Training Loss =  0.000355557247530669\n",
            "Batch Training Loss =  0.00036168628139421344\n",
            "Batch Training Loss =  0.0003762360429391265\n",
            "Batch Training Loss =  0.0003490528033580631\n",
            "Batch Training Loss =  0.0003547445230651647\n",
            "Batch Training Loss =  0.0003540244360920042\n",
            "Batch Training Loss =  0.0003581979835871607\n",
            "Batch Training Loss =  0.00034381463774479926\n",
            "Batch Training Loss =  0.00035192116047255695\n",
            "Batch Training Loss =  0.0003565747174434364\n",
            "Batch Training Loss =  0.00035049885627813637\n",
            "Batch Training Loss =  0.00034935071016661823\n",
            "Batch Training Loss =  0.00037110288394615054\n",
            "Batch Training Loss =  0.0003499888989608735\n",
            "Batch Training Loss =  0.0003586511011235416\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.00034546985989436507\n",
            "Batch Training Loss =  0.0003601978824008256\n",
            "Batch Training Loss =  0.0003485608904156834\n",
            "Batch Training Loss =  0.0003742211265489459\n",
            "Batch Training Loss =  0.00035719838342629373\n",
            "Batch Training Loss =  0.00036332046147435904\n",
            "Batch Training Loss =  0.0003640152281150222\n",
            "Batch Training Loss =  0.00036346184788271785\n",
            "Batch Training Loss =  0.0003574823203962296\n",
            "Batch Training Loss =  0.00035677183768711984\n",
            "Batch Training Loss =  0.0003570230910554528\n",
            "Batch Training Loss =  0.000391368375858292\n",
            "Batch Training Loss =  0.00038412187132053077\n",
            "Batch Training Loss =  0.00037245434941723943\n",
            "Batch Training Loss =  0.00034494217834435403\n",
            "Batch Training Loss =  0.0003651233855634928\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0003615273453760892\n",
            "Batch Training Loss =  0.0003777843085117638\n",
            "Batch Training Loss =  0.00036329225986264646\n",
            "Batch Training Loss =  0.00038378153112716973\n",
            "Batch Training Loss =  0.00037675275234505534\n",
            "Batch Training Loss =  0.0003671165613923222\n",
            "Batch Training Loss =  0.0003635157772805542\n",
            "Batch Training Loss =  0.00037156196776777506\n",
            "Batch Training Loss =  0.0003746150468941778\n",
            "Batch Training Loss =  0.0003704078262671828\n",
            "Batch Training Loss =  0.00037333983345888555\n",
            "Batch Training Loss =  0.00037025901838205755\n",
            "Batch Training Loss =  0.0003540229517966509\n",
            "Batch Training Loss =  0.00037751722265966237\n",
            "Batch Training Loss =  0.00036729834391735494\n",
            "Batch Training Loss =  0.0003736159997060895\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0003726946597453207\n",
            "Batch Training Loss =  0.00039929201011545956\n",
            "Batch Training Loss =  0.0003787243040278554\n",
            "Batch Training Loss =  0.0003641604271251708\n",
            "Batch Training Loss =  0.00037545853410847485\n",
            "Batch Training Loss =  0.0003769950708374381\n",
            "Batch Training Loss =  0.00037335275555960834\n",
            "Batch Training Loss =  0.0003808924520853907\n",
            "Batch Training Loss =  0.0003558899916242808\n",
            "Batch Training Loss =  0.0003760207036975771\n",
            "Batch Training Loss =  0.0003799796395469457\n",
            "Batch Training Loss =  0.0003647030971478671\n",
            "Batch Training Loss =  0.0003788305330090225\n",
            "Batch Training Loss =  0.0003587042447179556\n",
            "Batch Training Loss =  0.00037938394234515727\n",
            "Batch Training Loss =  0.00038853962905704975\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0003785287553910166\n",
            "Batch Training Loss =  0.00036940479185432196\n",
            "Batch Training Loss =  0.0004003046778962016\n",
            "Batch Training Loss =  0.0003780014521908015\n",
            "Batch Training Loss =  0.0004086405679117888\n",
            "Batch Training Loss =  0.000380882149329409\n",
            "Batch Training Loss =  0.00036580543383024633\n",
            "Batch Training Loss =  0.000391467590816319\n",
            "Batch Training Loss =  0.00038665320607833564\n",
            "Batch Training Loss =  0.0003751738113351166\n",
            "Batch Training Loss =  0.000376961164874956\n",
            "Batch Training Loss =  0.000383820355636999\n",
            "Batch Training Loss =  0.00038184772711247206\n",
            "Batch Training Loss =  0.00037831044755876064\n",
            "Batch Training Loss =  0.0003788342291954905\n",
            "Batch Training Loss =  0.00039028364699333906\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0003949223319068551\n",
            "Batch Training Loss =  0.00039165414636954665\n",
            "Batch Training Loss =  0.00038347888039425015\n",
            "Batch Training Loss =  0.00037472962867468596\n",
            "Batch Training Loss =  0.0004105887492187321\n",
            "Batch Training Loss =  0.00039362444658763707\n",
            "Batch Training Loss =  0.0003804446314461529\n",
            "Batch Training Loss =  0.0004196361405774951\n",
            "Batch Training Loss =  0.00039756056503392756\n",
            "Batch Training Loss =  0.0003920065937563777\n",
            "Batch Training Loss =  0.0003651767910923809\n",
            "Batch Training Loss =  0.0003792658681049943\n",
            "Batch Training Loss =  0.000400956894736737\n",
            "Batch Training Loss =  0.0003792953793890774\n",
            "Batch Training Loss =  0.00036897812969982624\n",
            "Batch Training Loss =  0.00039999771979637444\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0003885436162818223\n",
            "Batch Training Loss =  0.00040989808621816337\n",
            "Batch Training Loss =  0.0004069534770678729\n",
            "Batch Training Loss =  0.00038517083157785237\n",
            "Batch Training Loss =  0.0003855772374663502\n",
            "Batch Training Loss =  0.00038067318382672966\n",
            "Batch Training Loss =  0.0003940044844057411\n",
            "Batch Training Loss =  0.00038282963214442134\n",
            "Batch Training Loss =  0.0003901280870195478\n",
            "Batch Training Loss =  0.00039200542960315943\n",
            "Batch Training Loss =  0.00039314947207458317\n",
            "Batch Training Loss =  0.0003774654178414494\n",
            "Batch Training Loss =  0.00039275578455999494\n",
            "Batch Training Loss =  0.0004072735318914056\n",
            "Batch Training Loss =  0.00041241382132284343\n",
            "Batch Training Loss =  0.000399282987928018\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0004020298656541854\n",
            "Batch Training Loss =  0.0003925166674889624\n",
            "Batch Training Loss =  0.0003897222049999982\n",
            "Batch Training Loss =  0.000394241651520133\n",
            "Batch Training Loss =  0.0003875430265907198\n",
            "Batch Training Loss =  0.0003839518176391721\n",
            "Batch Training Loss =  0.0004143101687077433\n",
            "Batch Training Loss =  0.00041650619823485613\n",
            "Batch Training Loss =  0.0004012225253973156\n",
            "Batch Training Loss =  0.00041252298979088664\n",
            "Batch Training Loss =  0.0003962455957662314\n",
            "Batch Training Loss =  0.0004086324479430914\n",
            "Batch Training Loss =  0.0004186955338809639\n",
            "Batch Training Loss =  0.0004441167111508548\n",
            "Batch Training Loss =  0.0004018469189759344\n",
            "Batch Training Loss =  0.0003806292952504009\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.00041519885417073965\n",
            "Batch Training Loss =  0.000427361810579896\n",
            "Batch Training Loss =  0.0003814065712504089\n",
            "Batch Training Loss =  0.0003960983012802899\n",
            "Batch Training Loss =  0.00042007878073491156\n",
            "Batch Training Loss =  0.0004341583698987961\n",
            "Batch Training Loss =  0.0004052354779560119\n",
            "Batch Training Loss =  0.00040002274909056723\n",
            "Batch Training Loss =  0.0004065026587340981\n",
            "Batch Training Loss =  0.00041061057709157467\n",
            "Batch Training Loss =  0.00040641785017214715\n",
            "Batch Training Loss =  0.0003980377805419266\n",
            "Batch Training Loss =  0.00039903976721689105\n",
            "Batch Training Loss =  0.00041023280937224627\n",
            "Batch Training Loss =  0.00039940429269336164\n",
            "Batch Training Loss =  0.00042061423300765455\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0004042295040562749\n",
            "Batch Training Loss =  0.0004016318416688591\n",
            "Batch Training Loss =  0.0004123089602217078\n",
            "Batch Training Loss =  0.0003876029804814607\n",
            "Batch Training Loss =  0.0004185427096672356\n",
            "Batch Training Loss =  0.00044053030433133245\n",
            "Batch Training Loss =  0.0004240843409206718\n",
            "Batch Training Loss =  0.0004092403396498412\n",
            "Batch Training Loss =  0.00040190829895436764\n",
            "Batch Training Loss =  0.0004154460912104696\n",
            "Batch Training Loss =  0.00041712421807460487\n",
            "Batch Training Loss =  0.0004410948313307017\n",
            "Batch Training Loss =  0.0004127445863559842\n",
            "Batch Training Loss =  0.0004204079450573772\n",
            "Batch Training Loss =  0.0004057206097058952\n",
            "Batch Training Loss =  0.00042879057582467794\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.00041547714499756694\n",
            "Batch Training Loss =  0.00042644463246688247\n",
            "Batch Training Loss =  0.0004286845796741545\n",
            "Batch Training Loss =  0.0004129904555156827\n",
            "Batch Training Loss =  0.000413356872741133\n",
            "Batch Training Loss =  0.0004471425781957805\n",
            "Batch Training Loss =  0.0004012045683339238\n",
            "Batch Training Loss =  0.0004123169928789139\n",
            "Batch Training Loss =  0.0004207793972454965\n",
            "Batch Training Loss =  0.0004310064250603318\n",
            "Batch Training Loss =  0.00039673259016126394\n",
            "Batch Training Loss =  0.0004334700934123248\n",
            "Batch Training Loss =  0.00042698532342910767\n",
            "Batch Training Loss =  0.0004204756405670196\n",
            "Batch Training Loss =  0.0004399013123475015\n",
            "Batch Training Loss =  0.00041341877658851445\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0004057621699757874\n",
            "Batch Training Loss =  0.0004399028839543462\n",
            "Batch Training Loss =  0.00044586366857402027\n",
            "Batch Training Loss =  0.00042033049976453185\n",
            "Batch Training Loss =  0.00043302669655531645\n",
            "Batch Training Loss =  0.00041143473936244845\n",
            "Batch Training Loss =  0.00042660950566641986\n",
            "Batch Training Loss =  0.00041503951069898903\n",
            "Batch Training Loss =  0.0004432564601302147\n",
            "Batch Training Loss =  0.000436292466474697\n",
            "Batch Training Loss =  0.00041133732884190977\n",
            "Batch Training Loss =  0.00043180346256121993\n",
            "Batch Training Loss =  0.0004133431939408183\n",
            "Batch Training Loss =  0.00042677376768551767\n",
            "Batch Training Loss =  0.0004283245652914047\n",
            "Batch Training Loss =  0.0004318903374951333\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0004046825924888253\n",
            "Batch Training Loss =  0.00039360474329441786\n",
            "Batch Training Loss =  0.00044848129618912935\n",
            "Batch Training Loss =  0.00045649759704247117\n",
            "Batch Training Loss =  0.00045424344716593623\n",
            "Batch Training Loss =  0.0004430677799973637\n",
            "Batch Training Loss =  0.00043312940397299826\n",
            "Batch Training Loss =  0.00043544586515054107\n",
            "Batch Training Loss =  0.00042370561277493834\n",
            "Batch Training Loss =  0.00042878519161604345\n",
            "Batch Training Loss =  0.0004151751345489174\n",
            "Batch Training Loss =  0.00045672894339077175\n",
            "Batch Training Loss =  0.00043686653953045607\n",
            "Batch Training Loss =  0.000408704683650285\n",
            "Batch Training Loss =  0.0004467648977879435\n",
            "Batch Training Loss =  0.000431013380875811\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.000461203686427325\n",
            "Batch Training Loss =  0.00043157392065040767\n",
            "Batch Training Loss =  0.000428204977652058\n",
            "Batch Training Loss =  0.00043243676191195846\n",
            "Batch Training Loss =  0.00044109264854341745\n",
            "Batch Training Loss =  0.00042139223660342395\n",
            "Batch Training Loss =  0.00043581670615822077\n",
            "Batch Training Loss =  0.0004210754414089024\n",
            "Batch Training Loss =  0.00043955829460173845\n",
            "Batch Training Loss =  0.0004559197404887527\n",
            "Batch Training Loss =  0.00043477900908328593\n",
            "Batch Training Loss =  0.00043051139800809324\n",
            "Batch Training Loss =  0.0004348898946773261\n",
            "Batch Training Loss =  0.00043415441177785397\n",
            "Batch Training Loss =  0.00045760697685182095\n",
            "Batch Training Loss =  0.00043339733383618295\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0004175436042714864\n",
            "Batch Training Loss =  0.00042759664938785136\n",
            "Batch Training Loss =  0.00043854487012140453\n",
            "Batch Training Loss =  0.00045698098256252706\n",
            "Batch Training Loss =  0.0004656397504732013\n",
            "Batch Training Loss =  0.00043382280273362994\n",
            "Batch Training Loss =  0.00046415854012593627\n",
            "Batch Training Loss =  0.0004340418672654778\n",
            "Batch Training Loss =  0.00045575827243737876\n",
            "Batch Training Loss =  0.00045680106268264353\n",
            "Batch Training Loss =  0.0004624372813850641\n",
            "Batch Training Loss =  0.0004318454011809081\n",
            "Batch Training Loss =  0.0004519558569882065\n",
            "Batch Training Loss =  0.00043732309131883085\n",
            "Batch Training Loss =  0.0004326502967160195\n",
            "Batch Training Loss =  0.0004357387078925967\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.00045151732047088444\n",
            "Batch Training Loss =  0.000443972647190094\n",
            "Batch Training Loss =  0.0004331451200414449\n",
            "Batch Training Loss =  0.0004646854940801859\n",
            "Batch Training Loss =  0.0004469130653887987\n",
            "Batch Training Loss =  0.0004576952778734267\n",
            "Batch Training Loss =  0.0004305001348257065\n",
            "Batch Training Loss =  0.0004453263827599585\n",
            "Batch Training Loss =  0.00044723949395120144\n",
            "Batch Training Loss =  0.00045627131476067007\n",
            "Batch Training Loss =  0.0004426230734679848\n",
            "Batch Training Loss =  0.00045310091809369624\n",
            "Batch Training Loss =  0.000494328502099961\n",
            "Batch Training Loss =  0.0004518209316302091\n",
            "Batch Training Loss =  0.00046243969700299203\n",
            "Batch Training Loss =  0.0004322200547903776\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.000457275309599936\n",
            "Batch Training Loss =  0.00044888374395668507\n",
            "Batch Training Loss =  0.00047113228356465697\n",
            "Batch Training Loss =  0.00047460998757742345\n",
            "Batch Training Loss =  0.00043806835310533643\n",
            "Batch Training Loss =  0.00045935052912682295\n",
            "Batch Training Loss =  0.00045328857959248126\n",
            "Batch Training Loss =  0.0004472451691981405\n",
            "Batch Training Loss =  0.0004668208712246269\n",
            "Batch Training Loss =  0.0004502316296566278\n",
            "Batch Training Loss =  0.000440167379565537\n",
            "Batch Training Loss =  0.00046309965546242893\n",
            "Batch Training Loss =  0.00044203465222381055\n",
            "Batch Training Loss =  0.00046434582327492535\n",
            "Batch Training Loss =  0.0004458384937606752\n",
            "Batch Training Loss =  0.00045765668619424105\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.00043683534022420645\n",
            "Batch Training Loss =  0.00046226000995375216\n",
            "Batch Training Loss =  0.000460171140730381\n",
            "Batch Training Loss =  0.00046663597458973527\n",
            "Batch Training Loss =  0.0004554454644676298\n",
            "Batch Training Loss =  0.00043529694085009396\n",
            "Batch Training Loss =  0.0004644177097361535\n",
            "Batch Training Loss =  0.0004647630557883531\n",
            "Batch Training Loss =  0.0004835956497117877\n",
            "Batch Training Loss =  0.00045718380715698004\n",
            "Batch Training Loss =  0.00047974957851693034\n",
            "Batch Training Loss =  0.00048363173846155405\n",
            "Batch Training Loss =  0.0004629103350453079\n",
            "Batch Training Loss =  0.00047311800881288946\n",
            "Batch Training Loss =  0.0004870656121056527\n",
            "Batch Training Loss =  0.00047067750710994005\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0004672168579418212\n",
            "Batch Training Loss =  0.00045742240035906434\n",
            "Batch Training Loss =  0.00045451533515006304\n",
            "Batch Training Loss =  0.00047266052570194006\n",
            "Batch Training Loss =  0.00045397813664749265\n",
            "Batch Training Loss =  0.00045473285717889667\n",
            "Batch Training Loss =  0.0004419668111950159\n",
            "Batch Training Loss =  0.0004886672832071781\n",
            "Batch Training Loss =  0.00047927931882441044\n",
            "Batch Training Loss =  0.00046569734695367515\n",
            "Batch Training Loss =  0.0004548027063719928\n",
            "Batch Training Loss =  0.0004935297183692455\n",
            "Batch Training Loss =  0.0004581543034873903\n",
            "Batch Training Loss =  0.0004763581673614681\n",
            "Batch Training Loss =  0.0004728688800241798\n",
            "Batch Training Loss =  0.0004660832055378705\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0004826610966119915\n",
            "Batch Training Loss =  0.00047043856466189027\n",
            "Batch Training Loss =  0.00048709477414377034\n",
            "Batch Training Loss =  0.00046188742271624506\n",
            "Batch Training Loss =  0.00047826606896705925\n",
            "Batch Training Loss =  0.00046457056305371225\n",
            "Batch Training Loss =  0.0004794113046955317\n",
            "Batch Training Loss =  0.0004743464232888073\n",
            "Batch Training Loss =  0.0004694281378760934\n",
            "Batch Training Loss =  0.0004828310338780284\n",
            "Batch Training Loss =  0.00046645826660096645\n",
            "Batch Training Loss =  0.00047669574269093573\n",
            "Batch Training Loss =  0.00044812686974182725\n",
            "Batch Training Loss =  0.0005040188552811742\n",
            "Batch Training Loss =  0.0004801056638825685\n",
            "Batch Training Loss =  0.0004693789524026215\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.00046125135850161314\n",
            "Batch Training Loss =  0.0004664492153096944\n",
            "Batch Training Loss =  0.0004703136219177395\n",
            "Batch Training Loss =  0.0004955855547450483\n",
            "Batch Training Loss =  0.0004917866899631917\n",
            "Batch Training Loss =  0.00047478172928094864\n",
            "Batch Training Loss =  0.0004776141722686589\n",
            "Batch Training Loss =  0.000490301288664341\n",
            "Batch Training Loss =  0.0004994500777684152\n",
            "Batch Training Loss =  0.0004627723537851125\n",
            "Batch Training Loss =  0.0004770094819832593\n",
            "Batch Training Loss =  0.00048766614054329693\n",
            "Batch Training Loss =  0.000487261830130592\n",
            "Batch Training Loss =  0.00046855132677592337\n",
            "Batch Training Loss =  0.00046403578016906977\n",
            "Batch Training Loss =  0.0004631823976524174\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0004797915753442794\n",
            "Batch Training Loss =  0.00047360319877043366\n",
            "Batch Training Loss =  0.0004760115116368979\n",
            "Batch Training Loss =  0.00047973747132346034\n",
            "Batch Training Loss =  0.000499337911605835\n",
            "Batch Training Loss =  0.0004748629289679229\n",
            "Batch Training Loss =  0.0004784505581483245\n",
            "Batch Training Loss =  0.00048252378473989666\n",
            "Batch Training Loss =  0.0004997322685085237\n",
            "Batch Training Loss =  0.0004987029242329299\n",
            "Batch Training Loss =  0.0004952892195433378\n",
            "Batch Training Loss =  0.00047174119390547276\n",
            "Batch Training Loss =  0.0005150740616954863\n",
            "Batch Training Loss =  0.0005005025304853916\n",
            "Batch Training Loss =  0.00048411753959953785\n",
            "Batch Training Loss =  0.0004689281340688467\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.00048286799574270844\n",
            "Batch Training Loss =  0.0005227410583756864\n",
            "Batch Training Loss =  0.00046796994865871966\n",
            "Batch Training Loss =  0.0005013315239921212\n",
            "Batch Training Loss =  0.00045911147026345134\n",
            "Batch Training Loss =  0.0004916126490570605\n",
            "Batch Training Loss =  0.0004748926730826497\n",
            "Batch Training Loss =  0.00047317188000306487\n",
            "Batch Training Loss =  0.0005354189779609442\n",
            "Batch Training Loss =  0.0004856740415561944\n",
            "Batch Training Loss =  0.0004887484246864915\n",
            "Batch Training Loss =  0.0004859839682467282\n",
            "Batch Training Loss =  0.0004643617430701852\n",
            "Batch Training Loss =  0.0005253424169495702\n",
            "Batch Training Loss =  0.000507840421050787\n",
            "Batch Training Loss =  0.0004915765020996332\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0005181858432479203\n",
            "Batch Training Loss =  0.000493512605316937\n",
            "Batch Training Loss =  0.00048435755888931453\n",
            "Batch Training Loss =  0.0005228751106187701\n",
            "Batch Training Loss =  0.0005054784705862403\n",
            "Batch Training Loss =  0.0004837277519982308\n",
            "Batch Training Loss =  0.0005014936905354261\n",
            "Batch Training Loss =  0.0004904677625745535\n",
            "Batch Training Loss =  0.000505575560964644\n",
            "Batch Training Loss =  0.0004901000065729022\n",
            "Batch Training Loss =  0.0005087845493108034\n",
            "Batch Training Loss =  0.000483633135445416\n",
            "Batch Training Loss =  0.0004967954591847956\n",
            "Batch Training Loss =  0.0004754859837703407\n",
            "Batch Training Loss =  0.0005188462673686445\n",
            "Batch Training Loss =  0.0005071433843113482\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.000482354051200673\n",
            "Batch Training Loss =  0.0004987939610145986\n",
            "Batch Training Loss =  0.00048731151036918163\n",
            "Batch Training Loss =  0.00048310679267160594\n",
            "Batch Training Loss =  0.00048645681818015873\n",
            "Batch Training Loss =  0.0004716768744401634\n",
            "Batch Training Loss =  0.0005406001000665128\n",
            "Batch Training Loss =  0.0005217629950493574\n",
            "Batch Training Loss =  0.0004959615762345493\n",
            "Batch Training Loss =  0.0005124134477227926\n",
            "Batch Training Loss =  0.0004865260561928153\n",
            "Batch Training Loss =  0.0005025495775043964\n",
            "Batch Training Loss =  0.0005334339803084731\n",
            "Batch Training Loss =  0.0005009722081013024\n",
            "Batch Training Loss =  0.00049738516099751\n",
            "Batch Training Loss =  0.0005160535802133381\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0004815393767785281\n",
            "Batch Training Loss =  0.00048399201477877796\n",
            "Batch Training Loss =  0.0005175056867301464\n",
            "Batch Training Loss =  0.0004914166638627648\n",
            "Batch Training Loss =  0.0005051980260759592\n",
            "Batch Training Loss =  0.0005091289640404284\n",
            "Batch Training Loss =  0.0005221723113209009\n",
            "Batch Training Loss =  0.0005215569399297237\n",
            "Batch Training Loss =  0.0005191363161429763\n",
            "Batch Training Loss =  0.0005073680076748133\n",
            "Batch Training Loss =  0.0005183995235711336\n",
            "Batch Training Loss =  0.000525025010574609\n",
            "Batch Training Loss =  0.0005141962319612503\n",
            "Batch Training Loss =  0.0005008080042898655\n",
            "Batch Training Loss =  0.0005136877298355103\n",
            "Batch Training Loss =  0.0005134742241352797\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0005089276819489896\n",
            "Batch Training Loss =  0.00048751779831945896\n",
            "Batch Training Loss =  0.0005273330607451499\n",
            "Batch Training Loss =  0.00048322879592888057\n",
            "Batch Training Loss =  0.0004916931502521038\n",
            "Batch Training Loss =  0.0005578271811828017\n",
            "Batch Training Loss =  0.0004977518110536039\n",
            "Batch Training Loss =  0.0005143762100487947\n",
            "Batch Training Loss =  0.0005037373630329967\n",
            "Batch Training Loss =  0.0005303393118083477\n",
            "Batch Training Loss =  0.0005196282872930169\n",
            "Batch Training Loss =  0.0005069988546893001\n",
            "Batch Training Loss =  0.0005052236374467611\n",
            "Batch Training Loss =  0.0005469119059853256\n",
            "Batch Training Loss =  0.0005037875380367041\n",
            "Batch Training Loss =  0.000537460611667484\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0004974452313035727\n",
            "Batch Training Loss =  0.000506614800542593\n",
            "Batch Training Loss =  0.000523204100318253\n",
            "Batch Training Loss =  0.0005356557085178792\n",
            "Batch Training Loss =  0.000510130834300071\n",
            "Batch Training Loss =  0.0004903367371298373\n",
            "Batch Training Loss =  0.0005307047395035625\n",
            "Batch Training Loss =  0.0005065304576419294\n",
            "Batch Training Loss =  0.0005009372835047543\n",
            "Batch Training Loss =  0.0005316036404110491\n",
            "Batch Training Loss =  0.0005261871265247464\n",
            "Batch Training Loss =  0.0005063076969236135\n",
            "Batch Training Loss =  0.0005103088333271444\n",
            "Batch Training Loss =  0.000560749089345336\n",
            "Batch Training Loss =  0.000520191912073642\n",
            "Batch Training Loss =  0.0005133937229402363\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0005261190235614777\n",
            "Batch Training Loss =  0.0005140399443916976\n",
            "Batch Training Loss =  0.0005272739217616618\n",
            "Batch Training Loss =  0.000518613145686686\n",
            "Batch Training Loss =  0.0005251066759228706\n",
            "Batch Training Loss =  0.0005140432040207088\n",
            "Batch Training Loss =  0.0005292053683660924\n",
            "Batch Training Loss =  0.0004894131561741233\n",
            "Batch Training Loss =  0.0005382323288358748\n",
            "Batch Training Loss =  0.0005172383971512318\n",
            "Batch Training Loss =  0.0005123657174408436\n",
            "Batch Training Loss =  0.0005154110258445144\n",
            "Batch Training Loss =  0.0005082040443085134\n",
            "Batch Training Loss =  0.0005157181876711547\n",
            "Batch Training Loss =  0.000527952506672591\n",
            "Batch Training Loss =  0.0005375441396608949\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.000519650406204164\n",
            "Batch Training Loss =  0.0004873565339948982\n",
            "Batch Training Loss =  0.0004801394825335592\n",
            "Batch Training Loss =  0.0004776790738105774\n",
            "Batch Training Loss =  0.0004467203398235142\n",
            "Batch Training Loss =  0.00044935994083061814\n",
            "Batch Training Loss =  0.0005056980880908668\n",
            "Batch Training Loss =  0.0004708906926680356\n",
            "Batch Training Loss =  0.00044685640023089945\n",
            "Batch Training Loss =  0.00042854584171436727\n",
            "Batch Training Loss =  0.000416990602388978\n",
            "Batch Training Loss =  0.00041157874511554837\n",
            "Batch Training Loss =  0.0003965081996284425\n",
            "Batch Training Loss =  0.000408231804613024\n",
            "Batch Training Loss =  0.0004061591753270477\n",
            "Batch Training Loss =  0.00040303534478880465\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.00039864773862063885\n",
            "Batch Training Loss =  0.0003769673057831824\n",
            "Batch Training Loss =  0.00035802923957817256\n",
            "Batch Training Loss =  0.00036425076541490853\n",
            "Batch Training Loss =  0.00034709065221250057\n",
            "Batch Training Loss =  0.000350213231286034\n",
            "Batch Training Loss =  0.00035245344042778015\n",
            "Batch Training Loss =  0.00032809830736368895\n",
            "Batch Training Loss =  0.00036702334182336926\n",
            "Batch Training Loss =  0.0003184597590006888\n",
            "Batch Training Loss =  0.0003308371815364808\n",
            "Batch Training Loss =  0.00032418363844044507\n",
            "Batch Training Loss =  0.0003263235848862678\n",
            "Batch Training Loss =  0.0003237297059968114\n",
            "Batch Training Loss =  0.00031372677767649293\n",
            "Batch Training Loss =  0.0003035650297533721\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.00031160173239186406\n",
            "Batch Training Loss =  0.0002990875218529254\n",
            "Batch Training Loss =  0.0003042688185814768\n",
            "Batch Training Loss =  0.0002868302690330893\n",
            "Batch Training Loss =  0.00029094755882397294\n",
            "Batch Training Loss =  0.0002999897114932537\n",
            "Batch Training Loss =  0.0002752640575636178\n",
            "Batch Training Loss =  0.0002760728821158409\n",
            "Batch Training Loss =  0.00027869228506460786\n",
            "Batch Training Loss =  0.0002699262404348701\n",
            "Batch Training Loss =  0.0002635895798448473\n",
            "Batch Training Loss =  0.00026501945103518665\n",
            "Batch Training Loss =  0.0002673258713912219\n",
            "Batch Training Loss =  0.0002674386778380722\n",
            "Batch Training Loss =  0.0002547675103414804\n",
            "Batch Training Loss =  0.0002482766576576978\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0002542597067076713\n",
            "Batch Training Loss =  0.00025537729379720986\n",
            "Batch Training Loss =  0.00025017239386215806\n",
            "Batch Training Loss =  0.00024100398877635598\n",
            "Batch Training Loss =  0.00023238194989971817\n",
            "Batch Training Loss =  0.00024689047131687403\n",
            "Batch Training Loss =  0.00023745372891426086\n",
            "Batch Training Loss =  0.00024144648341462016\n",
            "Batch Training Loss =  0.00023553127539344132\n",
            "Batch Training Loss =  0.000227601412916556\n",
            "Batch Training Loss =  0.00022706406889483333\n",
            "Batch Training Loss =  0.00021567752992268652\n",
            "Batch Training Loss =  0.0002162421151297167\n",
            "Batch Training Loss =  0.00020670366939157248\n",
            "Batch Training Loss =  0.00021785139688290656\n",
            "Batch Training Loss =  0.00021227779507171363\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.00020910620514769107\n",
            "Batch Training Loss =  0.0002143029705621302\n",
            "Batch Training Loss =  0.000206600729143247\n",
            "Batch Training Loss =  0.00021705370454583317\n",
            "Batch Training Loss =  0.00021717598428949714\n",
            "Batch Training Loss =  0.0001969692821148783\n",
            "Batch Training Loss =  0.00020317686721682549\n",
            "Batch Training Loss =  0.00018772021576296538\n",
            "Batch Training Loss =  0.00019454244466032833\n",
            "Batch Training Loss =  0.00019927298126276582\n",
            "Batch Training Loss =  0.00020278430019970983\n",
            "Batch Training Loss =  0.0001835869043134153\n",
            "Batch Training Loss =  0.00019507369142957032\n",
            "Batch Training Loss =  0.00019979954231530428\n",
            "Batch Training Loss =  0.00019208934099879116\n",
            "Batch Training Loss =  0.00018734870536718518\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0001856837625382468\n",
            "Batch Training Loss =  0.00018925838230643421\n",
            "Batch Training Loss =  0.0001870536943897605\n",
            "Batch Training Loss =  0.0001813904964365065\n",
            "Batch Training Loss =  0.00018075220577884465\n",
            "Batch Training Loss =  0.00016994558973237872\n",
            "Batch Training Loss =  0.00018751839525066316\n",
            "Batch Training Loss =  0.00017464488337282091\n",
            "Batch Training Loss =  0.0001639409747440368\n",
            "Batch Training Loss =  0.00017692292749416083\n",
            "Batch Training Loss =  0.00017420583753846586\n",
            "Batch Training Loss =  0.00017041375394910574\n",
            "Batch Training Loss =  0.00017422751989215612\n",
            "Batch Training Loss =  0.00017021609528455883\n",
            "Batch Training Loss =  0.00016277216491289437\n",
            "Batch Training Loss =  0.00016471710114274174\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.00016739346028771251\n",
            "Batch Training Loss =  0.00016266018792521209\n",
            "Batch Training Loss =  0.00015857143444009125\n",
            "Batch Training Loss =  0.0001589478924870491\n",
            "Batch Training Loss =  0.00016687750758137554\n",
            "Batch Training Loss =  0.00015537047875113785\n",
            "Batch Training Loss =  0.00015511299716308713\n",
            "Batch Training Loss =  0.0001564335252624005\n",
            "Batch Training Loss =  0.00015947721840348095\n",
            "Batch Training Loss =  0.00016608941950835288\n",
            "Batch Training Loss =  0.00015411607455462217\n",
            "Batch Training Loss =  0.00014876312343403697\n",
            "Batch Training Loss =  0.0001485287066316232\n",
            "Batch Training Loss =  0.0001548599684610963\n",
            "Batch Training Loss =  0.00015390285989269614\n",
            "Batch Training Loss =  0.00014657659630756825\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.00014739608741365373\n",
            "Batch Training Loss =  0.00015652074944227934\n",
            "Batch Training Loss =  0.00014775636373087764\n",
            "Batch Training Loss =  0.00014237436698749661\n",
            "Batch Training Loss =  0.00014313396241050214\n",
            "Batch Training Loss =  0.0001458186306990683\n",
            "Batch Training Loss =  0.00014113746874500066\n",
            "Batch Training Loss =  0.00014294405991677195\n",
            "Batch Training Loss =  0.00014100843691267073\n",
            "Batch Training Loss =  0.00013441483315546066\n",
            "Batch Training Loss =  0.00013606649008579552\n",
            "Batch Training Loss =  0.00013554691395256668\n",
            "Batch Training Loss =  0.00013696476526092738\n",
            "Batch Training Loss =  0.00013791820674668998\n",
            "Batch Training Loss =  0.00013532020966522396\n",
            "Batch Training Loss =  0.00013348495122045279\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.000132848130306229\n",
            "Batch Training Loss =  0.00013002369087189436\n",
            "Batch Training Loss =  0.00013089139247313142\n",
            "Batch Training Loss =  0.00012686524132732302\n",
            "Batch Training Loss =  0.0001295899273827672\n",
            "Batch Training Loss =  0.00012809285544790328\n",
            "Batch Training Loss =  0.00013078399933874607\n",
            "Batch Training Loss =  0.00012989461538381875\n",
            "Batch Training Loss =  0.00013495999155566096\n",
            "Batch Training Loss =  0.00012727452849503607\n",
            "Batch Training Loss =  0.00012312683975324035\n",
            "Batch Training Loss =  0.00012851688370574266\n",
            "Batch Training Loss =  0.00012800675176549703\n",
            "Batch Training Loss =  0.0001216443270095624\n",
            "Batch Training Loss =  0.00012838529073633254\n",
            "Batch Training Loss =  0.00012431427603587508\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.00012467816122807562\n",
            "Batch Training Loss =  0.00012354650243651122\n",
            "Batch Training Loss =  0.00012683590466622263\n",
            "Batch Training Loss =  0.0001214989461004734\n",
            "Batch Training Loss =  0.0001165406865766272\n",
            "Batch Training Loss =  0.00011946081212954596\n",
            "Batch Training Loss =  0.00012329407036304474\n",
            "Batch Training Loss =  0.00011993737280135974\n",
            "Batch Training Loss =  0.00011757509491872042\n",
            "Batch Training Loss =  0.00011636361159617081\n",
            "Batch Training Loss =  0.00011630917288130149\n",
            "Batch Training Loss =  0.00011743931099772453\n",
            "Batch Training Loss =  0.00011422814714023843\n",
            "Batch Training Loss =  0.00011671944230329245\n",
            "Batch Training Loss =  0.00010865944204851985\n",
            "Batch Training Loss =  0.0001121161476476118\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.00011291942792013288\n",
            "Batch Training Loss =  0.00011391599400667474\n",
            "Batch Training Loss =  0.00010918178304564208\n",
            "Batch Training Loss =  0.000112569221528247\n",
            "Batch Training Loss =  0.00011410853767301887\n",
            "Batch Training Loss =  0.00011223740148125216\n",
            "Batch Training Loss =  0.00011552165233297274\n",
            "Batch Training Loss =  0.00011329795233905315\n",
            "Batch Training Loss =  0.00010631392797222361\n",
            "Batch Training Loss =  0.0001110005468945019\n",
            "Batch Training Loss =  0.00010633744386723265\n",
            "Batch Training Loss =  0.00010367013601353392\n",
            "Batch Training Loss =  0.0001104900729842484\n",
            "Batch Training Loss =  0.00010860439215321094\n",
            "Batch Training Loss =  0.00010634669160936028\n",
            "Batch Training Loss =  0.00010983696120092645\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.00011011053720721975\n",
            "Batch Training Loss =  0.00010845388896996155\n",
            "Batch Training Loss =  0.00010459966870257631\n",
            "Batch Training Loss =  0.00010165812273044139\n",
            "Batch Training Loss =  0.00010954113531624898\n",
            "Batch Training Loss =  0.00010183915583183989\n",
            "Batch Training Loss =  0.00010571478196652606\n",
            "Batch Training Loss =  9.622052311897278e-05\n",
            "Batch Training Loss =  0.00010274757369188592\n",
            "Batch Training Loss =  9.874984971247613e-05\n",
            "Batch Training Loss =  9.841103747021407e-05\n",
            "Batch Training Loss =  0.00010500948701519519\n",
            "Batch Training Loss =  9.758146188687533e-05\n",
            "Batch Training Loss =  9.60821271291934e-05\n",
            "Batch Training Loss =  0.00010043485963251442\n",
            "Batch Training Loss =  9.998918540077284e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0001010117048281245\n",
            "Batch Training Loss =  0.0001003728830255568\n",
            "Batch Training Loss =  9.957419388229027e-05\n",
            "Batch Training Loss =  9.42965125432238e-05\n",
            "Batch Training Loss =  9.659199713496491e-05\n",
            "Batch Training Loss =  9.725968993734568e-05\n",
            "Batch Training Loss =  9.691607556305826e-05\n",
            "Batch Training Loss =  9.437978587811813e-05\n",
            "Batch Training Loss =  9.489965304965153e-05\n",
            "Batch Training Loss =  9.13047042558901e-05\n",
            "Batch Training Loss =  9.676338959252462e-05\n",
            "Batch Training Loss =  9.557874000165612e-05\n",
            "Batch Training Loss =  9.18864825507626e-05\n",
            "Batch Training Loss =  9.283025428885594e-05\n",
            "Batch Training Loss =  9.03537220438011e-05\n",
            "Batch Training Loss =  9.852772927843034e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  9.684424730949104e-05\n",
            "Batch Training Loss =  9.243217937182635e-05\n",
            "Batch Training Loss =  9.112116094911471e-05\n",
            "Batch Training Loss =  8.829859143588692e-05\n",
            "Batch Training Loss =  8.888496086001396e-05\n",
            "Batch Training Loss =  9.084210614673793e-05\n",
            "Batch Training Loss =  9.223420056514442e-05\n",
            "Batch Training Loss =  9.132849663728848e-05\n",
            "Batch Training Loss =  8.635095582576469e-05\n",
            "Batch Training Loss =  8.765232632867992e-05\n",
            "Batch Training Loss =  8.688969683134928e-05\n",
            "Batch Training Loss =  8.746881212573498e-05\n",
            "Batch Training Loss =  8.432241156697273e-05\n",
            "Batch Training Loss =  8.969530608737841e-05\n",
            "Batch Training Loss =  8.655557758174837e-05\n",
            "Batch Training Loss =  9.335706272395328e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  8.525669545633718e-05\n",
            "Batch Training Loss =  8.633896504761651e-05\n",
            "Batch Training Loss =  9.239585779141635e-05\n",
            "Batch Training Loss =  8.312529826071113e-05\n",
            "Batch Training Loss =  8.213848923332989e-05\n",
            "Batch Training Loss =  8.335665188496932e-05\n",
            "Batch Training Loss =  8.988816261989996e-05\n",
            "Batch Training Loss =  8.57499981066212e-05\n",
            "Batch Training Loss =  8.164253813447431e-05\n",
            "Batch Training Loss =  8.527797035640106e-05\n",
            "Batch Training Loss =  8.12896469142288e-05\n",
            "Batch Training Loss =  8.593845268478617e-05\n",
            "Batch Training Loss =  8.460793469566852e-05\n",
            "Batch Training Loss =  7.869129331083968e-05\n",
            "Batch Training Loss =  8.716823504073545e-05\n",
            "Batch Training Loss =  8.379269274882972e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  8.896804502001032e-05\n",
            "Batch Training Loss =  7.958753121783957e-05\n",
            "Batch Training Loss =  7.962581003084779e-05\n",
            "Batch Training Loss =  8.304166840389371e-05\n",
            "Batch Training Loss =  8.162812446244061e-05\n",
            "Batch Training Loss =  7.830512913642451e-05\n",
            "Batch Training Loss =  7.822630141163245e-05\n",
            "Batch Training Loss =  8.474134665448219e-05\n",
            "Batch Training Loss =  7.890094275353476e-05\n",
            "Batch Training Loss =  8.208330837078393e-05\n",
            "Batch Training Loss =  8.133461960824206e-05\n",
            "Batch Training Loss =  7.626433216501027e-05\n",
            "Batch Training Loss =  7.399489550152794e-05\n",
            "Batch Training Loss =  7.652422209503129e-05\n",
            "Batch Training Loss =  7.549919246230274e-05\n",
            "Batch Training Loss =  8.174244430847466e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  7.45240758988075e-05\n",
            "Batch Training Loss =  7.420217298204079e-05\n",
            "Batch Training Loss =  7.562548125861213e-05\n",
            "Batch Training Loss =  7.836203440092504e-05\n",
            "Batch Training Loss =  7.834056305000558e-05\n",
            "Batch Training Loss =  7.832632400095463e-05\n",
            "Batch Training Loss =  7.390657265204936e-05\n",
            "Batch Training Loss =  7.495794125134125e-05\n",
            "Batch Training Loss =  7.659074617549777e-05\n",
            "Batch Training Loss =  7.674115477129817e-05\n",
            "Batch Training Loss =  7.256205572048202e-05\n",
            "Batch Training Loss =  7.578014628961682e-05\n",
            "Batch Training Loss =  7.294569513760507e-05\n",
            "Batch Training Loss =  7.592331530759111e-05\n",
            "Batch Training Loss =  7.428316166624427e-05\n",
            "Batch Training Loss =  7.376112625934184e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  7.428539538523182e-05\n",
            "Batch Training Loss =  7.424735667882487e-05\n",
            "Batch Training Loss =  7.471923890989274e-05\n",
            "Batch Training Loss =  7.231636845972389e-05\n",
            "Batch Training Loss =  7.148917211452499e-05\n",
            "Batch Training Loss =  6.902667519170791e-05\n",
            "Batch Training Loss =  8.013328624656424e-05\n",
            "Batch Training Loss =  7.067623664624989e-05\n",
            "Batch Training Loss =  7.207782618934289e-05\n",
            "Batch Training Loss =  7.010408444330096e-05\n",
            "Batch Training Loss =  7.110284059308469e-05\n",
            "Batch Training Loss =  6.68690845486708e-05\n",
            "Batch Training Loss =  6.783472053939477e-05\n",
            "Batch Training Loss =  7.263342558871955e-05\n",
            "Batch Training Loss =  6.991336704231799e-05\n",
            "Batch Training Loss =  7.094090688042343e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  7.162007386796176e-05\n",
            "Batch Training Loss =  7.259537233039737e-05\n",
            "Batch Training Loss =  6.958425365155563e-05\n",
            "Batch Training Loss =  7.428308890666813e-05\n",
            "Batch Training Loss =  6.760813994333148e-05\n",
            "Batch Training Loss =  6.685724656563252e-05\n",
            "Batch Training Loss =  6.674034375464544e-05\n",
            "Batch Training Loss =  6.858781125629321e-05\n",
            "Batch Training Loss =  7.015420851530507e-05\n",
            "Batch Training Loss =  6.713836773997173e-05\n",
            "Batch Training Loss =  6.617528561037034e-05\n",
            "Batch Training Loss =  6.640412902925164e-05\n",
            "Batch Training Loss =  6.444693281082436e-05\n",
            "Batch Training Loss =  6.60583536955528e-05\n",
            "Batch Training Loss =  6.457322160713375e-05\n",
            "Batch Training Loss =  6.532656698254868e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  6.703112012473866e-05\n",
            "Batch Training Loss =  6.595587910851464e-05\n",
            "Batch Training Loss =  6.670444417977706e-05\n",
            "Batch Training Loss =  6.763177952961996e-05\n",
            "Batch Training Loss =  6.583666254300624e-05\n",
            "Batch Training Loss =  6.546723307110369e-05\n",
            "Batch Training Loss =  6.521456816699356e-05\n",
            "Batch Training Loss =  6.483298056991771e-05\n",
            "Batch Training Loss =  6.390101771103218e-05\n",
            "Batch Training Loss =  6.477825081674382e-05\n",
            "Batch Training Loss =  6.300451059360057e-05\n",
            "Batch Training Loss =  6.536216824315488e-05\n",
            "Batch Training Loss =  6.337399099720642e-05\n",
            "Batch Training Loss =  6.546478834934533e-05\n",
            "Batch Training Loss =  6.497607682831585e-05\n",
            "Batch Training Loss =  6.703096732962877e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  6.379127444233745e-05\n",
            "Batch Training Loss =  6.357912207022309e-05\n",
            "Batch Training Loss =  6.49046414764598e-05\n",
            "Batch Training Loss =  6.476391718024388e-05\n",
            "Batch Training Loss =  6.070163726690225e-05\n",
            "Batch Training Loss =  6.024385220371187e-05\n",
            "Batch Training Loss =  6.473768735304475e-05\n",
            "Batch Training Loss =  6.1640894273296e-05\n",
            "Batch Training Loss =  6.258006760617718e-05\n",
            "Batch Training Loss =  6.278510409174487e-05\n",
            "Batch Training Loss =  6.331446638796479e-05\n",
            "Batch Training Loss =  6.203408702276647e-05\n",
            "Batch Training Loss =  6.12332223681733e-05\n",
            "Batch Training Loss =  6.0403581301216036e-05\n",
            "Batch Training Loss =  6.109262903919443e-05\n",
            "Batch Training Loss =  6.012229641783051e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  6.296869833022356e-05\n",
            "Batch Training Loss =  6.0930451581953093e-05\n",
            "Batch Training Loss =  6.059902807464823e-05\n",
            "Batch Training Loss =  5.646048521157354e-05\n",
            "Batch Training Loss =  6.019614011165686e-05\n",
            "Batch Training Loss =  6.014843165758066e-05\n",
            "Batch Training Loss =  5.9755089750979096e-05\n",
            "Batch Training Loss =  6.118316377978772e-05\n",
            "Batch Training Loss =  6.151681009214371e-05\n",
            "Batch Training Loss =  5.957635221420787e-05\n",
            "Batch Training Loss =  5.969316771370359e-05\n",
            "Batch Training Loss =  5.6748911447357386e-05\n",
            "Batch Training Loss =  5.917821908951737e-05\n",
            "Batch Training Loss =  5.940696792094968e-05\n",
            "Batch Training Loss =  6.11830546404235e-05\n",
            "Batch Training Loss =  5.7886019931174815e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  5.7123157603200525e-05\n",
            "Batch Training Loss =  5.675356442225166e-05\n",
            "Batch Training Loss =  6.0484409914352e-05\n",
            "Batch Training Loss =  5.809824142488651e-05\n",
            "Batch Training Loss =  5.91399148106575e-05\n",
            "Batch Training Loss =  5.8746652939589694e-05\n",
            "Batch Training Loss =  5.769533890997991e-05\n",
            "Batch Training Loss =  5.639846858684905e-05\n",
            "Batch Training Loss =  5.7995734096039087e-05\n",
            "Batch Training Loss =  5.622689059237018e-05\n",
            "Batch Training Loss =  5.65891205042135e-05\n",
            "Batch Training Loss =  5.957388930255547e-05\n",
            "Batch Training Loss =  5.523501749848947e-05\n",
            "Batch Training Loss =  5.660820170305669e-05\n",
            "Batch Training Loss =  5.592640445684083e-05\n",
            "Batch Training Loss =  5.672735642292537e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  5.726133895223029e-05\n",
            "Batch Training Loss =  5.6677312386455014e-05\n",
            "Batch Training Loss =  5.48869684280362e-05\n",
            "Batch Training Loss =  5.579773642239161e-05\n",
            "Batch Training Loss =  5.5687934946035966e-05\n",
            "Batch Training Loss =  5.599312135018408e-05\n",
            "Batch Training Loss =  5.4953656217548996e-05\n",
            "Batch Training Loss =  5.291299385135062e-05\n",
            "Batch Training Loss =  5.5130072723841295e-05\n",
            "Batch Training Loss =  5.6965793191920966e-05\n",
            "Batch Training Loss =  5.3106137784197927e-05\n",
            "Batch Training Loss =  5.413361213868484e-05\n",
            "Batch Training Loss =  5.7642842875793576e-05\n",
            "Batch Training Loss =  5.517071622307412e-05\n",
            "Batch Training Loss =  5.749246702180244e-05\n",
            "Batch Training Loss =  5.43457645107992e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  5.389289071899839e-05\n",
            "Batch Training Loss =  5.2631596190622076e-05\n",
            "Batch Training Loss =  5.504663204192184e-05\n",
            "Batch Training Loss =  5.4770163842476904e-05\n",
            "Batch Training Loss =  5.455071004689671e-05\n",
            "Batch Training Loss =  5.489160321303643e-05\n",
            "Batch Training Loss =  5.5656964832451195e-05\n",
            "Batch Training Loss =  5.4636468121316284e-05\n",
            "Batch Training Loss =  5.237178993411362e-05\n",
            "Batch Training Loss =  5.1806768169626594e-05\n",
            "Batch Training Loss =  5.174004036234692e-05\n",
            "Batch Training Loss =  5.270083056529984e-05\n",
            "Batch Training Loss =  5.211675670580007e-05\n",
            "Batch Training Loss =  5.00426504004281e-05\n",
            "Batch Training Loss =  5.274608702166006e-05\n",
            "Batch Training Loss =  5.130855060997419e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  5.2948700613342226e-05\n",
            "Batch Training Loss =  5.15588944836054e-05\n",
            "Batch Training Loss =  5.1849670853698626e-05\n",
            "Batch Training Loss =  5.219301237957552e-05\n",
            "Batch Training Loss =  5.23191956744995e-05\n",
            "Batch Training Loss =  5.093183426652104e-05\n",
            "Batch Training Loss =  4.992579488316551e-05\n",
            "Batch Training Loss =  5.3206174925435334e-05\n",
            "Batch Training Loss =  5.0331127567915246e-05\n",
            "Batch Training Loss =  5.253379640635103e-05\n",
            "Batch Training Loss =  5.2057035645702854e-05\n",
            "Batch Training Loss =  4.946812623529695e-05\n",
            "Batch Training Loss =  5.262677950668149e-05\n",
            "Batch Training Loss =  4.959442594554275e-05\n",
            "Batch Training Loss =  4.987093052477576e-05\n",
            "Batch Training Loss =  4.855017687077634e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  4.878380786976777e-05\n",
            "Batch Training Loss =  4.729859938379377e-05\n",
            "Batch Training Loss =  5.3873769502388313e-05\n",
            "Batch Training Loss =  4.869567055720836e-05\n",
            "Batch Training Loss =  5.004256672691554e-05\n",
            "Batch Training Loss =  4.867171082878485e-05\n",
            "Batch Training Loss =  4.987804277334362e-05\n",
            "Batch Training Loss =  5.264107676339336e-05\n",
            "Batch Training Loss =  4.947280831402168e-05\n",
            "Batch Training Loss =  5.0283331802347675e-05\n",
            "Batch Training Loss =  4.7916066250763834e-05\n",
            "Batch Training Loss =  4.752267705043778e-05\n",
            "Batch Training Loss =  5.40499750059098e-05\n",
            "Batch Training Loss =  4.793987318407744e-05\n",
            "Batch Training Loss =  4.5717944885836914e-05\n",
            "Batch Training Loss =  4.919869388686493e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  4.8752779548522085e-05\n",
            "Batch Training Loss =  4.9630059947958216e-05\n",
            "Batch Training Loss =  4.910800271318294e-05\n",
            "Batch Training Loss =  5.090547711006366e-05\n",
            "Batch Training Loss =  4.8583569878246635e-05\n",
            "Batch Training Loss =  4.5458109525498e-05\n",
            "Batch Training Loss =  4.79589361930266e-05\n",
            "Batch Training Loss =  4.8547768528806046e-05\n",
            "Batch Training Loss =  4.7293742682086304e-05\n",
            "Batch Training Loss =  4.62662719655782e-05\n",
            "Batch Training Loss =  4.676690878113732e-05\n",
            "Batch Training Loss =  4.6914694394217804e-05\n",
            "Batch Training Loss =  4.822347909794189e-05\n",
            "Batch Training Loss =  4.862876085098833e-05\n",
            "Batch Training Loss =  4.7985187848098576e-05\n",
            "Batch Training Loss =  4.721035293187015e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  4.635919685824774e-05\n",
            "Batch Training Loss =  4.92247381771449e-05\n",
            "Batch Training Loss =  4.97040236950852e-05\n",
            "Batch Training Loss =  4.4673728552879766e-05\n",
            "Batch Training Loss =  4.6235290938057005e-05\n",
            "Batch Training Loss =  4.6249562728917226e-05\n",
            "Batch Training Loss =  4.45926925749518e-05\n",
            "Batch Training Loss =  4.696709947893396e-05\n",
            "Batch Training Loss =  4.6430683141807094e-05\n",
            "Batch Training Loss =  4.5770368160447106e-05\n",
            "Batch Training Loss =  4.45831537945196e-05\n",
            "Batch Training Loss =  4.6518907765857875e-05\n",
            "Batch Training Loss =  4.6337750973179936e-05\n",
            "Batch Training Loss =  4.8197078285738826e-05\n",
            "Batch Training Loss =  4.680977508542128e-05\n",
            "Batch Training Loss =  4.560114030027762e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  4.694086601375602e-05\n",
            "Batch Training Loss =  4.577989238896407e-05\n",
            "Batch Training Loss =  4.488592821871862e-05\n",
            "Batch Training Loss =  4.6597513573942706e-05\n",
            "Batch Training Loss =  4.4888278353028e-05\n",
            "Batch Training Loss =  4.468084443942644e-05\n",
            "Batch Training Loss =  4.4373286073096097e-05\n",
            "Batch Training Loss =  4.350555536802858e-05\n",
            "Batch Training Loss =  4.467607504921034e-05\n",
            "Batch Training Loss =  4.524110408965498e-05\n",
            "Batch Training Loss =  4.5386554120341316e-05\n",
            "Batch Training Loss =  4.3162213842151687e-05\n",
            "Batch Training Loss =  4.602298577083275e-05\n",
            "Batch Training Loss =  4.415636431076564e-05\n",
            "Batch Training Loss =  4.502890078583732e-05\n",
            "Batch Training Loss =  4.56224697700236e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  4.2694966396084055e-05\n",
            "Batch Training Loss =  4.3867898057214916e-05\n",
            "Batch Training Loss =  4.455923044588417e-05\n",
            "Batch Training Loss =  4.293095480534248e-05\n",
            "Batch Training Loss =  4.426599480211735e-05\n",
            "Batch Training Loss =  4.3400610593380406e-05\n",
            "Batch Training Loss =  4.795162385562435e-05\n",
            "Batch Training Loss =  4.642589556169696e-05\n",
            "Batch Training Loss =  4.1140523535432294e-05\n",
            "Batch Training Loss =  4.2499388655414805e-05\n",
            "Batch Training Loss =  4.355312921688892e-05\n",
            "Batch Training Loss =  4.272829391993582e-05\n",
            "Batch Training Loss =  4.16912225773558e-05\n",
            "Batch Training Loss =  4.303107925807126e-05\n",
            "Batch Training Loss =  4.325516420067288e-05\n",
            "Batch Training Loss =  4.135029303142801e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  4.0990289562614635e-05\n",
            "Batch Training Loss =  4.358894511824474e-05\n",
            "Batch Training Loss =  4.1665010940050706e-05\n",
            "Batch Training Loss =  4.610157702700235e-05\n",
            "Batch Training Loss =  4.370098758954555e-05\n",
            "Batch Training Loss =  4.053496741107665e-05\n",
            "Batch Training Loss =  4.295714097679593e-05\n",
            "Batch Training Loss =  4.08234482165426e-05\n",
            "Batch Training Loss =  4.280457869754173e-05\n",
            "Batch Training Loss =  4.376535434857942e-05\n",
            "Batch Training Loss =  4.139558950555511e-05\n",
            "Batch Training Loss =  4.223001087666489e-05\n",
            "Batch Training Loss =  4.164591882727109e-05\n",
            "Batch Training Loss =  4.190810795989819e-05\n",
            "Batch Training Loss =  4.159111631452106e-05\n",
            "Batch Training Loss =  4.2089344788109884e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  4.1147664887830615e-05\n",
            "Batch Training Loss =  4.304302274249494e-05\n",
            "Batch Training Loss =  4.166732105659321e-05\n",
            "Batch Training Loss =  4.070419890922494e-05\n",
            "Batch Training Loss =  4.081153019797057e-05\n",
            "Batch Training Loss =  4.034183803014457e-05\n",
            "Batch Training Loss =  4.254228770150803e-05\n",
            "Batch Training Loss =  3.7819569115526974e-05\n",
            "Batch Training Loss =  3.9931837818585336e-05\n",
            "Batch Training Loss =  4.1710245568538085e-05\n",
            "Batch Training Loss =  4.028222610941157e-05\n",
            "Batch Training Loss =  4.0852013626135886e-05\n",
            "Batch Training Loss =  4.075665128766559e-05\n",
            "Batch Training Loss =  4.380823884275742e-05\n",
            "Batch Training Loss =  4.036325481138192e-05\n",
            "Batch Training Loss =  4.057547994307242e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  3.8553796912310645e-05\n",
            "Batch Training Loss =  3.956460568588227e-05\n",
            "Batch Training Loss =  3.8289192161755636e-05\n",
            "Batch Training Loss =  4.160540265729651e-05\n",
            "Batch Training Loss =  3.8846996176289394e-05\n",
            "Batch Training Loss =  4.0985560190165415e-05\n",
            "Batch Training Loss =  3.98006995965261e-05\n",
            "Batch Training Loss =  3.947880759369582e-05\n",
            "Batch Training Loss =  3.9116428524721414e-05\n",
            "Batch Training Loss =  3.911405292456038e-05\n",
            "Batch Training Loss =  4.037284816149622e-05\n",
            "Batch Training Loss =  4.191758489469066e-05\n",
            "Batch Training Loss =  3.949785968870856e-05\n",
            "Batch Training Loss =  4.089485082658939e-05\n",
            "Batch Training Loss =  4.013200305053033e-05\n",
            "Batch Training Loss =  3.7130546843400225e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  3.7597808841383085e-05\n",
            "Batch Training Loss =  3.882793680531904e-05\n",
            "Batch Training Loss =  4.00008684664499e-05\n",
            "Batch Training Loss =  4.0081948100123554e-05\n",
            "Batch Training Loss =  3.98840093112085e-05\n",
            "Batch Training Loss =  3.9507434848928824e-05\n",
            "Batch Training Loss =  3.798161924351007e-05\n",
            "Batch Training Loss =  3.765262954402715e-05\n",
            "Batch Training Loss =  3.8703979953425005e-05\n",
            "Batch Training Loss =  3.9717171603115276e-05\n",
            "Batch Training Loss =  3.978393579018302e-05\n",
            "Batch Training Loss =  3.8978163502179086e-05\n",
            "Batch Training Loss =  3.778611426241696e-05\n",
            "Batch Training Loss =  3.8487025449285284e-05\n",
            "Batch Training Loss =  3.819856283371337e-05\n",
            "Batch Training Loss =  3.746186484931968e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  3.949785605072975e-05\n",
            "Batch Training Loss =  3.758346429094672e-05\n",
            "Batch Training Loss =  3.8930429582251236e-05\n",
            "Batch Training Loss =  3.695645136758685e-05\n",
            "Batch Training Loss =  3.9228420064318925e-05\n",
            "Batch Training Loss =  3.85632629331667e-05\n",
            "Batch Training Loss =  3.819620906142518e-05\n",
            "Batch Training Loss =  3.659884168882854e-05\n",
            "Batch Training Loss =  3.734029087354429e-05\n",
            "Batch Training Loss =  3.869680585921742e-05\n",
            "Batch Training Loss =  3.689924778882414e-05\n",
            "Batch Training Loss =  3.7349804188124835e-05\n",
            "Batch Training Loss =  3.609102714108303e-05\n",
            "Batch Training Loss =  3.657739580376074e-05\n",
            "Batch Training Loss =  3.858236232190393e-05\n",
            "Batch Training Loss =  3.6236466257832944e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  3.609107079682872e-05\n",
            "Batch Training Loss =  3.816755270236172e-05\n",
            "Batch Training Loss =  3.601233765948564e-05\n",
            "Batch Training Loss =  3.770023977267556e-05\n",
            "Batch Training Loss =  3.579066105885431e-05\n",
            "Batch Training Loss =  3.641052535385825e-05\n",
            "Batch Training Loss =  3.7392735976027325e-05\n",
            "Batch Training Loss =  3.603143704822287e-05\n",
            "Batch Training Loss =  3.6179248127155006e-05\n",
            "Batch Training Loss =  3.679908695630729e-05\n",
            "Batch Training Loss =  3.65082269127015e-05\n",
            "Batch Training Loss =  3.73736402252689e-05\n",
            "Batch Training Loss =  3.745469803106971e-05\n",
            "Batch Training Loss =  3.565707811503671e-05\n",
            "Batch Training Loss =  3.6832483601756394e-05\n",
            "Batch Training Loss =  3.597661998355761e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  3.8684815081069246e-05\n",
            "Batch Training Loss =  3.730452590389177e-05\n",
            "Batch Training Loss =  3.5831173590850085e-05\n",
            "Batch Training Loss =  3.523038321873173e-05\n",
            "Batch Training Loss =  3.5711938835447654e-05\n",
            "Batch Training Loss =  3.4086027881130576e-05\n",
            "Batch Training Loss =  3.54210969817359e-05\n",
            "Batch Training Loss =  3.5409171687206253e-05\n",
            "Batch Training Loss =  3.4405533369863406e-05\n",
            "Batch Training Loss =  3.625309545896016e-05\n",
            "Batch Training Loss =  3.551881673047319e-05\n",
            "Batch Training Loss =  3.610057683545165e-05\n",
            "Batch Training Loss =  3.525899592204951e-05\n",
            "Batch Training Loss =  3.587886749301106e-05\n",
            "Batch Training Loss =  3.3921547583304346e-05\n",
            "Batch Training Loss =  3.706844290718436e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  3.665843541966751e-05\n",
            "Batch Training Loss =  3.466537600615993e-05\n",
            "Batch Training Loss =  3.644860044005327e-05\n",
            "Batch Training Loss =  3.629837010521442e-05\n",
            "Batch Training Loss =  3.485606430331245e-05\n",
            "Batch Training Loss =  3.3826156141003594e-05\n",
            "Batch Training Loss =  3.537576048984192e-05\n",
            "Batch Training Loss =  3.600517084123567e-05\n",
            "Batch Training Loss =  3.505635686451569e-05\n",
            "Batch Training Loss =  3.572390414774418e-05\n",
            "Batch Training Loss =  3.368311809026636e-05\n",
            "Batch Training Loss =  3.410268254810944e-05\n",
            "Batch Training Loss =  3.56905184162315e-05\n",
            "Batch Training Loss =  3.326353180455044e-05\n",
            "Batch Training Loss =  3.3852396882139146e-05\n",
            "Batch Training Loss =  3.378562905709259e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  3.459379149717279e-05\n",
            "Batch Training Loss =  3.446505434112623e-05\n",
            "Batch Training Loss =  3.387385731912218e-05\n",
            "Batch Training Loss =  3.294405178166926e-05\n",
            "Batch Training Loss =  3.572629429982044e-05\n",
            "Batch Training Loss =  3.347329038660973e-05\n",
            "Batch Training Loss =  3.538531018421054e-05\n",
            "Batch Training Loss =  3.4763066651066765e-05\n",
            "Batch Training Loss =  3.477257996564731e-05\n",
            "Batch Training Loss =  3.538052624207921e-05\n",
            "Batch Training Loss =  3.456283229752444e-05\n",
            "Batch Training Loss =  3.412894147913903e-05\n",
            "Batch Training Loss =  3.185453897458501e-05\n",
            "Batch Training Loss =  3.485601337160915e-05\n",
            "Batch Training Loss =  3.215491960872896e-05\n",
            "Batch Training Loss =  3.293450572527945e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  3.4322016290389e-05\n",
            "Batch Training Loss =  3.399304478080012e-05\n",
            "Batch Training Loss =  3.299410309409723e-05\n",
            "Batch Training Loss =  3.324445060570724e-05\n",
            "Batch Training Loss =  3.392153303138912e-05\n",
            "Batch Training Loss =  3.289397500338964e-05\n",
            "Batch Training Loss =  3.462005770415999e-05\n",
            "Batch Training Loss =  3.3318327041342854e-05\n",
            "Batch Training Loss =  3.277477662777528e-05\n",
            "Batch Training Loss =  3.360202754265629e-05\n",
            "Batch Training Loss =  3.353527426952496e-05\n",
            "Batch Training Loss =  3.3029868063749745e-05\n",
            "Batch Training Loss =  3.146592644043267e-05\n",
            "Batch Training Loss =  3.358054527780041e-05\n",
            "Batch Training Loss =  3.337793896207586e-05\n",
            "Batch Training Loss =  3.2541127438889816e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  3.257213029428385e-05\n",
            "Batch Training Loss =  3.33612370013725e-05\n",
            "Batch Training Loss =  3.387384276720695e-05\n",
            "Batch Training Loss =  3.372599894646555e-05\n",
            "Batch Training Loss =  3.299411037005484e-05\n",
            "Batch Training Loss =  3.220498183509335e-05\n",
            "Batch Training Loss =  3.2526808354305103e-05\n",
            "Batch Training Loss =  3.2128682505572215e-05\n",
            "Batch Training Loss =  3.204521999577992e-05\n",
            "Batch Training Loss =  3.2283631298923865e-05\n",
            "Batch Training Loss =  3.2967898732749745e-05\n",
            "Batch Training Loss =  3.160180494887754e-05\n",
            "Batch Training Loss =  3.2104853744385764e-05\n",
            "Batch Training Loss =  3.266270141466521e-05\n",
            "Batch Training Loss =  3.185927926097065e-05\n",
            "Batch Training Loss =  3.217155608581379e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  3.1165502150543034e-05\n",
            "Batch Training Loss =  3.1251351174432784e-05\n",
            "Batch Training Loss =  3.230748188798316e-05\n",
            "Batch Training Loss =  3.38761747116223e-05\n",
            "Batch Training Loss =  3.189744529663585e-05\n",
            "Batch Training Loss =  3.295118949608877e-05\n",
            "Batch Training Loss =  3.2924945116974413e-05\n",
            "Batch Training Loss =  3.313234628876671e-05\n",
            "Batch Training Loss =  3.282484976807609e-05\n",
            "Batch Training Loss =  3.0309623980429024e-05\n",
            "Batch Training Loss =  3.201899380655959e-05\n",
            "Batch Training Loss =  3.1082065106602386e-05\n",
            "Batch Training Loss =  3.183541048201732e-05\n",
            "Batch Training Loss =  3.1027189834276214e-05\n",
            "Batch Training Loss =  3.1349070923170075e-05\n",
            "Batch Training Loss =  3.151597047690302e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  3.2448162528453395e-05\n",
            "Batch Training Loss =  3.0810257158009335e-05\n",
            "Batch Training Loss =  3.2376647141063586e-05\n",
            "Batch Training Loss =  3.207617191947065e-05\n",
            "Batch Training Loss =  3.321579788462259e-05\n",
            "Batch Training Loss =  3.108445162069984e-05\n",
            "Batch Training Loss =  2.9684966648346744e-05\n",
            "Batch Training Loss =  2.9839977287338115e-05\n",
            "Batch Training Loss =  3.167089380440302e-05\n",
            "Batch Training Loss =  3.155886952299625e-05\n",
            "Batch Training Loss =  3.12775737256743e-05\n",
            "Batch Training Loss =  3.2252646633423865e-05\n",
            "Batch Training Loss =  2.9448970963130705e-05\n",
            "Batch Training Loss =  2.9408440241240896e-05\n",
            "Batch Training Loss =  2.9620612622238696e-05\n",
            "Batch Training Loss =  3.179249688400887e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  3.118931272183545e-05\n",
            "Batch Training Loss =  3.191642099409364e-05\n",
            "Batch Training Loss =  2.9339313186937943e-05\n",
            "Batch Training Loss =  3.08269627566915e-05\n",
            "Batch Training Loss =  3.0016359232831746e-05\n",
            "Batch Training Loss =  2.9789873224217445e-05\n",
            "Batch Training Loss =  3.0331053494592197e-05\n",
            "Batch Training Loss =  3.1132127332966775e-05\n",
            "Batch Training Loss =  3.0910417990526184e-05\n",
            "Batch Training Loss =  3.0207107556634583e-05\n",
            "Batch Training Loss =  3.0066432373132557e-05\n",
            "Batch Training Loss =  3.063622716581449e-05\n",
            "Batch Training Loss =  3.210241629858501e-05\n",
            "Batch Training Loss =  3.0857943784212694e-05\n",
            "Batch Training Loss =  3.00401898130076e-05\n",
            "Batch Training Loss =  2.9580069167423062e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  3.208574344171211e-05\n",
            "Batch Training Loss =  2.9925773560535163e-05\n",
            "Batch Training Loss =  2.8965007004444487e-05\n",
            "Batch Training Loss =  3.019038740603719e-05\n",
            "Batch Training Loss =  3.051465864700731e-05\n",
            "Batch Training Loss =  2.9670654839719646e-05\n",
            "Batch Training Loss =  3.077688961639069e-05\n",
            "Batch Training Loss =  2.9925737180747092e-05\n",
            "Batch Training Loss =  2.8914928407175466e-05\n",
            "Batch Training Loss =  2.8869637390016578e-05\n",
            "Batch Training Loss =  2.864550333470106e-05\n",
            "Batch Training Loss =  3.084123454755172e-05\n",
            "Batch Training Loss =  2.880287320294883e-05\n",
            "Batch Training Loss =  3.0466964744846337e-05\n",
            "Batch Training Loss =  2.8953067157999612e-05\n",
            "Batch Training Loss =  2.9060343877063133e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  3.0657636671094224e-05\n",
            "Batch Training Loss =  2.9406013709376566e-05\n",
            "Batch Training Loss =  2.961342761409469e-05\n",
            "Batch Training Loss =  2.9406044632196426e-05\n",
            "Batch Training Loss =  2.8323665901552886e-05\n",
            "Batch Training Loss =  3.0168948796926998e-05\n",
            "Batch Training Loss =  2.846672578016296e-05\n",
            "Batch Training Loss =  2.9184324375819415e-05\n",
            "Batch Training Loss =  2.9115182769601233e-05\n",
            "Batch Training Loss =  2.9982977139297873e-05\n",
            "Batch Training Loss =  2.950379166577477e-05\n",
            "Batch Training Loss =  2.930589107563719e-05\n",
            "Batch Training Loss =  2.995198155986145e-05\n",
            "Batch Training Loss =  2.9298751542228274e-05\n",
            "Batch Training Loss =  2.731280619627796e-05\n",
            "Batch Training Loss =  2.8204462068970315e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  2.9010272555751726e-05\n",
            "Batch Training Loss =  2.8151978767709807e-05\n",
            "Batch Training Loss =  2.9684961191378534e-05\n",
            "Batch Training Loss =  2.9830369385308586e-05\n",
            "Batch Training Loss =  2.8898230084450915e-05\n",
            "Batch Training Loss =  2.765135832305532e-05\n",
            "Batch Training Loss =  2.7908825359190814e-05\n",
            "Batch Training Loss =  2.933687028416898e-05\n",
            "Batch Training Loss =  2.8588310669874772e-05\n",
            "Batch Training Loss =  2.821397538355086e-05\n",
            "Batch Training Loss =  2.758935806923546e-05\n",
            "Batch Training Loss =  2.8705107979476452e-05\n",
            "Batch Training Loss =  2.8798089260817505e-05\n",
            "Batch Training Loss =  2.7689491616911255e-05\n",
            "Batch Training Loss =  2.8471469704527408e-05\n",
            "Batch Training Loss =  2.8643135010497645e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  2.73151854344178e-05\n",
            "Batch Training Loss =  2.876947655749973e-05\n",
            "Batch Training Loss =  2.74200920102885e-05\n",
            "Batch Training Loss =  2.8476217266870663e-05\n",
            "Batch Training Loss =  2.824499097187072e-05\n",
            "Batch Training Loss =  2.790883263514843e-05\n",
            "Batch Training Loss =  2.7815829525934532e-05\n",
            "Batch Training Loss =  2.8030390240019187e-05\n",
            "Batch Training Loss =  2.7861138732987456e-05\n",
            "Batch Training Loss =  2.811862577800639e-05\n",
            "Batch Training Loss =  2.7262723961030133e-05\n",
            "Batch Training Loss =  2.9444137908285484e-05\n",
            "Batch Training Loss =  2.7613185011432506e-05\n",
            "Batch Training Loss =  2.8299826226430014e-05\n",
            "Batch Training Loss =  2.8364196623442695e-05\n",
            "Batch Training Loss =  2.82020428130636e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  2.816390770021826e-05\n",
            "Batch Training Loss =  2.688842505449429e-05\n",
            "Batch Training Loss =  2.786590630421415e-05\n",
            "Batch Training Loss =  2.7339016014593653e-05\n",
            "Batch Training Loss =  2.8128129997639917e-05\n",
            "Batch Training Loss =  2.743197910604067e-05\n",
            "Batch Training Loss =  2.708152896957472e-05\n",
            "Batch Training Loss =  2.8123358788434416e-05\n",
            "Batch Training Loss =  2.702433630474843e-05\n",
            "Batch Training Loss =  2.8526295864139684e-05\n",
            "Batch Training Loss =  2.6990943297278136e-05\n",
            "Batch Training Loss =  2.735811358434148e-05\n",
            "Batch Training Loss =  2.7644175133900717e-05\n",
            "Batch Training Loss =  2.769662933133077e-05\n",
            "Batch Training Loss =  2.75821967079537e-05\n",
            "Batch Training Loss =  2.6890797016676515e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  2.6535561119089834e-05\n",
            "Batch Training Loss =  2.6862211598199792e-05\n",
            "Batch Training Loss =  2.9859023925382644e-05\n",
            "Batch Training Loss =  3.0397823138628155e-05\n",
            "Batch Training Loss =  3.102246773778461e-05\n",
            "Batch Training Loss =  3.086512515437789e-05\n",
            "Batch Training Loss =  3.275809285696596e-05\n",
            "Batch Training Loss =  3.434825339354575e-05\n",
            "Batch Training Loss =  3.536385338520631e-05\n",
            "Batch Training Loss =  3.601238131523132e-05\n",
            "Batch Training Loss =  3.9402497350238264e-05\n",
            "Batch Training Loss =  3.817238757619634e-05\n",
            "Batch Training Loss =  4.125017221667804e-05\n",
            "Batch Training Loss =  4.070904105901718e-05\n",
            "Batch Training Loss =  4.2680618207668886e-05\n",
            "Batch Training Loss =  4.61111958429683e-05\n",
            "Validation Loss in this epoch is 0.000\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  4.6394980017794296e-05\n",
            "Batch Training Loss =  4.7341451136162505e-05\n",
            "Batch Training Loss =  5.095795131637715e-05\n",
            "Batch Training Loss =  5.1880557293770835e-05\n",
            "Batch Training Loss =  5.119884372106753e-05\n",
            "Batch Training Loss =  5.130861245561391e-05\n",
            "Batch Training Loss =  5.538043114938773e-05\n",
            "Batch Training Loss =  5.405020419857465e-05\n",
            "Batch Training Loss =  5.845823397976346e-05\n",
            "Batch Training Loss =  6.156703602755442e-05\n",
            "Batch Training Loss =  6.359811231959611e-05\n",
            "Batch Training Loss =  6.193181616254151e-05\n",
            "Batch Training Loss =  6.602041685255244e-05\n",
            "Batch Training Loss =  6.740308890584856e-05\n",
            "Batch Training Loss =  7.032108260318637e-05\n",
            "Batch Training Loss =  7.164652924984694e-05\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  7.602336700074375e-05\n",
            "Batch Training Loss =  7.608078885823488e-05\n",
            "Batch Training Loss =  7.847214874345809e-05\n",
            "Batch Training Loss =  7.828605157556012e-05\n",
            "Batch Training Loss =  8.274619176518172e-05\n",
            "Batch Training Loss =  8.685397915542126e-05\n",
            "Batch Training Loss =  8.482532575726509e-05\n",
            "Batch Training Loss =  9.380528354085982e-05\n",
            "Batch Training Loss =  9.79319229372777e-05\n",
            "Batch Training Loss =  9.477591811446473e-05\n",
            "Batch Training Loss =  9.798703104024753e-05\n",
            "Batch Training Loss =  0.00010402500629425049\n",
            "Batch Training Loss =  0.00010310057405149564\n",
            "Batch Training Loss =  0.00010578721412457526\n",
            "Batch Training Loss =  0.00010725558968260884\n",
            "Batch Training Loss =  0.0001101139496313408\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.00011738258763216436\n",
            "Batch Training Loss =  0.00012014519597869366\n",
            "Batch Training Loss =  0.00012295825581531972\n",
            "Batch Training Loss =  0.00012543011689558625\n",
            "Batch Training Loss =  0.00012655089085455984\n",
            "Batch Training Loss =  0.00013374043919611722\n",
            "Batch Training Loss =  0.00013994010805618018\n",
            "Batch Training Loss =  0.00014006727724336088\n",
            "Batch Training Loss =  0.00014349939010571688\n",
            "Batch Training Loss =  0.0001473157899454236\n",
            "Batch Training Loss =  0.0001531940361019224\n",
            "Batch Training Loss =  0.000162802854902111\n",
            "Batch Training Loss =  0.00015328754670917988\n",
            "Batch Training Loss =  0.00015837674436625093\n",
            "Batch Training Loss =  0.00016137117927428335\n",
            "Batch Training Loss =  0.00018318994261790067\n",
            "Validation Loss in this epoch is 0.001\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.00017961367848329246\n",
            "Batch Training Loss =  0.00017921747348736972\n",
            "Batch Training Loss =  0.00017714215209707618\n",
            "Batch Training Loss =  0.00018749617447610945\n",
            "Batch Training Loss =  0.00019421864999458194\n",
            "Batch Training Loss =  0.0002004030393436551\n",
            "Batch Training Loss =  0.00020114168000873178\n",
            "Batch Training Loss =  0.00020518062228802592\n",
            "Batch Training Loss =  0.0002090065972879529\n",
            "Batch Training Loss =  0.00021514607942663133\n",
            "Batch Training Loss =  0.00021243668743409216\n",
            "Batch Training Loss =  0.00022449293464887887\n",
            "Batch Training Loss =  0.00022859878663439304\n",
            "Batch Training Loss =  0.000235227111261338\n",
            "Batch Training Loss =  0.0002350908180233091\n",
            "Batch Training Loss =  0.0002535104285925627\n",
            "Validation Loss in this epoch is 0.002\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.00024547663633711636\n",
            "Batch Training Loss =  0.0002543135196901858\n",
            "Batch Training Loss =  0.0002585088659543544\n",
            "Batch Training Loss =  0.00026374528533779085\n",
            "Batch Training Loss =  0.0002738962066359818\n",
            "Batch Training Loss =  0.0002738316834438592\n",
            "Batch Training Loss =  0.00027737123309634626\n",
            "Batch Training Loss =  0.00029775817529298365\n",
            "Batch Training Loss =  0.00028538552578538656\n",
            "Batch Training Loss =  0.0003160680062137544\n",
            "Batch Training Loss =  0.00030159938614815474\n",
            "Batch Training Loss =  0.00030871934723109007\n",
            "Batch Training Loss =  0.0003228425921406597\n",
            "Batch Training Loss =  0.0003228195710107684\n",
            "Batch Training Loss =  0.00031617257627658546\n",
            "Batch Training Loss =  0.00034094424336217344\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.00033032576902769506\n",
            "Batch Training Loss =  0.0003394613740965724\n",
            "Batch Training Loss =  0.00035810592817142606\n",
            "Batch Training Loss =  0.0003512180701363832\n",
            "Batch Training Loss =  0.00035853165900334716\n",
            "Batch Training Loss =  0.00036762942909263074\n",
            "Batch Training Loss =  0.00037225111736916006\n",
            "Batch Training Loss =  0.00037574258749373257\n",
            "Batch Training Loss =  0.00038887839764356613\n",
            "Batch Training Loss =  0.00040228423313237727\n",
            "Batch Training Loss =  0.00040791439823806286\n",
            "Batch Training Loss =  0.0004267483891453594\n",
            "Batch Training Loss =  0.0004146946594119072\n",
            "Batch Training Loss =  0.0004213150532450527\n",
            "Batch Training Loss =  0.0004243385046720505\n",
            "Batch Training Loss =  0.00041938581853173673\n",
            "Validation Loss in this epoch is 0.003\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0004420720797497779\n",
            "Batch Training Loss =  0.00043137892498634756\n",
            "Batch Training Loss =  0.00045078678522258997\n",
            "Batch Training Loss =  0.0004493019077926874\n",
            "Batch Training Loss =  0.00045713488361798227\n",
            "Batch Training Loss =  0.00047382025513798\n",
            "Batch Training Loss =  0.00045888498425483704\n",
            "Batch Training Loss =  0.00047108810395002365\n",
            "Batch Training Loss =  0.0004839230969082564\n",
            "Batch Training Loss =  0.00048343930393457413\n",
            "Batch Training Loss =  0.000504079507663846\n",
            "Batch Training Loss =  0.0005133244558237493\n",
            "Batch Training Loss =  0.0005055112997069955\n",
            "Batch Training Loss =  0.000513484759721905\n",
            "Batch Training Loss =  0.0005191760137677193\n",
            "Batch Training Loss =  0.0005203399923630059\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0005348578561097383\n",
            "Batch Training Loss =  0.0005131823709234595\n",
            "Batch Training Loss =  0.0005367596168071032\n",
            "Batch Training Loss =  0.0005491197225637734\n",
            "Batch Training Loss =  0.0005358711932785809\n",
            "Batch Training Loss =  0.0005616776179522276\n",
            "Batch Training Loss =  0.0005653096595779061\n",
            "Batch Training Loss =  0.0005709052202291787\n",
            "Batch Training Loss =  0.000580608902964741\n",
            "Batch Training Loss =  0.0005775431054644287\n",
            "Batch Training Loss =  0.0005990375066176057\n",
            "Batch Training Loss =  0.000586624548304826\n",
            "Batch Training Loss =  0.0006017130217514932\n",
            "Batch Training Loss =  0.0005864514969289303\n",
            "Batch Training Loss =  0.0006064540357328951\n",
            "Batch Training Loss =  0.0006049550138413906\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0006062564207240939\n",
            "Batch Training Loss =  0.0005970733473077416\n",
            "Batch Training Loss =  0.0006245357217267156\n",
            "Batch Training Loss =  0.0006398468394763768\n",
            "Batch Training Loss =  0.0006339017418213189\n",
            "Batch Training Loss =  0.0006431282381527126\n",
            "Batch Training Loss =  0.0006361959967762232\n",
            "Batch Training Loss =  0.0006472048116847873\n",
            "Batch Training Loss =  0.0006659008795395494\n",
            "Batch Training Loss =  0.0006623794906772673\n",
            "Batch Training Loss =  0.0006801963318139315\n",
            "Batch Training Loss =  0.0006654864409938455\n",
            "Batch Training Loss =  0.0006572305574081838\n",
            "Batch Training Loss =  0.0006754030473530293\n",
            "Batch Training Loss =  0.0006974589778110385\n",
            "Batch Training Loss =  0.0006878688000142574\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0006882348097860813\n",
            "Batch Training Loss =  0.0006917872815392911\n",
            "Batch Training Loss =  0.0006882920861244202\n",
            "Batch Training Loss =  0.0006987394881434739\n",
            "Batch Training Loss =  0.0007108674035407603\n",
            "Batch Training Loss =  0.0007363034528680146\n",
            "Batch Training Loss =  0.0007350319065153599\n",
            "Batch Training Loss =  0.0007225730223581195\n",
            "Batch Training Loss =  0.0007535898475907743\n",
            "Batch Training Loss =  0.0007206459413282573\n",
            "Batch Training Loss =  0.000742232718039304\n",
            "Batch Training Loss =  0.0007484634988941252\n",
            "Batch Training Loss =  0.0007270571077242494\n",
            "Batch Training Loss =  0.0007536167395301163\n",
            "Batch Training Loss =  0.0007436857558786869\n",
            "Batch Training Loss =  0.0007518940255977213\n",
            "Validation Loss in this epoch is 0.004\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0007512642187066376\n",
            "Batch Training Loss =  0.0007388849044218659\n",
            "Batch Training Loss =  0.0007703052251599729\n",
            "Batch Training Loss =  0.00076916697435081\n",
            "Batch Training Loss =  0.0007552018505521119\n",
            "Batch Training Loss =  0.0007820467581041157\n",
            "Batch Training Loss =  0.0007974217878654599\n",
            "Batch Training Loss =  0.0007765298360027373\n",
            "Batch Training Loss =  0.0008100753766484559\n",
            "Batch Training Loss =  0.0007849836838431656\n",
            "Batch Training Loss =  0.0007790476083755493\n",
            "Batch Training Loss =  0.000798088381998241\n",
            "Batch Training Loss =  0.0007841199403628707\n",
            "Batch Training Loss =  0.0007783086039125919\n",
            "Batch Training Loss =  0.0008072771597653627\n",
            "Batch Training Loss =  0.0008061080588959157\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0008234639535658062\n",
            "Batch Training Loss =  0.0007917517214082181\n",
            "Batch Training Loss =  0.0008194827823899686\n",
            "Batch Training Loss =  0.0008295820443890989\n",
            "Batch Training Loss =  0.0008496231748722494\n",
            "Batch Training Loss =  0.0008244814816862345\n",
            "Batch Training Loss =  0.0008110162452794611\n",
            "Batch Training Loss =  0.0008288501412607729\n",
            "Batch Training Loss =  0.0008196040289476514\n",
            "Batch Training Loss =  0.0008181773591786623\n",
            "Batch Training Loss =  0.000832320423796773\n",
            "Batch Training Loss =  0.0008225335041061044\n",
            "Batch Training Loss =  0.0008398707723245025\n",
            "Batch Training Loss =  0.0008556813700124621\n",
            "Batch Training Loss =  0.0008428524597547948\n",
            "Batch Training Loss =  0.0008639334118925035\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0008631437085568905\n",
            "Batch Training Loss =  0.000857831968460232\n",
            "Batch Training Loss =  0.0008771377615630627\n",
            "Batch Training Loss =  0.0008641260210424662\n",
            "Batch Training Loss =  0.0008577111293561757\n",
            "Batch Training Loss =  0.0008709733374416828\n",
            "Batch Training Loss =  0.0008638815488666296\n",
            "Batch Training Loss =  0.0008860325324349105\n",
            "Batch Training Loss =  0.0008806465775705874\n",
            "Batch Training Loss =  0.000875149096827954\n",
            "Batch Training Loss =  0.0008628310170024633\n",
            "Batch Training Loss =  0.0008770955610089004\n",
            "Batch Training Loss =  0.0008814659086056054\n",
            "Batch Training Loss =  0.0008710401016287506\n",
            "Batch Training Loss =  0.0008996764081530273\n",
            "Batch Training Loss =  0.000880545238032937\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0008717013406567276\n",
            "Batch Training Loss =  0.0008803684031590819\n",
            "Batch Training Loss =  0.0009128022356890142\n",
            "Batch Training Loss =  0.0008974687661975622\n",
            "Batch Training Loss =  0.000900626415386796\n",
            "Batch Training Loss =  0.0008949980838224292\n",
            "Batch Training Loss =  0.0008895891951397061\n",
            "Batch Training Loss =  0.0009156505111604929\n",
            "Batch Training Loss =  0.000882185238879174\n",
            "Batch Training Loss =  0.0009322704281657934\n",
            "Batch Training Loss =  0.0009222926455549896\n",
            "Batch Training Loss =  0.0009224847890436649\n",
            "Batch Training Loss =  0.0009141765767708421\n",
            "Batch Training Loss =  0.0009122213814407587\n",
            "Batch Training Loss =  0.0009374399669468403\n",
            "Batch Training Loss =  0.0009420604328624904\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0009351272019557655\n",
            "Batch Training Loss =  0.0009221393847838044\n",
            "Batch Training Loss =  0.0009272668394260108\n",
            "Batch Training Loss =  0.0009341083350591362\n",
            "Batch Training Loss =  0.0009370554471388459\n",
            "Batch Training Loss =  0.0009273370378650725\n",
            "Batch Training Loss =  0.0009495180565863848\n",
            "Batch Training Loss =  0.0009451481164433062\n",
            "Batch Training Loss =  0.00094412179896608\n",
            "Batch Training Loss =  0.0009423065348528326\n",
            "Batch Training Loss =  0.0009672537562437356\n",
            "Batch Training Loss =  0.0009565509972162545\n",
            "Batch Training Loss =  0.0009396816021762788\n",
            "Batch Training Loss =  0.00095083296764642\n",
            "Batch Training Loss =  0.0009722932591103017\n",
            "Batch Training Loss =  0.0009506947826594114\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0009436463005840778\n",
            "Batch Training Loss =  0.0009579904144629836\n",
            "Batch Training Loss =  0.0009655861649662256\n",
            "Batch Training Loss =  0.000943792168982327\n",
            "Batch Training Loss =  0.0009678567876107991\n",
            "Batch Training Loss =  0.0009914204711094499\n",
            "Batch Training Loss =  0.000983198988251388\n",
            "Batch Training Loss =  0.0009813238866627216\n",
            "Batch Training Loss =  0.0009679613285697997\n",
            "Batch Training Loss =  0.000984519487246871\n",
            "Batch Training Loss =  0.0009847474284470081\n",
            "Batch Training Loss =  0.0009978283196687698\n",
            "Batch Training Loss =  0.0009693432366475463\n",
            "Batch Training Loss =  0.000999425770714879\n",
            "Batch Training Loss =  0.0009710562881082296\n",
            "Batch Training Loss =  0.0009980994509533048\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0009842190193012357\n",
            "Batch Training Loss =  0.0009499883162789047\n",
            "Batch Training Loss =  0.0009979894384741783\n",
            "Batch Training Loss =  0.001002846285700798\n",
            "Batch Training Loss =  0.001002446049824357\n",
            "Batch Training Loss =  0.000994364032521844\n",
            "Batch Training Loss =  0.0009998118039220572\n",
            "Batch Training Loss =  0.0010038666659966111\n",
            "Batch Training Loss =  0.0010108936112374067\n",
            "Batch Training Loss =  0.0010297381086274981\n",
            "Batch Training Loss =  0.0010228108149021864\n",
            "Batch Training Loss =  0.0010311704827472568\n",
            "Batch Training Loss =  0.0010095436591655016\n",
            "Batch Training Loss =  0.0010097079211845994\n",
            "Batch Training Loss =  0.0010380862513557076\n",
            "Batch Training Loss =  0.001025910722091794\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0010225640144199133\n",
            "Batch Training Loss =  0.0010103870881721377\n",
            "Batch Training Loss =  0.0010081439977511764\n",
            "Batch Training Loss =  0.0010462888749316335\n",
            "Batch Training Loss =  0.0010313150705769658\n",
            "Batch Training Loss =  0.0010315653635188937\n",
            "Batch Training Loss =  0.00104217987973243\n",
            "Batch Training Loss =  0.0010609838645905256\n",
            "Batch Training Loss =  0.0010401698527857661\n",
            "Batch Training Loss =  0.0010366401402279735\n",
            "Batch Training Loss =  0.0010337019339203835\n",
            "Batch Training Loss =  0.001032434869557619\n",
            "Batch Training Loss =  0.0010297680273652077\n",
            "Batch Training Loss =  0.0010571007151156664\n",
            "Batch Training Loss =  0.0010403762571513653\n",
            "Batch Training Loss =  0.0010661175474524498\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.00106527388561517\n",
            "Batch Training Loss =  0.0010544084943830967\n",
            "Batch Training Loss =  0.001048488193191588\n",
            "Batch Training Loss =  0.0010542416712269187\n",
            "Batch Training Loss =  0.0010652411729097366\n",
            "Batch Training Loss =  0.0010549880098551512\n",
            "Batch Training Loss =  0.0010542202508077025\n",
            "Batch Training Loss =  0.0010561818489804864\n",
            "Batch Training Loss =  0.0010571146849542856\n",
            "Batch Training Loss =  0.0010737875709310174\n",
            "Batch Training Loss =  0.0010666577145457268\n",
            "Batch Training Loss =  0.0010709164198487997\n",
            "Batch Training Loss =  0.0010864160722121596\n",
            "Batch Training Loss =  0.0010675482917577028\n",
            "Batch Training Loss =  0.0010823070770129561\n",
            "Batch Training Loss =  0.0010812068358063698\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0010949060088023543\n",
            "Batch Training Loss =  0.0010762226302176714\n",
            "Batch Training Loss =  0.0011002675164490938\n",
            "Batch Training Loss =  0.0010926569811999798\n",
            "Batch Training Loss =  0.001092118676751852\n",
            "Batch Training Loss =  0.0010692921932786703\n",
            "Batch Training Loss =  0.0010943218367174268\n",
            "Batch Training Loss =  0.0011182904709130526\n",
            "Batch Training Loss =  0.001091987593099475\n",
            "Batch Training Loss =  0.001086218748241663\n",
            "Batch Training Loss =  0.0011091574560850859\n",
            "Batch Training Loss =  0.0010985656408593059\n",
            "Batch Training Loss =  0.0010978742502629757\n",
            "Batch Training Loss =  0.001116693951189518\n",
            "Batch Training Loss =  0.0011092856293544173\n",
            "Batch Training Loss =  0.0011258050799369812\n",
            "Validation Loss in this epoch is 0.005\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0011288750683888793\n",
            "Batch Training Loss =  0.0011062782723456621\n",
            "Batch Training Loss =  0.001103289076127112\n",
            "Batch Training Loss =  0.001114449929445982\n",
            "Batch Training Loss =  0.0011257570004090667\n",
            "Batch Training Loss =  0.0011152036022394896\n",
            "Batch Training Loss =  0.0011261613108217716\n",
            "Batch Training Loss =  0.0011202305322512984\n",
            "Batch Training Loss =  0.0011603983584791422\n",
            "Batch Training Loss =  0.0011330782435834408\n",
            "Batch Training Loss =  0.0011478408705443144\n",
            "Batch Training Loss =  0.0011189025826752186\n",
            "Batch Training Loss =  0.0011453571496531367\n",
            "Batch Training Loss =  0.00114517193287611\n",
            "Batch Training Loss =  0.00114021310582757\n",
            "Batch Training Loss =  0.001143739209510386\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.001143183559179306\n",
            "Batch Training Loss =  0.0011620892910286784\n",
            "Batch Training Loss =  0.0011553603690117598\n",
            "Batch Training Loss =  0.0011394511675462127\n",
            "Batch Training Loss =  0.0011607856722548604\n",
            "Batch Training Loss =  0.0011744245421141386\n",
            "Batch Training Loss =  0.001150789437815547\n",
            "Batch Training Loss =  0.0011589424684643745\n",
            "Batch Training Loss =  0.0011456204811111093\n",
            "Batch Training Loss =  0.0011636693961918354\n",
            "Batch Training Loss =  0.0011487766169011593\n",
            "Batch Training Loss =  0.00116442097350955\n",
            "Batch Training Loss =  0.0011634432012215257\n",
            "Batch Training Loss =  0.0011712390696629882\n",
            "Batch Training Loss =  0.0011556377867236733\n",
            "Batch Training Loss =  0.0011709098471328616\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0011739851906895638\n",
            "Batch Training Loss =  0.0011732939165085554\n",
            "Batch Training Loss =  0.0012028232449665666\n",
            "Batch Training Loss =  0.001201130449771881\n",
            "Batch Training Loss =  0.0011774534359574318\n",
            "Batch Training Loss =  0.0011778948828577995\n",
            "Batch Training Loss =  0.0011821901425719261\n",
            "Batch Training Loss =  0.0011740396730601788\n",
            "Batch Training Loss =  0.0011982255382463336\n",
            "Batch Training Loss =  0.0011787115363404155\n",
            "Batch Training Loss =  0.00119359593372792\n",
            "Batch Training Loss =  0.0011950850021094084\n",
            "Batch Training Loss =  0.0012222789227962494\n",
            "Batch Training Loss =  0.0012010657228529453\n",
            "Batch Training Loss =  0.0011881939135491848\n",
            "Batch Training Loss =  0.0012094079283997416\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0012022507144138217\n",
            "Batch Training Loss =  0.0012048252392560244\n",
            "Batch Training Loss =  0.0012240925570949912\n",
            "Batch Training Loss =  0.0012375622754916549\n",
            "Batch Training Loss =  0.0012274231994524598\n",
            "Batch Training Loss =  0.001218860736116767\n",
            "Batch Training Loss =  0.0012021934380754828\n",
            "Batch Training Loss =  0.0012102905893698335\n",
            "Batch Training Loss =  0.0012186798267066479\n",
            "Batch Training Loss =  0.0012308399891480803\n",
            "Batch Training Loss =  0.0012094362173229456\n",
            "Batch Training Loss =  0.0012249450664967299\n",
            "Batch Training Loss =  0.001219592522829771\n",
            "Batch Training Loss =  0.0012385934824123979\n",
            "Batch Training Loss =  0.001225898740813136\n",
            "Batch Training Loss =  0.0012451205402612686\n",
            "Validation Loss in this epoch is 0.007\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.001269096857868135\n",
            "Batch Training Loss =  0.00122634950093925\n",
            "Batch Training Loss =  0.0012353593483567238\n",
            "Batch Training Loss =  0.0012521910248324275\n",
            "Batch Training Loss =  0.0012366091832518578\n",
            "Batch Training Loss =  0.0012799768010154366\n",
            "Batch Training Loss =  0.001247531734406948\n",
            "Batch Training Loss =  0.0012537837028503418\n",
            "Batch Training Loss =  0.001264779595658183\n",
            "Batch Training Loss =  0.001255735638551414\n",
            "Batch Training Loss =  0.0012497318675741553\n",
            "Batch Training Loss =  0.0012658059131354094\n",
            "Batch Training Loss =  0.0012524501653388143\n",
            "Batch Training Loss =  0.001255623297765851\n",
            "Batch Training Loss =  0.0012678524944931269\n",
            "Batch Training Loss =  0.0012712562456727028\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0012472138041630387\n",
            "Batch Training Loss =  0.0012756073847413063\n",
            "Batch Training Loss =  0.0012956438586115837\n",
            "Batch Training Loss =  0.0012769019231200218\n",
            "Batch Training Loss =  0.0012785578146576881\n",
            "Batch Training Loss =  0.0012736518401652575\n",
            "Batch Training Loss =  0.0012857279507443309\n",
            "Batch Training Loss =  0.0012920473236590624\n",
            "Batch Training Loss =  0.0012857046676799655\n",
            "Batch Training Loss =  0.0013092244043946266\n",
            "Batch Training Loss =  0.0012808204628527164\n",
            "Batch Training Loss =  0.0012831182684749365\n",
            "Batch Training Loss =  0.001297294395044446\n",
            "Batch Training Loss =  0.0013120992807671428\n",
            "Batch Training Loss =  0.0013040193589404225\n",
            "Batch Training Loss =  0.001300716307014227\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0013118070783093572\n",
            "Batch Training Loss =  0.0012979883467778563\n",
            "Batch Training Loss =  0.0013171349419280887\n",
            "Batch Training Loss =  0.001327371341176331\n",
            "Batch Training Loss =  0.0012875513639301062\n",
            "Batch Training Loss =  0.0013376715360209346\n",
            "Batch Training Loss =  0.0013289814814925194\n",
            "Batch Training Loss =  0.0013255755184218287\n",
            "Batch Training Loss =  0.0013055686140432954\n",
            "Batch Training Loss =  0.001310676452703774\n",
            "Batch Training Loss =  0.0013372887624427676\n",
            "Batch Training Loss =  0.0013236034428700805\n",
            "Batch Training Loss =  0.0013354906113818288\n",
            "Batch Training Loss =  0.0013394620036706328\n",
            "Batch Training Loss =  0.0013151324819773436\n",
            "Batch Training Loss =  0.0013201036490499973\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0013316802214831114\n",
            "Batch Training Loss =  0.001348984893411398\n",
            "Batch Training Loss =  0.001333481166511774\n",
            "Batch Training Loss =  0.0013520492939278483\n",
            "Batch Training Loss =  0.001343599520623684\n",
            "Batch Training Loss =  0.0013602764811366796\n",
            "Batch Training Loss =  0.0013614484341815114\n",
            "Batch Training Loss =  0.0013595212949439883\n",
            "Batch Training Loss =  0.0013462671777233481\n",
            "Batch Training Loss =  0.0013586757704615593\n",
            "Batch Training Loss =  0.0013474988518282771\n",
            "Batch Training Loss =  0.0013637137599289417\n",
            "Batch Training Loss =  0.0013616516953334212\n",
            "Batch Training Loss =  0.0013612109469249845\n",
            "Batch Training Loss =  0.0013700970448553562\n",
            "Batch Training Loss =  0.0013718090485781431\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0013799407752230763\n",
            "Batch Training Loss =  0.0013975920155644417\n",
            "Batch Training Loss =  0.0013694027438759804\n",
            "Batch Training Loss =  0.0013763685710728168\n",
            "Batch Training Loss =  0.0013805112103000283\n",
            "Batch Training Loss =  0.001389958430081606\n",
            "Batch Training Loss =  0.0013966463739052415\n",
            "Batch Training Loss =  0.0013955815229564905\n",
            "Batch Training Loss =  0.0013996906345710158\n",
            "Batch Training Loss =  0.0014053561026230454\n",
            "Batch Training Loss =  0.0013965837424620986\n",
            "Batch Training Loss =  0.0014030272141098976\n",
            "Batch Training Loss =  0.0014043744886294007\n",
            "Batch Training Loss =  0.0013821788597851992\n",
            "Batch Training Loss =  0.0013935674214735627\n",
            "Batch Training Loss =  0.0014003958785906434\n",
            "Validation Loss in this epoch is 0.007\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0014050728641450405\n",
            "Batch Training Loss =  0.0014088484458625317\n",
            "Batch Training Loss =  0.001417670981027186\n",
            "Batch Training Loss =  0.0014238900039345026\n",
            "Batch Training Loss =  0.0014221100136637688\n",
            "Batch Training Loss =  0.0014234345871955156\n",
            "Batch Training Loss =  0.0014325684169307351\n",
            "Batch Training Loss =  0.0014175057876855135\n",
            "Batch Training Loss =  0.0014238011790439487\n",
            "Batch Training Loss =  0.001426758710294962\n",
            "Batch Training Loss =  0.00142485904507339\n",
            "Batch Training Loss =  0.0014464090345427394\n",
            "Batch Training Loss =  0.0014287286903709173\n",
            "Batch Training Loss =  0.0014717034064233303\n",
            "Batch Training Loss =  0.001453666714951396\n",
            "Batch Training Loss =  0.0014487948501482606\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0014372100122272968\n",
            "Batch Training Loss =  0.0014320851769298315\n",
            "Batch Training Loss =  0.0014552706852555275\n",
            "Batch Training Loss =  0.001472713309340179\n",
            "Batch Training Loss =  0.0014691523974761367\n",
            "Batch Training Loss =  0.001461911597289145\n",
            "Batch Training Loss =  0.0014655471313744783\n",
            "Batch Training Loss =  0.001459574676118791\n",
            "Batch Training Loss =  0.0014516946393996477\n",
            "Batch Training Loss =  0.001445271191187203\n",
            "Batch Training Loss =  0.001448968192562461\n",
            "Batch Training Loss =  0.0014704118948429823\n",
            "Batch Training Loss =  0.0014722165651619434\n",
            "Batch Training Loss =  0.0014973669312894344\n",
            "Batch Training Loss =  0.0014770484995096922\n",
            "Batch Training Loss =  0.001501132152043283\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0015011337818577886\n",
            "Batch Training Loss =  0.0014820701908320189\n",
            "Batch Training Loss =  0.001479766913689673\n",
            "Batch Training Loss =  0.0014996124664321542\n",
            "Batch Training Loss =  0.0014993064105510712\n",
            "Batch Training Loss =  0.001484085456468165\n",
            "Batch Training Loss =  0.0014827223494648933\n",
            "Batch Training Loss =  0.0015054972609505057\n",
            "Batch Training Loss =  0.0015093277906998992\n",
            "Batch Training Loss =  0.001499874982982874\n",
            "Batch Training Loss =  0.0015167166711762547\n",
            "Batch Training Loss =  0.0015031024813652039\n",
            "Batch Training Loss =  0.001495702425017953\n",
            "Batch Training Loss =  0.0015091702807694674\n",
            "Batch Training Loss =  0.0015177057357504964\n",
            "Batch Training Loss =  0.001514845178462565\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0015194572042673826\n",
            "Batch Training Loss =  0.0015152483247220516\n",
            "Batch Training Loss =  0.0015294732293114066\n",
            "Batch Training Loss =  0.0015305045526474714\n",
            "Batch Training Loss =  0.0015199787449091673\n",
            "Batch Training Loss =  0.001540031167678535\n",
            "Batch Training Loss =  0.0015338341472670436\n",
            "Batch Training Loss =  0.001525041414424777\n",
            "Batch Training Loss =  0.0015401176642626524\n",
            "Batch Training Loss =  0.001575459842570126\n",
            "Batch Training Loss =  0.0015330446185544133\n",
            "Batch Training Loss =  0.0015317924553528428\n",
            "Batch Training Loss =  0.0015624914085492492\n",
            "Batch Training Loss =  0.0015566629590466619\n",
            "Batch Training Loss =  0.0015301876701414585\n",
            "Batch Training Loss =  0.001557778101414442\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.001569561194628477\n",
            "Batch Training Loss =  0.0015619286568835378\n",
            "Batch Training Loss =  0.0015626625390723348\n",
            "Batch Training Loss =  0.0015516761923208833\n",
            "Batch Training Loss =  0.0015659801429137588\n",
            "Batch Training Loss =  0.0015783256385475397\n",
            "Batch Training Loss =  0.0015772127080708742\n",
            "Batch Training Loss =  0.001590063446201384\n",
            "Batch Training Loss =  0.0015718275681138039\n",
            "Batch Training Loss =  0.0015838082181289792\n",
            "Batch Training Loss =  0.001587067381478846\n",
            "Batch Training Loss =  0.001574239693582058\n",
            "Batch Training Loss =  0.00157975556794554\n",
            "Batch Training Loss =  0.0016059320187196136\n",
            "Batch Training Loss =  0.0015883916057646275\n",
            "Batch Training Loss =  0.001591533306054771\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0015945686027407646\n",
            "Batch Training Loss =  0.0015923872124403715\n",
            "Batch Training Loss =  0.0016167513094842434\n",
            "Batch Training Loss =  0.0016305121826007962\n",
            "Batch Training Loss =  0.001614208216778934\n",
            "Batch Training Loss =  0.0016062090871855617\n",
            "Batch Training Loss =  0.001600967487320304\n",
            "Batch Training Loss =  0.0016095525352284312\n",
            "Batch Training Loss =  0.0016242493875324726\n",
            "Batch Training Loss =  0.0016047641402110457\n",
            "Batch Training Loss =  0.0016356706619262695\n",
            "Batch Training Loss =  0.0016125553520396352\n",
            "Batch Training Loss =  0.0016225543804466724\n",
            "Batch Training Loss =  0.0016498728655278683\n",
            "Batch Training Loss =  0.0016220323741436005\n",
            "Batch Training Loss =  0.0016552112065255642\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0016372204991057515\n",
            "Batch Training Loss =  0.0016434723511338234\n",
            "Batch Training Loss =  0.0016481237253174186\n",
            "Batch Training Loss =  0.0016431588446721435\n",
            "Batch Training Loss =  0.0016478587640449405\n",
            "Batch Training Loss =  0.0016543067758902907\n",
            "Batch Training Loss =  0.0016487006796523929\n",
            "Batch Training Loss =  0.0016547766281291842\n",
            "Batch Training Loss =  0.00167540879920125\n",
            "Batch Training Loss =  0.001661974936723709\n",
            "Batch Training Loss =  0.0016600320814177394\n",
            "Batch Training Loss =  0.001654169987887144\n",
            "Batch Training Loss =  0.0016615921631455421\n",
            "Batch Training Loss =  0.0016814421396702528\n",
            "Batch Training Loss =  0.0016862046904861927\n",
            "Batch Training Loss =  0.001673586550168693\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0016941252397373319\n",
            "Batch Training Loss =  0.0016669019823893905\n",
            "Batch Training Loss =  0.001677621272392571\n",
            "Batch Training Loss =  0.0016761429142206907\n",
            "Batch Training Loss =  0.0016961127985268831\n",
            "Batch Training Loss =  0.0016754950629547238\n",
            "Batch Training Loss =  0.0016912345308810472\n",
            "Batch Training Loss =  0.0017066035652533174\n",
            "Batch Training Loss =  0.001700358116067946\n",
            "Batch Training Loss =  0.0017162971198558807\n",
            "Batch Training Loss =  0.0016957554034888744\n",
            "Batch Training Loss =  0.0017131465720012784\n",
            "Batch Training Loss =  0.0017171098152175546\n",
            "Batch Training Loss =  0.0017068290617316961\n",
            "Batch Training Loss =  0.0017262314213439822\n",
            "Batch Training Loss =  0.0017240659799426794\n",
            "Validation Loss in this epoch is 0.007\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0016994753386825323\n",
            "Batch Training Loss =  0.0017361837672069669\n",
            "Batch Training Loss =  0.001713680219836533\n",
            "Batch Training Loss =  0.0017417293274775147\n",
            "Batch Training Loss =  0.0017206579213961959\n",
            "Batch Training Loss =  0.0017260484164580703\n",
            "Batch Training Loss =  0.0017530062468722463\n",
            "Batch Training Loss =  0.0017593168886378407\n",
            "Batch Training Loss =  0.0017410225700587034\n",
            "Batch Training Loss =  0.0017573131481185555\n",
            "Batch Training Loss =  0.0017435397021472454\n",
            "Batch Training Loss =  0.0017394832102581859\n",
            "Batch Training Loss =  0.0017614291282370687\n",
            "Batch Training Loss =  0.0017546097515150905\n",
            "Batch Training Loss =  0.0017467800062149763\n",
            "Batch Training Loss =  0.0017726289806887507\n",
            "Validation Loss in this epoch is 0.007\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.0017644253093749285\n",
            "Batch Training Loss =  0.0017706124344840646\n",
            "Batch Training Loss =  0.0017627538181841373\n",
            "Batch Training Loss =  0.0017907152650877833\n",
            "Batch Training Loss =  0.001775443903170526\n",
            "Batch Training Loss =  0.0017823034431785345\n",
            "Batch Training Loss =  0.0017795306630432606\n",
            "Batch Training Loss =  0.0017660747980698943\n",
            "Batch Training Loss =  0.0017854140605777502\n",
            "Batch Training Loss =  0.0017666687490418553\n",
            "Batch Training Loss =  0.0017980380216613412\n",
            "Batch Training Loss =  0.0018114629201591015\n",
            "Batch Training Loss =  0.0017912080511450768\n",
            "Batch Training Loss =  0.0017869726289063692\n",
            "Batch Training Loss =  0.001817396841943264\n",
            "Batch Training Loss =  0.0017972012283280492\n",
            "Validation Loss in this epoch is 0.007\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.0017947590677067637\n",
            "Batch Training Loss =  0.0018262490630149841\n",
            "Batch Training Loss =  0.0018142539774999022\n",
            "Batch Training Loss =  0.001809898647479713\n",
            "Batch Training Loss =  0.0018168148817494512\n",
            "Batch Training Loss =  0.0018150946125388145\n",
            "Batch Training Loss =  0.001822059042751789\n",
            "Batch Training Loss =  0.0018491697264835238\n",
            "Batch Training Loss =  0.0018294864566996694\n",
            "Batch Training Loss =  0.0018376143416389823\n",
            "Batch Training Loss =  0.0018345991848036647\n",
            "Batch Training Loss =  0.0018446968169882894\n",
            "Batch Training Loss =  0.0018321200041100383\n",
            "Batch Training Loss =  0.0018451981013640761\n",
            "Batch Training Loss =  0.0018440387211740017\n",
            "Batch Training Loss =  0.0018604042707011104\n",
            "Validation Loss in this epoch is 0.006\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.0018526411149650812\n",
            "Batch Training Loss =  0.001868117251433432\n",
            "Batch Training Loss =  0.0018644616939127445\n",
            "Batch Training Loss =  0.0018487120978534222\n",
            "Batch Training Loss =  0.0018517719581723213\n",
            "Batch Training Loss =  0.0018564456840977073\n",
            "Batch Training Loss =  0.0018624893855303526\n",
            "Batch Training Loss =  0.0018741112435236573\n",
            "Batch Training Loss =  0.0018808613531291485\n",
            "Batch Training Loss =  0.0018753950716927648\n",
            "Batch Training Loss =  0.0018722021486610174\n",
            "Batch Training Loss =  0.0018817265518009663\n",
            "Batch Training Loss =  0.0019067204557359219\n",
            "Batch Training Loss =  0.0018899681745097041\n",
            "Batch Training Loss =  0.0019196803914383054\n",
            "Batch Training Loss =  0.0018985798815265298\n",
            "Validation Loss in this epoch is 0.008\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.0018973029218614101\n",
            "Batch Training Loss =  0.0019087031250819564\n",
            "Batch Training Loss =  0.0019004358910024166\n",
            "Batch Training Loss =  0.0019051071722060442\n",
            "Batch Training Loss =  0.00189957395195961\n",
            "Batch Training Loss =  0.0019076131284236908\n",
            "Batch Training Loss =  0.0019417303847149014\n",
            "Batch Training Loss =  0.0019203413976356387\n",
            "Batch Training Loss =  0.0019394524861127138\n",
            "Batch Training Loss =  0.00191374565474689\n",
            "Batch Training Loss =  0.0019186122808605433\n",
            "Batch Training Loss =  0.0019331513904035091\n",
            "Batch Training Loss =  0.0019312271615490317\n",
            "Batch Training Loss =  0.0019498259061947465\n",
            "Batch Training Loss =  0.0019450797699391842\n",
            "Batch Training Loss =  0.001921732909977436\n",
            "Validation Loss in this epoch is 0.007\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.0019422125769779086\n",
            "Batch Training Loss =  0.001931474544107914\n",
            "Batch Training Loss =  0.0019673367496579885\n",
            "Batch Training Loss =  0.0019574088510125875\n",
            "Batch Training Loss =  0.001967201940715313\n",
            "Batch Training Loss =  0.0019531326834112406\n",
            "Batch Training Loss =  0.0019491969142109156\n",
            "Batch Training Loss =  0.0019546877592802048\n",
            "Batch Training Loss =  0.001959588611498475\n",
            "Batch Training Loss =  0.0019742429722100496\n",
            "Batch Training Loss =  0.001973723992705345\n",
            "Batch Training Loss =  0.001975219463929534\n",
            "Batch Training Loss =  0.001983506139367819\n",
            "Batch Training Loss =  0.0019806583877652884\n",
            "Batch Training Loss =  0.0019989851862192154\n",
            "Batch Training Loss =  0.0019974615424871445\n",
            "Validation Loss in this epoch is 0.008\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.0019936724565923214\n",
            "Batch Training Loss =  0.0019940142519772053\n",
            "Batch Training Loss =  0.0019925152882933617\n",
            "Batch Training Loss =  0.0020106015726923943\n",
            "Batch Training Loss =  0.0019930799026042223\n",
            "Batch Training Loss =  0.001995788188651204\n",
            "Batch Training Loss =  0.002016226062551141\n",
            "Batch Training Loss =  0.0019943625666201115\n",
            "Batch Training Loss =  0.0020039521623402834\n",
            "Batch Training Loss =  0.0020173657685518265\n",
            "Batch Training Loss =  0.002022496424615383\n",
            "Batch Training Loss =  0.0020270782988518476\n",
            "Batch Training Loss =  0.0020283563062548637\n",
            "Batch Training Loss =  0.002051926450803876\n",
            "Batch Training Loss =  0.0020225055050104856\n",
            "Batch Training Loss =  0.002045507775619626\n",
            "Validation Loss in this epoch is 0.008\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.0020417526829987764\n",
            "Batch Training Loss =  0.002043140586465597\n",
            "Batch Training Loss =  0.0020466321147978306\n",
            "Batch Training Loss =  0.00203154981136322\n",
            "Batch Training Loss =  0.002052714815363288\n",
            "Batch Training Loss =  0.0020447936840355396\n",
            "Batch Training Loss =  0.002059423364698887\n",
            "Batch Training Loss =  0.002054165117442608\n",
            "Batch Training Loss =  0.0020679228473454714\n",
            "Batch Training Loss =  0.00206282758153975\n",
            "Batch Training Loss =  0.0020628818310797215\n",
            "Batch Training Loss =  0.0020718120504170656\n",
            "Batch Training Loss =  0.002058940939605236\n",
            "Batch Training Loss =  0.0020745834335684776\n",
            "Batch Training Loss =  0.002088286215439439\n",
            "Batch Training Loss =  0.0021250371355563402\n",
            "Validation Loss in this epoch is 0.007\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.0020899688825011253\n",
            "Batch Training Loss =  0.002078213030472398\n",
            "Batch Training Loss =  0.0020867090206593275\n",
            "Batch Training Loss =  0.0020750996191054583\n",
            "Batch Training Loss =  0.002089272253215313\n",
            "Batch Training Loss =  0.00211387244053185\n",
            "Batch Training Loss =  0.002104660961776972\n",
            "Batch Training Loss =  0.0021025186870247126\n",
            "Batch Training Loss =  0.0021091410890221596\n",
            "Batch Training Loss =  0.0021115841809660196\n",
            "Batch Training Loss =  0.0021191097330302\n",
            "Batch Training Loss =  0.0021496594417840242\n",
            "Batch Training Loss =  0.002150952350348234\n",
            "Batch Training Loss =  0.002139432355761528\n",
            "Batch Training Loss =  0.0021232576109468937\n",
            "Batch Training Loss =  0.0021291286684572697\n",
            "Validation Loss in this epoch is 0.007\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.0021359578240662813\n",
            "Batch Training Loss =  0.0021576217841356993\n",
            "Batch Training Loss =  0.0021308762952685356\n",
            "Batch Training Loss =  0.002137510571628809\n",
            "Batch Training Loss =  0.0021721688099205494\n",
            "Batch Training Loss =  0.002152792178094387\n",
            "Batch Training Loss =  0.0021475739777088165\n",
            "Batch Training Loss =  0.002146870829164982\n",
            "Batch Training Loss =  0.002168721752241254\n",
            "Batch Training Loss =  0.00214953999966383\n",
            "Batch Training Loss =  0.002157089300453663\n",
            "Batch Training Loss =  0.002178345574066043\n",
            "Batch Training Loss =  0.0021663035731762648\n",
            "Batch Training Loss =  0.0021848028991371393\n",
            "Batch Training Loss =  0.002190330531448126\n",
            "Batch Training Loss =  0.0021674097515642643\n",
            "Validation Loss in this epoch is 0.008\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.0021741939708590508\n",
            "Batch Training Loss =  0.002193648600950837\n",
            "Batch Training Loss =  0.0021996772848069668\n",
            "Batch Training Loss =  0.002215923275798559\n",
            "Batch Training Loss =  0.002188486512750387\n",
            "Batch Training Loss =  0.002195592038333416\n",
            "Batch Training Loss =  0.002213960513472557\n",
            "Batch Training Loss =  0.0022247794549912214\n",
            "Batch Training Loss =  0.0022121567744761705\n",
            "Batch Training Loss =  0.0022200066596269608\n",
            "Batch Training Loss =  0.0022055867593735456\n",
            "Batch Training Loss =  0.00221531605347991\n",
            "Batch Training Loss =  0.0022081146016716957\n",
            "Batch Training Loss =  0.0022325606551021338\n",
            "Batch Training Loss =  0.0022300337441265583\n",
            "Batch Training Loss =  0.00222939345985651\n",
            "Validation Loss in this epoch is 0.009\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.002248554490506649\n",
            "Batch Training Loss =  0.0022237044759094715\n",
            "Batch Training Loss =  0.002242616843432188\n",
            "Batch Training Loss =  0.0022434419952332973\n",
            "Batch Training Loss =  0.0022487149108201265\n",
            "Batch Training Loss =  0.0022467870730906725\n",
            "Batch Training Loss =  0.0022352442611008883\n",
            "Batch Training Loss =  0.0022645550779998302\n",
            "Batch Training Loss =  0.0022659827955067158\n",
            "Batch Training Loss =  0.0022752457298338413\n",
            "Batch Training Loss =  0.0023045651614665985\n",
            "Batch Training Loss =  0.0022928225807845592\n",
            "Batch Training Loss =  0.002264406532049179\n",
            "Batch Training Loss =  0.002297228667885065\n",
            "Batch Training Loss =  0.0022901089396327734\n",
            "Batch Training Loss =  0.002283388515934348\n",
            "Validation Loss in this epoch is 0.009\n"
          ]
        }
      ],
      "source": [
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "weight_decays = [0., 0.01]\n",
        "batch_size = 50\n",
        "n_epochs = 10\n",
        "n_folds = 5\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        val_accs = []  # store validation accuracy for each fold\n",
        "        train_accs = []  # store training accuracy for each fold\n",
        "\n",
        "        # TODO: iterate over folds, remember to use \"shuffle=True\", as datapoints are not shuffled\n",
        "\n",
        "        myKFold = KFold(n_splits = n_folds, shuffle = True, random_state = 0)\n",
        "\n",
        "        # TODO: Split data into train and validation\n",
        "        for trainIndex, valIndex in myKFold.split(X):\n",
        "\n",
        "                # TODO: Create data loaders to pass to training loop\n",
        "                XTrain, XVal = X[trainIndex], X[valIndex]\n",
        "                yTrain, yVal = y[trainIndex], y[valIndex]\n",
        "\n",
        "\n",
        "                trainLoader = DataLoader(TensorDataset(XTrain, yTrain), batch_size = batch_size,shuffle = True)\n",
        "                valLoader = DataLoader(TensorDataset(XVal, yVal), batch_size = len(XVal))\n",
        "\n",
        "                # TODO: Initialize model, criterion (Cross entropy loss), and optimizer (SGD with various hyperparameters)\n",
        "                model = resNetModel\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = torch.optim.SGD(model.parameters(), lr = lr, weight_decay=wd)\n",
        "\n",
        "                # Call your training function\n",
        "                train(model, trainLoader, valLoader, n_epochs, optimizer, criterion, verbose=True)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # TODO: Use the trained model to estimate train/val accuracy\n",
        "                    # (Hint: our model outputs logits, argmax is good to get the class prediction corresponding to max logit)\n",
        "                    yPredTrain = model(XTrain).argmax(dim = 1)\n",
        "                    yPredVal = model(XVal).argmax(dim = 1)\n",
        "\n",
        "                    yPredTrain = yPredTrain.to('cpu')\n",
        "                    yTrain = yTrain.to('cpu')\n",
        "                    yPredVal = yPredVal.to('cpu')\n",
        "                    yVal = yVal.to('cpu')\n",
        "\n",
        "                    train_acc = accuracy_score(yPredTrain, yTrain)\n",
        "                    train_accs.append(train_acc)\n",
        "\n",
        "                    val_acc = accuracy_score(yPredVal, yVal)\n",
        "                    val_accs.append(val_acc)\n",
        "\n",
        "        # For each hyper-parameter, I'm storing the parameter values and the mean and standard error of accuracy in a list in \"results\".\n",
        "        train_std, train_mean = torch.std_mean(torch.tensor(train_accs))\n",
        "        val_std, val_mean = torch.std_mean(torch.tensor(val_accs))\n",
        "        rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds\n",
        "        train_se, val_se = train_std / rootn, val_std / rootn\n",
        "        # Storing learning rate, weight decay value, train mean accuracy, standard error, val mean accuracy, standard error\n",
        "        results.append((lr, wd, train_mean.item(), train_se.item(), val_mean.item(), val_se.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "58PhsVvi0j5y",
        "outputId": "ebeac453-199d-4f89-dad9-92da538785b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training results\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007071067811865475,\n        \"min\": 0.0,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1.000 +/- 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1.000 +/- 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1.000 +/- 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "pivot_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-16fe898a-60b4-4398-9a5a-2f496cb360f9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16fe898a-60b4-4398-9a5a-2f496cb360f9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-16fe898a-60b4-4398-9a5a-2f496cb360f9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-16fe898a-60b4-4398-9a5a-2f496cb360f9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7ce20575-a734-4435-bc61-af991fb8305a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ce20575-a734-4435-bc61-af991fb8305a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7ce20575-a734-4435-bc61-af991fb8305a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_20eeaa8e-4bc9-4978-869a-1a76dd22adec\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_20eeaa8e-4bc9-4978-869a-1a76dd22adec button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           1.000 +/- 0.000  1.000 +/- 0.000  1.000 +/- 0.000\n",
              "0.01           1.000 +/- 0.000  1.000 +/- 0.000  1.000 +/- 0.000"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation results\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007071067811865475,\n        \"min\": 0.0,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1.000 +/- 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1.000 +/- 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1.000 +/- 0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "pivot_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2115f4ff-c33f-4dc0-8620-a97e0d03ecbe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "      <td>1.000 +/- 0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2115f4ff-c33f-4dc0-8620-a97e0d03ecbe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2115f4ff-c33f-4dc0-8620-a97e0d03ecbe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2115f4ff-c33f-4dc0-8620-a97e0d03ecbe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3fd56fe7-55ac-486b-956d-beaa72e901a9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fd56fe7-55ac-486b-956d-beaa72e901a9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3fd56fe7-55ac-486b-956d-beaa72e901a9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_39e79722-cdd8-407f-9e98-c3c010a0f1ab\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_39e79722-cdd8-407f-9e98-c3c010a0f1ab button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           1.000 +/- 0.000  1.000 +/- 0.000  1.000 +/- 0.000\n",
              "0.01           1.000 +/- 0.000  1.000 +/- 0.000  1.000 +/- 0.000"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO [3 points]. Print the final result (should be no need to modify code)\n",
        "# You should be able to see a best train acc > 95% , and a best val acc > 80%\n",
        "\n",
        "# Create a DataFrame from the list of tuples, with labeled columns\n",
        "column_names = ['learning_rate', 'weight_decay', 'train_mean', 'train_se','val_mean', 'val_se']\n",
        "df = pd.DataFrame(results, columns=column_names)\n",
        "\n",
        "# Make pretty printable strings, with standard error bars\n",
        "df['train_output'] = df.apply(lambda row: f\"{row['train_mean']:.3f} +/- {row['train_se']:.3f}\", axis=1)\n",
        "df['val_output'] = df.apply(lambda row: f\"{row['val_mean']:.3f} +/- {row['val_se']:.3f}\", axis=1)\n",
        "\n",
        "print('Training results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='train_output')\n",
        "display(pivot_df)\n",
        "\n",
        "print('Validation results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='val_output')\n",
        "display(pivot_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGjkt2OwLxuK"
      },
      "source": [
        "## Extra Credit 4 - Tune with other hyper-parameters (architecture, n_epochs, maybe early stopping, more learning rate/weight decay settings, regularizers, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhPZaRKe4JEJ"
      },
      "outputs": [],
      "source": [
        "# Changed model architecture\n",
        "# Added more layers and increased the size of hidden layers\n",
        "\n",
        "class MyMLP(nn.Module):\n",
        "    def __init__(self, inputDimension = 3072, numOfClass = 2):\n",
        "        super(MyMLP, self).__init__()\n",
        "        self.d = inputDimension\n",
        "        self.fc1 = nn.Linear(self.d, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, 128)\n",
        "        self.fc6 = nn.Linear(128, 64)\n",
        "        self.fc7 = nn.Linear(64, numOfClass)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.relu(self.fc5(x))\n",
        "        x = self.relu(self.fc6(x))\n",
        "        return self.fc7(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4jPcN267vqn"
      },
      "outputs": [],
      "source": [
        "X, y = torch.load('/content/drive/MyDrive/CS224-FunadamentalsOfMachineLearning/HW2-DeepFakeCatDetector/hw2_data.pt')\n",
        "X = X.flatten(start_dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2dpQL457I3k"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    X = X.to('cuda')\n",
        "    resNetModel.to('cuda')\n",
        "    y = y.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UExcq-pzL9D8",
        "outputId": "79c215ba-86c4-452c-a1d0-b226f33c600a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch Training Loss =  0.694323718547821\n",
            "Batch Training Loss =  0.6929236650466919\n",
            "Batch Training Loss =  0.6940005421638489\n",
            "Batch Training Loss =  0.6929278373718262\n",
            "Batch Training Loss =  0.6939830183982849\n",
            "Batch Training Loss =  0.6929687261581421\n",
            "Batch Training Loss =  0.6907215118408203\n",
            "Batch Training Loss =  0.6941365003585815\n",
            "Batch Training Loss =  0.6941438317298889\n",
            "Batch Training Loss =  0.6932423114776611\n",
            "Batch Training Loss =  0.6930080652236938\n",
            "Batch Training Loss =  0.6919061541557312\n",
            "Batch Training Loss =  0.6938329339027405\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.6924738883972168\n",
            "Batch Training Loss =  0.6975808143615723\n",
            "Batch Training Loss =  0.6920196413993835\n",
            "Batch Training Loss =  0.6936019062995911\n",
            "Batch Training Loss =  0.6929470896720886\n",
            "Batch Training Loss =  0.6979186534881592\n",
            "Batch Training Loss =  0.6881599426269531\n",
            "Batch Training Loss =  0.699129045009613\n",
            "Batch Training Loss =  0.6929724812507629\n",
            "Batch Training Loss =  0.6920453906059265\n",
            "Batch Training Loss =  0.6941825747489929\n",
            "Batch Training Loss =  0.693249523639679\n",
            "Batch Training Loss =  0.6926320791244507\n",
            "Batch Training Loss =  0.6942106485366821\n",
            "Batch Training Loss =  0.6927857995033264\n",
            "Batch Training Loss =  0.6924265027046204\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6840794086456299\n",
            "Batch Training Loss =  0.6963886022567749\n",
            "Batch Training Loss =  0.7072780728340149\n",
            "Batch Training Loss =  0.6939977407455444\n",
            "Batch Training Loss =  0.695514440536499\n",
            "Batch Training Loss =  0.6931530833244324\n",
            "Batch Training Loss =  0.6932381987571716\n",
            "Batch Training Loss =  0.6914718747138977\n",
            "Batch Training Loss =  0.692467451095581\n",
            "Batch Training Loss =  0.7025143504142761\n",
            "Batch Training Loss =  0.692359447479248\n",
            "Batch Training Loss =  0.6914731860160828\n",
            "Batch Training Loss =  0.693076491355896\n",
            "Batch Training Loss =  0.6929827928543091\n",
            "Batch Training Loss =  0.6929534077644348\n",
            "Batch Training Loss =  0.6917304992675781\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6911884546279907\n",
            "Batch Training Loss =  0.6973907351493835\n",
            "Batch Training Loss =  0.6931889057159424\n",
            "Batch Training Loss =  0.6927227973937988\n",
            "Batch Training Loss =  0.6929385662078857\n",
            "Batch Training Loss =  0.6957572102546692\n",
            "Batch Training Loss =  0.6909000873565674\n",
            "Batch Training Loss =  0.7013394236564636\n",
            "Batch Training Loss =  0.6941822171211243\n",
            "Batch Training Loss =  0.6933988332748413\n",
            "Batch Training Loss =  0.6928907632827759\n",
            "Batch Training Loss =  0.6923560500144958\n",
            "Batch Training Loss =  0.6944774389266968\n",
            "Batch Training Loss =  0.69292151927948\n",
            "Batch Training Loss =  0.6918244957923889\n",
            "Batch Training Loss =  0.691667914390564\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.698372483253479\n",
            "Batch Training Loss =  0.692808985710144\n",
            "Batch Training Loss =  0.6948400735855103\n",
            "Batch Training Loss =  0.6927034854888916\n",
            "Batch Training Loss =  0.6937569975852966\n",
            "Batch Training Loss =  0.6915547847747803\n",
            "Batch Training Loss =  0.6991222500801086\n",
            "Batch Training Loss =  0.6932746767997742\n",
            "Batch Training Loss =  0.6932010650634766\n",
            "Batch Training Loss =  0.6935746073722839\n",
            "Batch Training Loss =  0.6942675113677979\n",
            "Batch Training Loss =  0.6922239065170288\n",
            "Batch Training Loss =  0.6902907490730286\n",
            "Batch Training Loss =  0.6980668902397156\n",
            "Batch Training Loss =  0.6925563812255859\n",
            "Batch Training Loss =  0.6938307881355286\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.6933771371841431\n",
            "Batch Training Loss =  0.6930207014083862\n",
            "Batch Training Loss =  0.6960741281509399\n",
            "Batch Training Loss =  0.6931451559066772\n",
            "Batch Training Loss =  0.6931974291801453\n",
            "Batch Training Loss =  0.6935596466064453\n",
            "Batch Training Loss =  0.6925591826438904\n",
            "Batch Training Loss =  0.6947089433670044\n",
            "Batch Training Loss =  0.6932035088539124\n",
            "Batch Training Loss =  0.6913838386535645\n",
            "Batch Training Loss =  0.6931037306785583\n",
            "Batch Training Loss =  0.6935824751853943\n",
            "Batch Training Loss =  0.6933310031890869\n",
            "Batch Training Loss =  0.6929754614830017\n",
            "Batch Training Loss =  0.6934950947761536\n",
            "Batch Training Loss =  0.6931533217430115\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.6929669380187988\n",
            "Batch Training Loss =  0.695097029209137\n",
            "Batch Training Loss =  0.6930447220802307\n",
            "Batch Training Loss =  0.6940791606903076\n",
            "Batch Training Loss =  0.6898754835128784\n",
            "Batch Training Loss =  0.6924991011619568\n",
            "Batch Training Loss =  0.6971119046211243\n",
            "Batch Training Loss =  0.6939041018486023\n",
            "Batch Training Loss =  0.693125307559967\n",
            "Batch Training Loss =  0.6933508515357971\n",
            "Batch Training Loss =  0.6932331919670105\n",
            "Batch Training Loss =  0.6928425431251526\n",
            "Batch Training Loss =  0.6923354268074036\n",
            "Batch Training Loss =  0.6959723830223083\n",
            "Batch Training Loss =  0.6923166513442993\n",
            "Batch Training Loss =  0.6954551935195923\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.6924704909324646\n",
            "Batch Training Loss =  0.6961683630943298\n",
            "Batch Training Loss =  0.6935888528823853\n",
            "Batch Training Loss =  0.6931536197662354\n",
            "Batch Training Loss =  0.6929622888565063\n",
            "Batch Training Loss =  0.6963923573493958\n",
            "Batch Training Loss =  0.6937880516052246\n",
            "Batch Training Loss =  0.6945696473121643\n",
            "Batch Training Loss =  0.6951127052307129\n",
            "Batch Training Loss =  0.691741943359375\n",
            "Batch Training Loss =  0.6870256066322327\n",
            "Batch Training Loss =  0.7039422392845154\n",
            "Batch Training Loss =  0.6947996616363525\n",
            "Batch Training Loss =  0.6950628757476807\n",
            "Batch Training Loss =  0.6941612362861633\n",
            "Batch Training Loss =  0.6930261254310608\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6912920475006104\n",
            "Batch Training Loss =  0.6962040066719055\n",
            "Batch Training Loss =  0.6971676349639893\n",
            "Batch Training Loss =  0.688630998134613\n",
            "Batch Training Loss =  0.6937597393989563\n",
            "Batch Training Loss =  0.6994339227676392\n",
            "Batch Training Loss =  0.6930736303329468\n",
            "Batch Training Loss =  0.6913752555847168\n",
            "Batch Training Loss =  0.6899304389953613\n",
            "Batch Training Loss =  0.6883144378662109\n",
            "Batch Training Loss =  0.6926167011260986\n",
            "Batch Training Loss =  0.691385805606842\n",
            "Batch Training Loss =  0.6972067952156067\n",
            "Batch Training Loss =  0.6929537057876587\n",
            "Batch Training Loss =  0.6910790801048279\n",
            "Batch Training Loss =  0.7050080299377441\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6930035948753357\n",
            "Batch Training Loss =  0.6944539546966553\n",
            "Batch Training Loss =  0.6936618685722351\n",
            "Batch Training Loss =  0.6923162937164307\n",
            "Batch Training Loss =  0.6929613351821899\n",
            "Batch Training Loss =  0.6981083750724792\n",
            "Batch Training Loss =  0.6915271878242493\n",
            "Batch Training Loss =  0.6998492479324341\n",
            "Batch Training Loss =  0.6920713782310486\n",
            "Batch Training Loss =  0.6955581903457642\n",
            "Batch Training Loss =  0.6939206123352051\n",
            "Batch Training Loss =  0.6931658387184143\n",
            "Batch Training Loss =  0.6933883428573608\n",
            "Batch Training Loss =  0.6931743025779724\n",
            "Batch Training Loss =  0.6931557655334473\n",
            "Batch Training Loss =  0.6927728056907654\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6850013732910156\n",
            "Batch Training Loss =  0.6983401775360107\n",
            "Batch Training Loss =  0.690409243106842\n",
            "Batch Training Loss =  0.6924213171005249\n",
            "Batch Training Loss =  0.6985099911689758\n",
            "Batch Training Loss =  0.6935121417045593\n",
            "Batch Training Loss =  0.6920439004898071\n",
            "Batch Training Loss =  0.6965629458427429\n",
            "Batch Training Loss =  0.6929364800453186\n",
            "Batch Training Loss =  0.6932406425476074\n",
            "Batch Training Loss =  0.6935424208641052\n",
            "Batch Training Loss =  0.6929873824119568\n",
            "Batch Training Loss =  0.693483293056488\n",
            "Batch Training Loss =  0.6926274299621582\n",
            "Batch Training Loss =  0.6957198977470398\n",
            "Batch Training Loss =  0.69677734375\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.6915738582611084\n",
            "Batch Training Loss =  0.6963626742362976\n",
            "Batch Training Loss =  0.6932087540626526\n",
            "Batch Training Loss =  0.6924511790275574\n",
            "Batch Training Loss =  0.6953694820404053\n",
            "Batch Training Loss =  0.6942750811576843\n",
            "Batch Training Loss =  0.6938364505767822\n",
            "Batch Training Loss =  0.7023610472679138\n",
            "Batch Training Loss =  0.6926485300064087\n",
            "Batch Training Loss =  0.6925462484359741\n",
            "Batch Training Loss =  0.693412184715271\n",
            "Batch Training Loss =  0.6911634206771851\n",
            "Batch Training Loss =  0.6968309879302979\n",
            "Batch Training Loss =  0.6915563941001892\n",
            "Batch Training Loss =  0.6923474669456482\n",
            "Batch Training Loss =  0.6944847702980042\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6943931579589844\n",
            "Batch Training Loss =  0.6928912401199341\n",
            "Batch Training Loss =  0.6938343048095703\n",
            "Batch Training Loss =  0.6945045590400696\n",
            "Batch Training Loss =  0.6970329880714417\n",
            "Batch Training Loss =  0.6929857134819031\n",
            "Batch Training Loss =  0.6937143206596375\n",
            "Batch Training Loss =  0.6931090354919434\n",
            "Batch Training Loss =  0.6929146647453308\n",
            "Batch Training Loss =  0.6918365955352783\n",
            "Batch Training Loss =  0.6957806348800659\n",
            "Batch Training Loss =  0.6924521923065186\n",
            "Batch Training Loss =  0.6912127733230591\n",
            "Batch Training Loss =  0.6923470497131348\n",
            "Batch Training Loss =  0.6902239322662354\n",
            "Batch Training Loss =  0.6991151571273804\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.6932923793792725\n",
            "Batch Training Loss =  0.69329434633255\n",
            "Batch Training Loss =  0.6860425472259521\n",
            "Batch Training Loss =  0.7005406022071838\n",
            "Batch Training Loss =  0.7005689740180969\n",
            "Batch Training Loss =  0.6946437954902649\n",
            "Batch Training Loss =  0.6925351023674011\n",
            "Batch Training Loss =  0.6914467811584473\n",
            "Batch Training Loss =  0.6967222094535828\n",
            "Batch Training Loss =  0.6934472918510437\n",
            "Batch Training Loss =  0.6918929815292358\n",
            "Batch Training Loss =  0.6867225766181946\n",
            "Batch Training Loss =  0.6959906816482544\n",
            "Batch Training Loss =  0.695332407951355\n",
            "Batch Training Loss =  0.6994293332099915\n",
            "Batch Training Loss =  0.6961008310317993\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.6943174600601196\n",
            "Batch Training Loss =  0.694388747215271\n",
            "Batch Training Loss =  0.6924980878829956\n",
            "Batch Training Loss =  0.6930394768714905\n",
            "Batch Training Loss =  0.694581151008606\n",
            "Batch Training Loss =  0.6928768754005432\n",
            "Batch Training Loss =  0.6947414875030518\n",
            "Batch Training Loss =  0.693571925163269\n",
            "Batch Training Loss =  0.6927611827850342\n",
            "Batch Training Loss =  0.69434654712677\n",
            "Batch Training Loss =  0.6929560899734497\n",
            "Batch Training Loss =  0.6943013668060303\n",
            "Batch Training Loss =  0.6925308108329773\n",
            "Batch Training Loss =  0.6929500102996826\n",
            "Batch Training Loss =  0.6913838386535645\n",
            "Batch Training Loss =  0.6968740820884705\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6933376789093018\n",
            "Batch Training Loss =  0.6924653053283691\n",
            "Batch Training Loss =  0.6941769123077393\n",
            "Batch Training Loss =  0.6958112120628357\n",
            "Batch Training Loss =  0.6930708885192871\n",
            "Batch Training Loss =  0.6924163103103638\n",
            "Batch Training Loss =  0.6895793676376343\n",
            "Batch Training Loss =  0.6924367547035217\n",
            "Batch Training Loss =  0.6977275609970093\n",
            "Batch Training Loss =  0.6928247809410095\n",
            "Batch Training Loss =  0.6932736039161682\n",
            "Batch Training Loss =  0.6925800442695618\n",
            "Batch Training Loss =  0.6981619000434875\n",
            "Batch Training Loss =  0.6915297508239746\n",
            "Batch Training Loss =  0.698168933391571\n",
            "Batch Training Loss =  0.693352222442627\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6957954168319702\n",
            "Batch Training Loss =  0.6930011510848999\n",
            "Batch Training Loss =  0.6934354305267334\n",
            "Batch Training Loss =  0.6930574178695679\n",
            "Batch Training Loss =  0.6936732530593872\n",
            "Batch Training Loss =  0.6930493712425232\n",
            "Batch Training Loss =  0.6922836899757385\n",
            "Batch Training Loss =  0.6913217902183533\n",
            "Batch Training Loss =  0.6923491954803467\n",
            "Batch Training Loss =  0.6958264708518982\n",
            "Batch Training Loss =  0.6928743124008179\n",
            "Batch Training Loss =  0.6923643350601196\n",
            "Batch Training Loss =  0.6899964809417725\n",
            "Batch Training Loss =  0.7026163339614868\n",
            "Batch Training Loss =  0.6928978562355042\n",
            "Batch Training Loss =  0.6949824690818787\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6945970058441162\n",
            "Batch Training Loss =  0.6933078765869141\n",
            "Batch Training Loss =  0.6963099837303162\n",
            "Batch Training Loss =  0.6920973062515259\n",
            "Batch Training Loss =  0.6976912021636963\n",
            "Batch Training Loss =  0.6900928020477295\n",
            "Batch Training Loss =  0.6816920638084412\n",
            "Batch Training Loss =  0.695696234703064\n",
            "Batch Training Loss =  0.6966715455055237\n",
            "Batch Training Loss =  0.6946368217468262\n",
            "Batch Training Loss =  0.696114182472229\n",
            "Batch Training Loss =  0.6916214227676392\n",
            "Batch Training Loss =  0.6951138377189636\n",
            "Batch Training Loss =  0.6932233572006226\n",
            "Batch Training Loss =  0.6944644451141357\n",
            "Batch Training Loss =  0.6979253888130188\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6930266618728638\n",
            "Batch Training Loss =  0.6928032636642456\n",
            "Batch Training Loss =  0.6949183940887451\n",
            "Batch Training Loss =  0.6932501792907715\n",
            "Batch Training Loss =  0.6926308274269104\n",
            "Batch Training Loss =  0.695054292678833\n",
            "Batch Training Loss =  0.6929149031639099\n",
            "Batch Training Loss =  0.6938104033470154\n",
            "Batch Training Loss =  0.693164050579071\n",
            "Batch Training Loss =  0.6938702464103699\n",
            "Batch Training Loss =  0.6975456476211548\n",
            "Batch Training Loss =  0.693041980266571\n",
            "Batch Training Loss =  0.6966715455055237\n",
            "Batch Training Loss =  0.6927502155303955\n",
            "Batch Training Loss =  0.68921959400177\n",
            "Batch Training Loss =  0.6985659599304199\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.6924731731414795\n",
            "Batch Training Loss =  0.6934769153594971\n",
            "Batch Training Loss =  0.6939542889595032\n",
            "Batch Training Loss =  0.693113386631012\n",
            "Batch Training Loss =  0.6925697326660156\n",
            "Batch Training Loss =  0.6938233375549316\n",
            "Batch Training Loss =  0.6928211450576782\n",
            "Batch Training Loss =  0.6942340731620789\n",
            "Batch Training Loss =  0.6935264468193054\n",
            "Batch Training Loss =  0.6949922442436218\n",
            "Batch Training Loss =  0.6926867961883545\n",
            "Batch Training Loss =  0.6940915584564209\n",
            "Batch Training Loss =  0.6932457685470581\n",
            "Batch Training Loss =  0.692676305770874\n",
            "Batch Training Loss =  0.6925374865531921\n",
            "Batch Training Loss =  0.6920198798179626\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6942050457000732\n",
            "Batch Training Loss =  0.6935457587242126\n",
            "Batch Training Loss =  0.692887544631958\n",
            "Batch Training Loss =  0.6954582929611206\n",
            "Batch Training Loss =  0.6896452903747559\n",
            "Batch Training Loss =  0.7024949789047241\n",
            "Batch Training Loss =  0.6936110854148865\n",
            "Batch Training Loss =  0.6926876306533813\n",
            "Batch Training Loss =  0.6908647418022156\n",
            "Batch Training Loss =  0.6976544260978699\n",
            "Batch Training Loss =  0.6931850910186768\n",
            "Batch Training Loss =  0.6925972104072571\n",
            "Batch Training Loss =  0.6895430684089661\n",
            "Batch Training Loss =  0.6945854425430298\n",
            "Batch Training Loss =  0.6958482265472412\n",
            "Batch Training Loss =  0.692716121673584\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.6925473809242249\n",
            "Batch Training Loss =  0.6980103254318237\n",
            "Batch Training Loss =  0.6916580200195312\n",
            "Batch Training Loss =  0.696128249168396\n",
            "Batch Training Loss =  0.6930016279220581\n",
            "Batch Training Loss =  0.6950051784515381\n",
            "Batch Training Loss =  0.6936820149421692\n",
            "Batch Training Loss =  0.6950742602348328\n",
            "Batch Training Loss =  0.6926719546318054\n",
            "Batch Training Loss =  0.6934354305267334\n",
            "Batch Training Loss =  0.6917076706886292\n",
            "Batch Training Loss =  0.6979824304580688\n",
            "Batch Training Loss =  0.6926007866859436\n",
            "Batch Training Loss =  0.6904933452606201\n",
            "Batch Training Loss =  0.6886831521987915\n",
            "Batch Training Loss =  0.6901477575302124\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.6819120645523071\n",
            "Batch Training Loss =  0.6900299191474915\n",
            "Batch Training Loss =  0.6883291602134705\n",
            "Batch Training Loss =  0.7008009552955627\n",
            "Batch Training Loss =  0.6937748193740845\n",
            "Batch Training Loss =  0.6906484365463257\n",
            "Batch Training Loss =  0.6986555457115173\n",
            "Batch Training Loss =  0.6931563019752502\n",
            "Batch Training Loss =  0.6932613253593445\n",
            "Batch Training Loss =  0.6935093402862549\n",
            "Batch Training Loss =  0.692971408367157\n",
            "Batch Training Loss =  0.6947959661483765\n",
            "Batch Training Loss =  0.6942400932312012\n",
            "Batch Training Loss =  0.692436158657074\n",
            "Batch Training Loss =  0.7018950581550598\n",
            "Batch Training Loss =  0.6937313675880432\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6930062174797058\n",
            "Batch Training Loss =  0.6942993402481079\n",
            "Batch Training Loss =  0.6936653256416321\n",
            "Batch Training Loss =  0.6926384568214417\n",
            "Batch Training Loss =  0.6958738565444946\n",
            "Batch Training Loss =  0.6920860409736633\n",
            "Batch Training Loss =  0.6973920464515686\n",
            "Batch Training Loss =  0.6928584575653076\n",
            "Batch Training Loss =  0.698811948299408\n",
            "Batch Training Loss =  0.6927642822265625\n",
            "Batch Training Loss =  0.6962018013000488\n",
            "Batch Training Loss =  0.6926618218421936\n",
            "Batch Training Loss =  0.6945226192474365\n",
            "Batch Training Loss =  0.6928857564926147\n",
            "Batch Training Loss =  0.6898691654205322\n",
            "Batch Training Loss =  0.6984527707099915\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.693229079246521\n",
            "Batch Training Loss =  0.6933481097221375\n",
            "Batch Training Loss =  0.6931067705154419\n",
            "Batch Training Loss =  0.6914451718330383\n",
            "Batch Training Loss =  0.6879828572273254\n",
            "Batch Training Loss =  0.6964470148086548\n",
            "Batch Training Loss =  0.6938100457191467\n",
            "Batch Training Loss =  0.6910601854324341\n",
            "Batch Training Loss =  0.6899237632751465\n",
            "Batch Training Loss =  0.6903596520423889\n",
            "Batch Training Loss =  0.6967015862464905\n",
            "Batch Training Loss =  0.6924645900726318\n",
            "Batch Training Loss =  0.6924445629119873\n",
            "Batch Training Loss =  0.6978180408477783\n",
            "Batch Training Loss =  0.6875514984130859\n",
            "Batch Training Loss =  0.6994985938072205\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6973642110824585\n",
            "Batch Training Loss =  0.6937469244003296\n",
            "Batch Training Loss =  0.6920377612113953\n",
            "Batch Training Loss =  0.690599262714386\n",
            "Batch Training Loss =  0.701253354549408\n",
            "Batch Training Loss =  0.691657543182373\n",
            "Batch Training Loss =  0.6974135041236877\n",
            "Batch Training Loss =  0.693026065826416\n",
            "Batch Training Loss =  0.6953590512275696\n",
            "Batch Training Loss =  0.6962724328041077\n",
            "Batch Training Loss =  0.6912201046943665\n",
            "Batch Training Loss =  0.6962518095970154\n",
            "Batch Training Loss =  0.6942394971847534\n",
            "Batch Training Loss =  0.6944087147712708\n",
            "Batch Training Loss =  0.6932775974273682\n",
            "Batch Training Loss =  0.6931789517402649\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.6930570006370544\n",
            "Batch Training Loss =  0.6935166716575623\n",
            "Batch Training Loss =  0.6925185918807983\n",
            "Batch Training Loss =  0.6893117427825928\n",
            "Batch Training Loss =  0.6979225873947144\n",
            "Batch Training Loss =  0.6933583617210388\n",
            "Batch Training Loss =  0.6929690837860107\n",
            "Batch Training Loss =  0.6943356394767761\n",
            "Batch Training Loss =  0.6920984387397766\n",
            "Batch Training Loss =  0.6975322961807251\n",
            "Batch Training Loss =  0.6933692097663879\n",
            "Batch Training Loss =  0.6932362914085388\n",
            "Batch Training Loss =  0.6953991651535034\n",
            "Batch Training Loss =  0.6932381987571716\n",
            "Batch Training Loss =  0.6934746503829956\n",
            "Batch Training Loss =  0.6943777203559875\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6932879686355591\n",
            "Batch Training Loss =  0.694078803062439\n",
            "Batch Training Loss =  0.694780170917511\n",
            "Batch Training Loss =  0.691652238368988\n",
            "Batch Training Loss =  0.6889602541923523\n",
            "Batch Training Loss =  0.689065158367157\n",
            "Batch Training Loss =  0.6876190900802612\n",
            "Batch Training Loss =  0.7040748596191406\n",
            "Batch Training Loss =  0.6926400661468506\n",
            "Batch Training Loss =  0.6961907148361206\n",
            "Batch Training Loss =  0.6938218474388123\n",
            "Batch Training Loss =  0.6935566067695618\n",
            "Batch Training Loss =  0.6931552886962891\n",
            "Batch Training Loss =  0.6933599710464478\n",
            "Batch Training Loss =  0.6936804056167603\n",
            "Batch Training Loss =  0.6930331587791443\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6939369440078735\n",
            "Batch Training Loss =  0.6927638053894043\n",
            "Batch Training Loss =  0.6915888786315918\n",
            "Batch Training Loss =  0.7025799751281738\n",
            "Batch Training Loss =  0.6964609622955322\n",
            "Batch Training Loss =  0.6946964859962463\n",
            "Batch Training Loss =  0.6923182010650635\n",
            "Batch Training Loss =  0.6969917416572571\n",
            "Batch Training Loss =  0.6919137835502625\n",
            "Batch Training Loss =  0.6834760308265686\n",
            "Batch Training Loss =  0.708249032497406\n",
            "Batch Training Loss =  0.6950575113296509\n",
            "Batch Training Loss =  0.689713716506958\n",
            "Batch Training Loss =  0.6900168061256409\n",
            "Batch Training Loss =  0.6954141855239868\n",
            "Batch Training Loss =  0.6951087713241577\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.6921659708023071\n",
            "Batch Training Loss =  0.692349374294281\n",
            "Batch Training Loss =  0.693763256072998\n",
            "Batch Training Loss =  0.6949182152748108\n",
            "Batch Training Loss =  0.6968235969543457\n",
            "Batch Training Loss =  0.692898154258728\n",
            "Batch Training Loss =  0.6999055743217468\n",
            "Batch Training Loss =  0.6933508515357971\n",
            "Batch Training Loss =  0.6901582479476929\n",
            "Batch Training Loss =  0.7028380036354065\n",
            "Batch Training Loss =  0.6868428587913513\n",
            "Batch Training Loss =  0.708721399307251\n",
            "Batch Training Loss =  0.6929345726966858\n",
            "Batch Training Loss =  0.6965168714523315\n",
            "Batch Training Loss =  0.6974573731422424\n",
            "Batch Training Loss =  0.6935149431228638\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.6931130290031433\n",
            "Batch Training Loss =  0.6954903602600098\n",
            "Batch Training Loss =  0.6941914558410645\n",
            "Batch Training Loss =  0.6930315494537354\n",
            "Batch Training Loss =  0.6936209797859192\n",
            "Batch Training Loss =  0.6924839615821838\n",
            "Batch Training Loss =  0.6916611194610596\n",
            "Batch Training Loss =  0.6941136717796326\n",
            "Batch Training Loss =  0.6943584680557251\n",
            "Batch Training Loss =  0.6897375583648682\n",
            "Batch Training Loss =  0.695335328578949\n",
            "Batch Training Loss =  0.7006059885025024\n",
            "Batch Training Loss =  0.6913812756538391\n",
            "Batch Training Loss =  0.7005141973495483\n",
            "Batch Training Loss =  0.6883766055107117\n",
            "Batch Training Loss =  0.703702449798584\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.6932732462882996\n",
            "Batch Training Loss =  0.6928661465644836\n",
            "Batch Training Loss =  0.6952632665634155\n",
            "Batch Training Loss =  0.691774845123291\n",
            "Batch Training Loss =  0.6940417289733887\n",
            "Batch Training Loss =  0.6926130652427673\n",
            "Batch Training Loss =  0.6892215609550476\n",
            "Batch Training Loss =  0.7008843421936035\n",
            "Batch Training Loss =  0.6929536461830139\n",
            "Batch Training Loss =  0.6942782402038574\n",
            "Batch Training Loss =  0.6929528713226318\n",
            "Batch Training Loss =  0.6906254291534424\n",
            "Batch Training Loss =  0.6915016174316406\n",
            "Batch Training Loss =  0.6937937140464783\n",
            "Batch Training Loss =  0.6951226592063904\n",
            "Batch Training Loss =  0.6931663751602173\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.6927539706230164\n",
            "Batch Training Loss =  0.6964232921600342\n",
            "Batch Training Loss =  0.6917256712913513\n",
            "Batch Training Loss =  0.6906315088272095\n",
            "Batch Training Loss =  0.6970716714859009\n",
            "Batch Training Loss =  0.6932097673416138\n",
            "Batch Training Loss =  0.6933627128601074\n",
            "Batch Training Loss =  0.6934266090393066\n",
            "Batch Training Loss =  0.6919057369232178\n",
            "Batch Training Loss =  0.6886135935783386\n",
            "Batch Training Loss =  0.6994011402130127\n",
            "Batch Training Loss =  0.693214476108551\n",
            "Batch Training Loss =  0.692786455154419\n",
            "Batch Training Loss =  0.6940227746963501\n",
            "Batch Training Loss =  0.6939577460289001\n",
            "Batch Training Loss =  0.6935015320777893\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6928774118423462\n",
            "Batch Training Loss =  0.6968421936035156\n",
            "Batch Training Loss =  0.6908610463142395\n",
            "Batch Training Loss =  0.702340841293335\n",
            "Batch Training Loss =  0.6923789978027344\n",
            "Batch Training Loss =  0.6884251236915588\n",
            "Batch Training Loss =  0.6951008439064026\n",
            "Batch Training Loss =  0.7003456354141235\n",
            "Batch Training Loss =  0.6955552697181702\n",
            "Batch Training Loss =  0.6945167779922485\n",
            "Batch Training Loss =  0.6980319023132324\n",
            "Batch Training Loss =  0.6939487457275391\n",
            "Batch Training Loss =  0.6914324760437012\n",
            "Batch Training Loss =  0.6972631812095642\n",
            "Batch Training Loss =  0.6927831172943115\n",
            "Batch Training Loss =  0.688118577003479\n",
            "Validation Loss in this epoch is 0.700\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.690717339515686\n",
            "Batch Training Loss =  0.6829229593276978\n",
            "Batch Training Loss =  0.7020958662033081\n",
            "Batch Training Loss =  0.6894411444664001\n",
            "Batch Training Loss =  0.7033820152282715\n",
            "Batch Training Loss =  0.690132737159729\n",
            "Batch Training Loss =  0.7023006677627563\n",
            "Batch Training Loss =  0.6934811472892761\n",
            "Batch Training Loss =  0.6968795657157898\n",
            "Batch Training Loss =  0.7031328678131104\n",
            "Batch Training Loss =  0.6913995146751404\n",
            "Batch Training Loss =  0.7041593790054321\n",
            "Batch Training Loss =  0.68681800365448\n",
            "Batch Training Loss =  0.6881677508354187\n",
            "Batch Training Loss =  0.6952010989189148\n",
            "Batch Training Loss =  0.6986585855484009\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6923792958259583\n",
            "Batch Training Loss =  0.6989330053329468\n",
            "Batch Training Loss =  0.6913691759109497\n",
            "Batch Training Loss =  0.6947593688964844\n",
            "Batch Training Loss =  0.6920409202575684\n",
            "Batch Training Loss =  0.6986037492752075\n",
            "Batch Training Loss =  0.690582275390625\n",
            "Batch Training Loss =  0.701744556427002\n",
            "Batch Training Loss =  0.6930892467498779\n",
            "Batch Training Loss =  0.6919683218002319\n",
            "Batch Training Loss =  0.6963374614715576\n",
            "Batch Training Loss =  0.6924039721488953\n",
            "Batch Training Loss =  0.6966300010681152\n",
            "Batch Training Loss =  0.6909037828445435\n",
            "Batch Training Loss =  0.7046125531196594\n",
            "Batch Training Loss =  0.6868876814842224\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.7084907293319702\n",
            "Batch Training Loss =  0.6938684582710266\n",
            "Batch Training Loss =  0.693097710609436\n",
            "Batch Training Loss =  0.6930005550384521\n",
            "Batch Training Loss =  0.6929495334625244\n",
            "Batch Training Loss =  0.6952106356620789\n",
            "Batch Training Loss =  0.6881043314933777\n",
            "Batch Training Loss =  0.6986663937568665\n",
            "Batch Training Loss =  0.6944536566734314\n",
            "Batch Training Loss =  0.693398118019104\n",
            "Batch Training Loss =  0.6937636733055115\n",
            "Batch Training Loss =  0.6895191073417664\n",
            "Batch Training Loss =  0.6969111561775208\n",
            "Batch Training Loss =  0.6929498314857483\n",
            "Batch Training Loss =  0.6936342716217041\n",
            "Batch Training Loss =  0.693215548992157\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6921373605728149\n",
            "Batch Training Loss =  0.6977012157440186\n",
            "Batch Training Loss =  0.6931620836257935\n",
            "Batch Training Loss =  0.6934106945991516\n",
            "Batch Training Loss =  0.698442280292511\n",
            "Batch Training Loss =  0.6921570301055908\n",
            "Batch Training Loss =  0.7064515948295593\n",
            "Batch Training Loss =  0.6945847272872925\n",
            "Batch Training Loss =  0.6939288377761841\n",
            "Batch Training Loss =  0.6887233257293701\n",
            "Batch Training Loss =  0.7017871141433716\n",
            "Batch Training Loss =  0.6925288438796997\n",
            "Batch Training Loss =  0.6981011033058167\n",
            "Batch Training Loss =  0.6915879249572754\n",
            "Batch Training Loss =  0.7002421021461487\n",
            "Batch Training Loss =  0.6875055432319641\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.6804558038711548\n",
            "Batch Training Loss =  0.703966498374939\n",
            "Batch Training Loss =  0.6973637938499451\n",
            "Batch Training Loss =  0.6952794790267944\n",
            "Batch Training Loss =  0.6887076497077942\n",
            "Batch Training Loss =  0.6955534219741821\n",
            "Batch Training Loss =  0.6895490884780884\n",
            "Batch Training Loss =  0.6996443867683411\n",
            "Batch Training Loss =  0.693246603012085\n",
            "Batch Training Loss =  0.6933285593986511\n",
            "Batch Training Loss =  0.6931762099266052\n",
            "Batch Training Loss =  0.6932125687599182\n",
            "Batch Training Loss =  0.6924833059310913\n",
            "Batch Training Loss =  0.6897423267364502\n",
            "Batch Training Loss =  0.6913730502128601\n",
            "Batch Training Loss =  0.6856490969657898\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.698425829410553\n",
            "Batch Training Loss =  0.691521406173706\n",
            "Batch Training Loss =  0.6979528069496155\n",
            "Batch Training Loss =  0.6938027143478394\n",
            "Batch Training Loss =  0.692945122718811\n",
            "Batch Training Loss =  0.698459267616272\n",
            "Batch Training Loss =  0.6926841139793396\n",
            "Batch Training Loss =  0.6963674426078796\n",
            "Batch Training Loss =  0.6892358660697937\n",
            "Batch Training Loss =  0.6936253309249878\n",
            "Batch Training Loss =  0.7071426510810852\n",
            "Batch Training Loss =  0.7057043313980103\n",
            "Batch Training Loss =  0.6935570240020752\n",
            "Batch Training Loss =  0.6934776902198792\n",
            "Batch Training Loss =  0.6969549655914307\n",
            "Batch Training Loss =  0.6939435601234436\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6931301355361938\n",
            "Batch Training Loss =  0.6877427101135254\n",
            "Batch Training Loss =  0.6916325092315674\n",
            "Batch Training Loss =  0.7142928242683411\n",
            "Batch Training Loss =  0.6958948373794556\n",
            "Batch Training Loss =  0.702613115310669\n",
            "Batch Training Loss =  0.6968910694122314\n",
            "Batch Training Loss =  0.6999567151069641\n",
            "Batch Training Loss =  0.6931482553482056\n",
            "Batch Training Loss =  0.6931359767913818\n",
            "Batch Training Loss =  0.693813145160675\n",
            "Batch Training Loss =  0.6929646134376526\n",
            "Batch Training Loss =  0.6937618255615234\n",
            "Batch Training Loss =  0.6933340430259705\n",
            "Batch Training Loss =  0.7042118906974792\n",
            "Batch Training Loss =  0.6979711651802063\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6956131458282471\n",
            "Batch Training Loss =  0.6964596509933472\n",
            "Batch Training Loss =  0.6913431286811829\n",
            "Batch Training Loss =  0.7020217180252075\n",
            "Batch Training Loss =  0.6928046345710754\n",
            "Batch Training Loss =  0.6943889856338501\n",
            "Batch Training Loss =  0.6925753951072693\n",
            "Batch Training Loss =  0.6937310099601746\n",
            "Batch Training Loss =  0.6937876343727112\n",
            "Batch Training Loss =  0.692962110042572\n",
            "Batch Training Loss =  0.6942207217216492\n",
            "Batch Training Loss =  0.6920068860054016\n",
            "Batch Training Loss =  0.69313645362854\n",
            "Batch Training Loss =  0.6939960718154907\n",
            "Batch Training Loss =  0.6931860446929932\n",
            "Batch Training Loss =  0.6937410235404968\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6923651099205017\n",
            "Batch Training Loss =  0.6977436542510986\n",
            "Batch Training Loss =  0.693299412727356\n",
            "Batch Training Loss =  0.6964209675788879\n",
            "Batch Training Loss =  0.6938896775245667\n",
            "Batch Training Loss =  0.6941916942596436\n",
            "Batch Training Loss =  0.6934310793876648\n",
            "Batch Training Loss =  0.6920486688613892\n",
            "Batch Training Loss =  0.6951819062232971\n",
            "Batch Training Loss =  0.689894437789917\n",
            "Batch Training Loss =  0.7027806639671326\n",
            "Batch Training Loss =  0.6922392249107361\n",
            "Batch Training Loss =  0.6989261507987976\n",
            "Batch Training Loss =  0.6929700374603271\n",
            "Batch Training Loss =  0.697763204574585\n",
            "Batch Training Loss =  0.6954794526100159\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6923688650131226\n",
            "Batch Training Loss =  0.6969879269599915\n",
            "Batch Training Loss =  0.6929572820663452\n",
            "Batch Training Loss =  0.6917585134506226\n",
            "Batch Training Loss =  0.6914195418357849\n",
            "Batch Training Loss =  0.6933727264404297\n",
            "Batch Training Loss =  0.6935878992080688\n",
            "Batch Training Loss =  0.6939308047294617\n",
            "Batch Training Loss =  0.6959904432296753\n",
            "Batch Training Loss =  0.6939430236816406\n",
            "Batch Training Loss =  0.6939135789871216\n",
            "Batch Training Loss =  0.6916065216064453\n",
            "Batch Training Loss =  0.6900514960289001\n",
            "Batch Training Loss =  0.6940140724182129\n",
            "Batch Training Loss =  0.6952760219573975\n",
            "Batch Training Loss =  0.6933766007423401\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.6943774223327637\n",
            "Batch Training Loss =  0.6952370405197144\n",
            "Batch Training Loss =  0.6911548376083374\n",
            "Batch Training Loss =  0.6976574063301086\n",
            "Batch Training Loss =  0.6928263902664185\n",
            "Batch Training Loss =  0.6924814581871033\n",
            "Batch Training Loss =  0.6894579529762268\n",
            "Batch Training Loss =  0.6883093118667603\n",
            "Batch Training Loss =  0.693259596824646\n",
            "Batch Training Loss =  0.6988197565078735\n",
            "Batch Training Loss =  0.6940693855285645\n",
            "Batch Training Loss =  0.6944781541824341\n",
            "Batch Training Loss =  0.6878603100776672\n",
            "Batch Training Loss =  0.6899741888046265\n",
            "Batch Training Loss =  0.6959810853004456\n",
            "Batch Training Loss =  0.6948110461235046\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6922286748886108\n",
            "Batch Training Loss =  0.696404218673706\n",
            "Batch Training Loss =  0.6932642459869385\n",
            "Batch Training Loss =  0.6930433511734009\n",
            "Batch Training Loss =  0.6934735774993896\n",
            "Batch Training Loss =  0.6929588913917542\n",
            "Batch Training Loss =  0.6929492354393005\n",
            "Batch Training Loss =  0.6888107061386108\n",
            "Batch Training Loss =  0.7012563347816467\n",
            "Batch Training Loss =  0.6975462436676025\n",
            "Batch Training Loss =  0.692436695098877\n",
            "Batch Training Loss =  0.6976070404052734\n",
            "Batch Training Loss =  0.6938493847846985\n",
            "Batch Training Loss =  0.6930670738220215\n",
            "Batch Training Loss =  0.6947713494300842\n",
            "Batch Training Loss =  0.6927958130836487\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.6936741471290588\n",
            "Batch Training Loss =  0.6927118897438049\n",
            "Batch Training Loss =  0.6924566030502319\n",
            "Batch Training Loss =  0.6954044103622437\n",
            "Batch Training Loss =  0.6943862438201904\n",
            "Batch Training Loss =  0.6961449980735779\n",
            "Batch Training Loss =  0.695457935333252\n",
            "Batch Training Loss =  0.6940307021141052\n",
            "Batch Training Loss =  0.6930727362632751\n",
            "Batch Training Loss =  0.6934435367584229\n",
            "Batch Training Loss =  0.6939738988876343\n",
            "Batch Training Loss =  0.6930767297744751\n",
            "Batch Training Loss =  0.6905121803283691\n",
            "Batch Training Loss =  0.698371171951294\n",
            "Batch Training Loss =  0.6933380961418152\n",
            "Batch Training Loss =  0.6926255226135254\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.6974776983261108\n",
            "Batch Training Loss =  0.693409264087677\n",
            "Batch Training Loss =  0.6937389373779297\n",
            "Batch Training Loss =  0.6898068785667419\n",
            "Batch Training Loss =  0.6979975700378418\n",
            "Batch Training Loss =  0.6938416957855225\n",
            "Batch Training Loss =  0.6932770013809204\n",
            "Batch Training Loss =  0.6913462281227112\n",
            "Batch Training Loss =  0.7021380066871643\n",
            "Batch Training Loss =  0.694319486618042\n",
            "Batch Training Loss =  0.6917840838432312\n",
            "Batch Training Loss =  0.6885563731193542\n",
            "Batch Training Loss =  0.7026035785675049\n",
            "Batch Training Loss =  0.6943390369415283\n",
            "Batch Training Loss =  0.7047065496444702\n",
            "Batch Training Loss =  0.6940830945968628\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6943365335464478\n",
            "Batch Training Loss =  0.6957201957702637\n",
            "Batch Training Loss =  0.6980549693107605\n",
            "Batch Training Loss =  0.6910198926925659\n",
            "Batch Training Loss =  0.6870893836021423\n",
            "Batch Training Loss =  0.7009764909744263\n",
            "Batch Training Loss =  0.6916172504425049\n",
            "Batch Training Loss =  0.6958862543106079\n",
            "Batch Training Loss =  0.6932277679443359\n",
            "Batch Training Loss =  0.6940746307373047\n",
            "Batch Training Loss =  0.7005301117897034\n",
            "Batch Training Loss =  0.6957122683525085\n",
            "Batch Training Loss =  0.6973511576652527\n",
            "Batch Training Loss =  0.6913776993751526\n",
            "Batch Training Loss =  0.707546055316925\n",
            "Batch Training Loss =  0.6795461177825928\n",
            "Validation Loss in this epoch is 0.702\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6941201090812683\n",
            "Batch Training Loss =  0.7000734210014343\n",
            "Batch Training Loss =  0.6937964558601379\n",
            "Batch Training Loss =  0.6944090127944946\n",
            "Batch Training Loss =  0.6923637390136719\n",
            "Batch Training Loss =  0.6923670172691345\n",
            "Batch Training Loss =  0.6916986107826233\n",
            "Batch Training Loss =  0.6914846897125244\n",
            "Batch Training Loss =  0.6942688226699829\n",
            "Batch Training Loss =  0.6933271288871765\n",
            "Batch Training Loss =  0.6940865516662598\n",
            "Batch Training Loss =  0.6936293840408325\n",
            "Batch Training Loss =  0.6930292248725891\n",
            "Batch Training Loss =  0.6938773989677429\n",
            "Batch Training Loss =  0.6932070851325989\n",
            "Batch Training Loss =  0.6923469305038452\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.696662425994873\n",
            "Batch Training Loss =  0.693988561630249\n",
            "Batch Training Loss =  0.6931628584861755\n",
            "Batch Training Loss =  0.6935940384864807\n",
            "Batch Training Loss =  0.6934579610824585\n",
            "Batch Training Loss =  0.6935779452323914\n",
            "Batch Training Loss =  0.6900742053985596\n",
            "Batch Training Loss =  0.6952695250511169\n",
            "Batch Training Loss =  0.6919212937355042\n",
            "Batch Training Loss =  0.6988190412521362\n",
            "Batch Training Loss =  0.696735143661499\n",
            "Batch Training Loss =  0.6929754614830017\n",
            "Batch Training Loss =  0.692959725856781\n",
            "Batch Training Loss =  0.6942347884178162\n",
            "Batch Training Loss =  0.6910655498504639\n",
            "Batch Training Loss =  0.6925350427627563\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.6993640661239624\n",
            "Batch Training Loss =  0.6940484642982483\n",
            "Batch Training Loss =  0.6926116943359375\n",
            "Batch Training Loss =  0.6929702162742615\n",
            "Batch Training Loss =  0.6941908001899719\n",
            "Batch Training Loss =  0.6918107867240906\n",
            "Batch Training Loss =  0.6900631189346313\n",
            "Batch Training Loss =  0.6966366767883301\n",
            "Batch Training Loss =  0.6922969818115234\n",
            "Batch Training Loss =  0.7018603682518005\n",
            "Batch Training Loss =  0.6881507635116577\n",
            "Batch Training Loss =  0.6952740550041199\n",
            "Batch Training Loss =  0.6959959268569946\n",
            "Batch Training Loss =  0.6929675936698914\n",
            "Batch Training Loss =  0.6924040913581848\n",
            "Batch Training Loss =  0.6915406584739685\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6960565447807312\n",
            "Batch Training Loss =  0.6931177377700806\n",
            "Batch Training Loss =  0.6949014067649841\n",
            "Batch Training Loss =  0.7081345915794373\n",
            "Batch Training Loss =  0.6983481049537659\n",
            "Batch Training Loss =  0.6925446391105652\n",
            "Batch Training Loss =  0.691296398639679\n",
            "Batch Training Loss =  0.7067525386810303\n",
            "Batch Training Loss =  0.7020675539970398\n",
            "Batch Training Loss =  0.6921490430831909\n",
            "Batch Training Loss =  0.6931087970733643\n",
            "Batch Training Loss =  0.6949768662452698\n",
            "Batch Training Loss =  0.6925487518310547\n",
            "Batch Training Loss =  0.6952161192893982\n",
            "Batch Training Loss =  0.6948289275169373\n",
            "Batch Training Loss =  0.6930245757102966\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.701474666595459\n",
            "Batch Training Loss =  0.694101870059967\n",
            "Batch Training Loss =  0.6935040354728699\n",
            "Batch Training Loss =  0.6980152130126953\n",
            "Batch Training Loss =  0.7053172588348389\n",
            "Batch Training Loss =  0.6986150145530701\n",
            "Batch Training Loss =  0.6934691071510315\n",
            "Batch Training Loss =  0.690856397151947\n",
            "Batch Training Loss =  0.6885603070259094\n",
            "Batch Training Loss =  0.6901675462722778\n",
            "Batch Training Loss =  0.6963647603988647\n",
            "Batch Training Loss =  0.6905159950256348\n",
            "Batch Training Loss =  0.7064911127090454\n",
            "Batch Training Loss =  0.6926980018615723\n",
            "Batch Training Loss =  0.6923673152923584\n",
            "Batch Training Loss =  0.6938872337341309\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.692046582698822\n",
            "Batch Training Loss =  0.6969906091690063\n",
            "Batch Training Loss =  0.6925094723701477\n",
            "Batch Training Loss =  0.6951460838317871\n",
            "Batch Training Loss =  0.6934133768081665\n",
            "Batch Training Loss =  0.691542387008667\n",
            "Batch Training Loss =  0.6913503408432007\n",
            "Batch Training Loss =  0.6966598033905029\n",
            "Batch Training Loss =  0.6930367350578308\n",
            "Batch Training Loss =  0.7055317759513855\n",
            "Batch Training Loss =  0.7061720490455627\n",
            "Batch Training Loss =  0.6915659308433533\n",
            "Batch Training Loss =  0.6979241967201233\n",
            "Batch Training Loss =  0.6930414438247681\n",
            "Batch Training Loss =  0.6932255625724792\n",
            "Batch Training Loss =  0.6932597160339355\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.6928593516349792\n",
            "Batch Training Loss =  0.6971286535263062\n",
            "Batch Training Loss =  0.7041342258453369\n",
            "Batch Training Loss =  0.6942816376686096\n",
            "Batch Training Loss =  0.6933108568191528\n",
            "Batch Training Loss =  0.6877785325050354\n",
            "Batch Training Loss =  0.7149814367294312\n",
            "Batch Training Loss =  0.6929605007171631\n",
            "Batch Training Loss =  0.6925460696220398\n",
            "Batch Training Loss =  0.6912978291511536\n",
            "Batch Training Loss =  0.6965067982673645\n",
            "Batch Training Loss =  0.6931370496749878\n",
            "Batch Training Loss =  0.6923830509185791\n",
            "Batch Training Loss =  0.695881724357605\n",
            "Batch Training Loss =  0.6928203105926514\n",
            "Batch Training Loss =  0.6885671019554138\n",
            "Validation Loss in this epoch is 0.701\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.6899451613426208\n",
            "Batch Training Loss =  0.697127103805542\n",
            "Batch Training Loss =  0.6907109618186951\n",
            "Batch Training Loss =  0.6899474263191223\n",
            "Batch Training Loss =  0.6999334692955017\n",
            "Batch Training Loss =  0.6929969191551208\n",
            "Batch Training Loss =  0.6944273114204407\n",
            "Batch Training Loss =  0.6905153393745422\n",
            "Batch Training Loss =  0.6930700540542603\n",
            "Batch Training Loss =  0.6924107074737549\n",
            "Batch Training Loss =  0.6980242729187012\n",
            "Batch Training Loss =  0.6910158395767212\n",
            "Batch Training Loss =  0.7009999752044678\n",
            "Batch Training Loss =  0.6992906332015991\n",
            "Batch Training Loss =  0.6913613080978394\n",
            "Batch Training Loss =  0.6913798451423645\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.6903740167617798\n",
            "Batch Training Loss =  0.6901397109031677\n",
            "Batch Training Loss =  0.699054479598999\n",
            "Batch Training Loss =  0.6929847598075867\n",
            "Batch Training Loss =  0.690777063369751\n",
            "Batch Training Loss =  0.6886355876922607\n",
            "Batch Training Loss =  0.7054529786109924\n",
            "Batch Training Loss =  0.687782883644104\n",
            "Batch Training Loss =  0.6725817322731018\n",
            "Batch Training Loss =  0.7363494634628296\n",
            "Batch Training Loss =  0.6920199394226074\n",
            "Batch Training Loss =  0.6907739043235779\n",
            "Batch Training Loss =  0.7058484554290771\n",
            "Batch Training Loss =  0.6930538415908813\n",
            "Batch Training Loss =  0.6985464692115784\n",
            "Batch Training Loss =  0.6929473876953125\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6936618089675903\n",
            "Batch Training Loss =  0.6929787993431091\n",
            "Batch Training Loss =  0.6948782205581665\n",
            "Batch Training Loss =  0.6934769153594971\n",
            "Batch Training Loss =  0.693631112575531\n",
            "Batch Training Loss =  0.6931523084640503\n",
            "Batch Training Loss =  0.6933532953262329\n",
            "Batch Training Loss =  0.6915337443351746\n",
            "Batch Training Loss =  0.702485978603363\n",
            "Batch Training Loss =  0.6902459859848022\n",
            "Batch Training Loss =  0.6977630853652954\n",
            "Batch Training Loss =  0.6922545433044434\n",
            "Batch Training Loss =  0.7064005136489868\n",
            "Batch Training Loss =  0.6935114860534668\n",
            "Batch Training Loss =  0.6925573945045471\n",
            "Batch Training Loss =  0.6903038620948792\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6951770186424255\n",
            "Batch Training Loss =  0.6909195184707642\n",
            "Batch Training Loss =  0.6925504803657532\n",
            "Batch Training Loss =  0.6932380199432373\n",
            "Batch Training Loss =  0.6946364045143127\n",
            "Batch Training Loss =  0.6935442090034485\n",
            "Batch Training Loss =  0.6959105134010315\n",
            "Batch Training Loss =  0.7017149329185486\n",
            "Batch Training Loss =  0.6921466588973999\n",
            "Batch Training Loss =  0.6957013010978699\n",
            "Batch Training Loss =  0.6943876147270203\n",
            "Batch Training Loss =  0.6932674646377563\n",
            "Batch Training Loss =  0.6952317953109741\n",
            "Batch Training Loss =  0.6910741329193115\n",
            "Batch Training Loss =  0.6913735270500183\n",
            "Batch Training Loss =  0.6981967091560364\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6931726932525635\n",
            "Batch Training Loss =  0.6930369734764099\n",
            "Batch Training Loss =  0.6929478645324707\n",
            "Batch Training Loss =  0.6914715766906738\n",
            "Batch Training Loss =  0.6867587566375732\n",
            "Batch Training Loss =  0.7049030065536499\n",
            "Batch Training Loss =  0.6930437684059143\n",
            "Batch Training Loss =  0.6921050548553467\n",
            "Batch Training Loss =  0.68927401304245\n",
            "Batch Training Loss =  0.7005085945129395\n",
            "Batch Training Loss =  0.6931517124176025\n",
            "Batch Training Loss =  0.6929982900619507\n",
            "Batch Training Loss =  0.6949203014373779\n",
            "Batch Training Loss =  0.6924504041671753\n",
            "Batch Training Loss =  0.6848258376121521\n",
            "Batch Training Loss =  0.708759605884552\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.6879006028175354\n",
            "Batch Training Loss =  0.680549681186676\n",
            "Batch Training Loss =  0.7132046222686768\n",
            "Batch Training Loss =  0.6935504674911499\n",
            "Batch Training Loss =  0.693073570728302\n",
            "Batch Training Loss =  0.6925165057182312\n",
            "Batch Training Loss =  0.688202440738678\n",
            "Batch Training Loss =  0.701301097869873\n",
            "Batch Training Loss =  0.6950719952583313\n",
            "Batch Training Loss =  0.6975021958351135\n",
            "Batch Training Loss =  0.6947763562202454\n",
            "Batch Training Loss =  0.6929475665092468\n",
            "Batch Training Loss =  0.6918476223945618\n",
            "Batch Training Loss =  0.6971079111099243\n",
            "Batch Training Loss =  0.693429172039032\n",
            "Batch Training Loss =  0.6933857798576355\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6940158009529114\n",
            "Batch Training Loss =  0.6937459707260132\n",
            "Batch Training Loss =  0.6947416663169861\n",
            "Batch Training Loss =  0.6941176652908325\n",
            "Batch Training Loss =  0.6904808282852173\n",
            "Batch Training Loss =  0.6986729502677917\n",
            "Batch Training Loss =  0.6964498162269592\n",
            "Batch Training Loss =  0.7074043154716492\n",
            "Batch Training Loss =  0.6859853863716125\n",
            "Batch Training Loss =  0.6902304887771606\n",
            "Batch Training Loss =  0.7046821117401123\n",
            "Batch Training Loss =  0.6912351846694946\n",
            "Batch Training Loss =  0.698055624961853\n",
            "Batch Training Loss =  0.6935781836509705\n",
            "Batch Training Loss =  0.6957018375396729\n",
            "Batch Training Loss =  0.6889166235923767\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.6973400115966797\n",
            "Batch Training Loss =  0.689409613609314\n",
            "Batch Training Loss =  0.6914528608322144\n",
            "Batch Training Loss =  0.7033378481864929\n",
            "Batch Training Loss =  0.6840882897377014\n",
            "Batch Training Loss =  0.7013691663742065\n",
            "Batch Training Loss =  0.6924154162406921\n",
            "Batch Training Loss =  0.6907188296318054\n",
            "Batch Training Loss =  0.7003603577613831\n",
            "Batch Training Loss =  0.6940730214118958\n",
            "Batch Training Loss =  0.6932264566421509\n",
            "Batch Training Loss =  0.6836335062980652\n",
            "Batch Training Loss =  0.7212486267089844\n",
            "Batch Training Loss =  0.6928955912590027\n",
            "Batch Training Loss =  0.694817066192627\n",
            "Batch Training Loss =  0.6977960467338562\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.6936286091804504\n",
            "Batch Training Loss =  0.7006386518478394\n",
            "Batch Training Loss =  0.7169287204742432\n",
            "Batch Training Loss =  0.7079777717590332\n",
            "Batch Training Loss =  0.6935765147209167\n",
            "Batch Training Loss =  0.6981795430183411\n",
            "Batch Training Loss =  0.694186806678772\n",
            "Batch Training Loss =  0.6936852931976318\n",
            "Batch Training Loss =  0.6963253021240234\n",
            "Batch Training Loss =  0.7144451141357422\n",
            "Batch Training Loss =  0.7039310336112976\n",
            "Batch Training Loss =  0.7024343609809875\n",
            "Batch Training Loss =  0.6911638379096985\n",
            "Batch Training Loss =  0.6901680827140808\n",
            "Batch Training Loss =  0.6926245093345642\n",
            "Batch Training Loss =  0.6932716369628906\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6941091418266296\n",
            "Batch Training Loss =  0.693070650100708\n",
            "Batch Training Loss =  0.6991055011749268\n",
            "Batch Training Loss =  0.6992811560630798\n",
            "Batch Training Loss =  0.6934337615966797\n",
            "Batch Training Loss =  0.6927206516265869\n",
            "Batch Training Loss =  0.686440110206604\n",
            "Batch Training Loss =  0.6945739984512329\n",
            "Batch Training Loss =  0.7064611315727234\n",
            "Batch Training Loss =  0.6952435970306396\n",
            "Batch Training Loss =  0.6893312335014343\n",
            "Batch Training Loss =  0.6899956464767456\n",
            "Batch Training Loss =  0.6975789070129395\n",
            "Batch Training Loss =  0.6950057744979858\n",
            "Batch Training Loss =  0.6879339814186096\n",
            "Batch Training Loss =  0.6840330362319946\n",
            "Validation Loss in this epoch is 0.705\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6926484704017639\n",
            "Batch Training Loss =  0.6839796304702759\n",
            "Batch Training Loss =  0.7123763561248779\n",
            "Batch Training Loss =  0.691692590713501\n",
            "Batch Training Loss =  0.6913530826568604\n",
            "Batch Training Loss =  0.6955786943435669\n",
            "Batch Training Loss =  0.6951965093612671\n",
            "Batch Training Loss =  0.6916699409484863\n",
            "Batch Training Loss =  0.7016489505767822\n",
            "Batch Training Loss =  0.691018283367157\n",
            "Batch Training Loss =  0.6973153948783875\n",
            "Batch Training Loss =  0.6933896541595459\n",
            "Batch Training Loss =  0.6924140453338623\n",
            "Batch Training Loss =  0.6917569041252136\n",
            "Batch Training Loss =  0.6949034333229065\n",
            "Batch Training Loss =  0.692890465259552\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6934073567390442\n",
            "Batch Training Loss =  0.6935539841651917\n",
            "Batch Training Loss =  0.6939346790313721\n",
            "Batch Training Loss =  0.6921610236167908\n",
            "Batch Training Loss =  0.7086997032165527\n",
            "Batch Training Loss =  0.6974985003471375\n",
            "Batch Training Loss =  0.7023007273674011\n",
            "Batch Training Loss =  0.6937307119369507\n",
            "Batch Training Loss =  0.6934950947761536\n",
            "Batch Training Loss =  0.6951029300689697\n",
            "Batch Training Loss =  0.7002527713775635\n",
            "Batch Training Loss =  0.6929629445075989\n",
            "Batch Training Loss =  0.6929562091827393\n",
            "Batch Training Loss =  0.6919756531715393\n",
            "Batch Training Loss =  0.6877157688140869\n",
            "Batch Training Loss =  0.6899792551994324\n",
            "Validation Loss in this epoch is 0.700\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6990222930908203\n",
            "Batch Training Loss =  0.6942299008369446\n",
            "Batch Training Loss =  0.6987872123718262\n",
            "Batch Training Loss =  0.6949127316474915\n",
            "Batch Training Loss =  0.6937615275382996\n",
            "Batch Training Loss =  0.6982814073562622\n",
            "Batch Training Loss =  0.6950700283050537\n",
            "Batch Training Loss =  0.6933414936065674\n",
            "Batch Training Loss =  0.6927639842033386\n",
            "Batch Training Loss =  0.6891031861305237\n",
            "Batch Training Loss =  0.699918806552887\n",
            "Batch Training Loss =  0.6911030411720276\n",
            "Batch Training Loss =  0.7036101818084717\n",
            "Batch Training Loss =  0.6936248540878296\n",
            "Batch Training Loss =  0.6998885273933411\n",
            "Batch Training Loss =  0.7043402194976807\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.6870760917663574\n",
            "Batch Training Loss =  0.6979035139083862\n",
            "Batch Training Loss =  0.6931313872337341\n",
            "Batch Training Loss =  0.6883391737937927\n",
            "Batch Training Loss =  0.6903932690620422\n",
            "Batch Training Loss =  0.6983857750892639\n",
            "Batch Training Loss =  0.6920012831687927\n",
            "Batch Training Loss =  0.6915566325187683\n",
            "Batch Training Loss =  0.6932802796363831\n",
            "Batch Training Loss =  0.6941157579421997\n",
            "Batch Training Loss =  0.6931480169296265\n",
            "Batch Training Loss =  0.6931904554367065\n",
            "Batch Training Loss =  0.6899599432945251\n",
            "Batch Training Loss =  0.6887260675430298\n",
            "Batch Training Loss =  0.697592556476593\n",
            "Batch Training Loss =  0.691709041595459\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6894853115081787\n",
            "Batch Training Loss =  0.6997072696685791\n",
            "Batch Training Loss =  0.6930609941482544\n",
            "Batch Training Loss =  0.6921120285987854\n",
            "Batch Training Loss =  0.6938825845718384\n",
            "Batch Training Loss =  0.6929577589035034\n",
            "Batch Training Loss =  0.6919861435890198\n",
            "Batch Training Loss =  0.6867927312850952\n",
            "Batch Training Loss =  0.7019441723823547\n",
            "Batch Training Loss =  0.6937311291694641\n",
            "Batch Training Loss =  0.693192720413208\n",
            "Batch Training Loss =  0.6937370300292969\n",
            "Batch Training Loss =  0.6961286664009094\n",
            "Batch Training Loss =  0.6931362748146057\n",
            "Batch Training Loss =  0.6955433487892151\n",
            "Batch Training Loss =  0.6924004554748535\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.6911049485206604\n",
            "Batch Training Loss =  0.6955931782722473\n",
            "Batch Training Loss =  0.6943218111991882\n",
            "Batch Training Loss =  0.7032939195632935\n",
            "Batch Training Loss =  0.6964860558509827\n",
            "Batch Training Loss =  0.69696444272995\n",
            "Batch Training Loss =  0.6901394724845886\n",
            "Batch Training Loss =  0.6848811507225037\n",
            "Batch Training Loss =  0.7008682489395142\n",
            "Batch Training Loss =  0.6982383728027344\n",
            "Batch Training Loss =  0.7050220966339111\n",
            "Batch Training Loss =  0.703464150428772\n",
            "Batch Training Loss =  0.6943555474281311\n",
            "Batch Training Loss =  0.6931524872779846\n",
            "Batch Training Loss =  0.6931738257408142\n",
            "Batch Training Loss =  0.6924335360527039\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.6949816942214966\n",
            "Batch Training Loss =  0.6927236318588257\n",
            "Batch Training Loss =  0.693692147731781\n",
            "Batch Training Loss =  0.6937629580497742\n",
            "Batch Training Loss =  0.6953157186508179\n",
            "Batch Training Loss =  0.7028945088386536\n",
            "Batch Training Loss =  0.6957941651344299\n",
            "Batch Training Loss =  0.6923032999038696\n",
            "Batch Training Loss =  0.6901628375053406\n",
            "Batch Training Loss =  0.7055709362030029\n",
            "Batch Training Loss =  0.696774959564209\n",
            "Batch Training Loss =  0.6933155655860901\n",
            "Batch Training Loss =  0.698310136795044\n",
            "Batch Training Loss =  0.6961867809295654\n",
            "Batch Training Loss =  0.6925855875015259\n",
            "Batch Training Loss =  0.6986302733421326\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6943026781082153\n",
            "Batch Training Loss =  0.6930380463600159\n",
            "Batch Training Loss =  0.6971743106842041\n",
            "Batch Training Loss =  0.6997302174568176\n",
            "Batch Training Loss =  0.6929737329483032\n",
            "Batch Training Loss =  0.6929591298103333\n",
            "Batch Training Loss =  0.6932757496833801\n",
            "Batch Training Loss =  0.6927830576896667\n",
            "Batch Training Loss =  0.6975852847099304\n",
            "Batch Training Loss =  0.6961285471916199\n",
            "Batch Training Loss =  0.6930042505264282\n",
            "Batch Training Loss =  0.7022320032119751\n",
            "Batch Training Loss =  0.6959334015846252\n",
            "Batch Training Loss =  0.6929563879966736\n",
            "Batch Training Loss =  0.6926286220550537\n",
            "Batch Training Loss =  0.6945590376853943\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6930035352706909\n",
            "Batch Training Loss =  0.6935169100761414\n",
            "Batch Training Loss =  0.6932471394538879\n",
            "Batch Training Loss =  0.6921969056129456\n",
            "Batch Training Loss =  0.6989449262619019\n",
            "Batch Training Loss =  0.692786455154419\n",
            "Batch Training Loss =  0.6848142147064209\n",
            "Batch Training Loss =  0.7011592388153076\n",
            "Batch Training Loss =  0.6903660297393799\n",
            "Batch Training Loss =  0.7034338116645813\n",
            "Batch Training Loss =  0.6896160840988159\n",
            "Batch Training Loss =  0.7064130306243896\n",
            "Batch Training Loss =  0.691373884677887\n",
            "Batch Training Loss =  0.6975149512290955\n",
            "Batch Training Loss =  0.6931926608085632\n",
            "Batch Training Loss =  0.6931542754173279\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6935458183288574\n",
            "Batch Training Loss =  0.7040823101997375\n",
            "Batch Training Loss =  0.6957180500030518\n",
            "Batch Training Loss =  0.692963719367981\n",
            "Batch Training Loss =  0.6986364722251892\n",
            "Batch Training Loss =  0.6927233934402466\n",
            "Batch Training Loss =  0.7036631107330322\n",
            "Batch Training Loss =  0.7013070583343506\n",
            "Batch Training Loss =  0.693661093711853\n",
            "Batch Training Loss =  0.701996922492981\n",
            "Batch Training Loss =  0.6933422684669495\n",
            "Batch Training Loss =  0.6938105225563049\n",
            "Batch Training Loss =  0.6938257813453674\n",
            "Batch Training Loss =  0.693845808506012\n",
            "Batch Training Loss =  0.6946568489074707\n",
            "Batch Training Loss =  0.6941307187080383\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.6943703293800354\n",
            "Batch Training Loss =  0.6938672065734863\n",
            "Batch Training Loss =  0.6931978464126587\n",
            "Batch Training Loss =  0.6947737336158752\n",
            "Batch Training Loss =  0.693668007850647\n",
            "Batch Training Loss =  0.6918999552726746\n",
            "Batch Training Loss =  0.6947891712188721\n",
            "Batch Training Loss =  0.6930410861968994\n",
            "Batch Training Loss =  0.691723644733429\n",
            "Batch Training Loss =  0.6952587366104126\n",
            "Batch Training Loss =  0.6919234395027161\n",
            "Batch Training Loss =  0.7012399435043335\n",
            "Batch Training Loss =  0.7032516002655029\n",
            "Batch Training Loss =  0.6953959465026855\n",
            "Batch Training Loss =  0.6906751394271851\n",
            "Batch Training Loss =  0.701566219329834\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6841512322425842\n",
            "Batch Training Loss =  0.7094077467918396\n",
            "Batch Training Loss =  0.6931174993515015\n",
            "Batch Training Loss =  0.6924536228179932\n",
            "Batch Training Loss =  0.6923955678939819\n",
            "Batch Training Loss =  0.6923786997795105\n",
            "Batch Training Loss =  0.6917161345481873\n",
            "Batch Training Loss =  0.6889007091522217\n",
            "Batch Training Loss =  0.7023109197616577\n",
            "Batch Training Loss =  0.6910240650177002\n",
            "Batch Training Loss =  0.6908784508705139\n",
            "Batch Training Loss =  0.6917846202850342\n",
            "Batch Training Loss =  0.6979660987854004\n",
            "Batch Training Loss =  0.6927805542945862\n",
            "Batch Training Loss =  0.6943295001983643\n",
            "Batch Training Loss =  0.6935771703720093\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.697857141494751\n",
            "Batch Training Loss =  0.7040137648582458\n",
            "Batch Training Loss =  0.6927306652069092\n",
            "Batch Training Loss =  0.6953269839286804\n",
            "Batch Training Loss =  0.6925877928733826\n",
            "Batch Training Loss =  0.7003408074378967\n",
            "Batch Training Loss =  0.6960059404373169\n",
            "Batch Training Loss =  0.6927361488342285\n",
            "Batch Training Loss =  0.6955321431159973\n",
            "Batch Training Loss =  0.6960873603820801\n",
            "Batch Training Loss =  0.6953229308128357\n",
            "Batch Training Loss =  0.6955944299697876\n",
            "Batch Training Loss =  0.6967217326164246\n",
            "Batch Training Loss =  0.6931064128875732\n",
            "Batch Training Loss =  0.6908810138702393\n",
            "Batch Training Loss =  0.6982476115226746\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.6936081051826477\n",
            "Batch Training Loss =  0.6907963752746582\n",
            "Batch Training Loss =  0.6998828053474426\n",
            "Batch Training Loss =  0.6910572648048401\n",
            "Batch Training Loss =  0.7113658785820007\n",
            "Batch Training Loss =  0.7011308073997498\n",
            "Batch Training Loss =  0.6929771900177002\n",
            "Batch Training Loss =  0.6899778246879578\n",
            "Batch Training Loss =  0.6952247619628906\n",
            "Batch Training Loss =  0.7006996273994446\n",
            "Batch Training Loss =  0.6932359933853149\n",
            "Batch Training Loss =  0.6935873627662659\n",
            "Batch Training Loss =  0.6961380243301392\n",
            "Batch Training Loss =  0.6914753913879395\n",
            "Batch Training Loss =  0.7032345533370972\n",
            "Batch Training Loss =  0.6950550079345703\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.6930373907089233\n",
            "Batch Training Loss =  0.6914592981338501\n",
            "Batch Training Loss =  0.6899567246437073\n",
            "Batch Training Loss =  0.6913959980010986\n",
            "Batch Training Loss =  0.6867006421089172\n",
            "Batch Training Loss =  0.7049524188041687\n",
            "Batch Training Loss =  0.6929966807365417\n",
            "Batch Training Loss =  0.6961103081703186\n",
            "Batch Training Loss =  0.6935277581214905\n",
            "Batch Training Loss =  0.6921043395996094\n",
            "Batch Training Loss =  0.7011569142341614\n",
            "Batch Training Loss =  0.6915844678878784\n",
            "Batch Training Loss =  0.691455066204071\n",
            "Batch Training Loss =  0.686603844165802\n",
            "Batch Training Loss =  0.6960812211036682\n",
            "Batch Training Loss =  0.6854134202003479\n",
            "Validation Loss in this epoch is 0.702\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.6793872117996216\n",
            "Batch Training Loss =  0.7048102021217346\n",
            "Batch Training Loss =  0.6932370066642761\n",
            "Batch Training Loss =  0.6957411170005798\n",
            "Batch Training Loss =  0.6941446661949158\n",
            "Batch Training Loss =  0.6931799054145813\n",
            "Batch Training Loss =  0.6933785080909729\n",
            "Batch Training Loss =  0.6931667923927307\n",
            "Batch Training Loss =  0.6932503581047058\n",
            "Batch Training Loss =  0.6914277076721191\n",
            "Batch Training Loss =  0.6901971697807312\n",
            "Batch Training Loss =  0.7027034163475037\n",
            "Batch Training Loss =  0.6944445967674255\n",
            "Batch Training Loss =  0.6930767893791199\n",
            "Batch Training Loss =  0.6918267607688904\n",
            "Batch Training Loss =  0.696526288986206\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.6929555535316467\n",
            "Batch Training Loss =  0.6916452050209045\n",
            "Batch Training Loss =  0.7003994584083557\n",
            "Batch Training Loss =  0.6924271583557129\n",
            "Batch Training Loss =  0.695482611656189\n",
            "Batch Training Loss =  0.6943594217300415\n",
            "Batch Training Loss =  0.6929513812065125\n",
            "Batch Training Loss =  0.6937149167060852\n",
            "Batch Training Loss =  0.6930568814277649\n",
            "Batch Training Loss =  0.6942034363746643\n",
            "Batch Training Loss =  0.6934471726417542\n",
            "Batch Training Loss =  0.6925196051597595\n",
            "Batch Training Loss =  0.698805034160614\n",
            "Batch Training Loss =  0.6933645009994507\n",
            "Batch Training Loss =  0.6947451233863831\n",
            "Batch Training Loss =  0.6890913248062134\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6899481415748596\n",
            "Batch Training Loss =  0.7046366930007935\n",
            "Batch Training Loss =  0.6923913359642029\n",
            "Batch Training Loss =  0.6968913078308105\n",
            "Batch Training Loss =  0.692351222038269\n",
            "Batch Training Loss =  0.6895725131034851\n",
            "Batch Training Loss =  0.7017945051193237\n",
            "Batch Training Loss =  0.6936150193214417\n",
            "Batch Training Loss =  0.6935902237892151\n",
            "Batch Training Loss =  0.6931539177894592\n",
            "Batch Training Loss =  0.693206787109375\n",
            "Batch Training Loss =  0.6979953050613403\n",
            "Batch Training Loss =  0.7061660289764404\n",
            "Batch Training Loss =  0.6941694021224976\n",
            "Batch Training Loss =  0.697265625\n",
            "Batch Training Loss =  0.7125251293182373\n",
            "Validation Loss in this epoch is 0.700\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6973523497581482\n",
            "Batch Training Loss =  0.6952993869781494\n",
            "Batch Training Loss =  0.6932661533355713\n",
            "Batch Training Loss =  0.6915988326072693\n",
            "Batch Training Loss =  0.7108378410339355\n",
            "Batch Training Loss =  0.699998676776886\n",
            "Batch Training Loss =  0.69761061668396\n",
            "Batch Training Loss =  0.7101119160652161\n",
            "Batch Training Loss =  0.6949354410171509\n",
            "Batch Training Loss =  0.6936603784561157\n",
            "Batch Training Loss =  0.6939986944198608\n",
            "Batch Training Loss =  0.6964409351348877\n",
            "Batch Training Loss =  0.6984032392501831\n",
            "Batch Training Loss =  0.6897536516189575\n",
            "Batch Training Loss =  0.6901013851165771\n",
            "Batch Training Loss =  0.6931182742118835\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.688270628452301\n",
            "Batch Training Loss =  0.6980190873146057\n",
            "Batch Training Loss =  0.6947410106658936\n",
            "Batch Training Loss =  0.695051908493042\n",
            "Batch Training Loss =  0.6974354386329651\n",
            "Batch Training Loss =  0.6885850429534912\n",
            "Batch Training Loss =  0.6946823000907898\n",
            "Batch Training Loss =  0.697348415851593\n",
            "Batch Training Loss =  0.696683943271637\n",
            "Batch Training Loss =  0.6889196038246155\n",
            "Batch Training Loss =  0.6929331421852112\n",
            "Batch Training Loss =  0.7053399682044983\n",
            "Batch Training Loss =  0.7188127040863037\n",
            "Batch Training Loss =  0.6983358860015869\n",
            "Batch Training Loss =  0.6957906484603882\n",
            "Batch Training Loss =  0.6891037821769714\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.6930006146430969\n",
            "Batch Training Loss =  0.6873460412025452\n",
            "Batch Training Loss =  0.6845794916152954\n",
            "Batch Training Loss =  0.6945292949676514\n",
            "Batch Training Loss =  0.7013794183731079\n",
            "Batch Training Loss =  0.6944521069526672\n",
            "Batch Training Loss =  0.6946039795875549\n",
            "Batch Training Loss =  0.6887242197990417\n",
            "Batch Training Loss =  0.6930083632469177\n",
            "Batch Training Loss =  0.7005075216293335\n",
            "Batch Training Loss =  0.6979075074195862\n",
            "Batch Training Loss =  0.6882721781730652\n",
            "Batch Training Loss =  0.6919068098068237\n",
            "Batch Training Loss =  0.6900224089622498\n",
            "Batch Training Loss =  0.6967380046844482\n",
            "Batch Training Loss =  0.6953161358833313\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6934992074966431\n",
            "Batch Training Loss =  0.6942641735076904\n",
            "Batch Training Loss =  0.6934037804603577\n",
            "Batch Training Loss =  0.7002249360084534\n",
            "Batch Training Loss =  0.6947702169418335\n",
            "Batch Training Loss =  0.6926729679107666\n",
            "Batch Training Loss =  0.6965965032577515\n",
            "Batch Training Loss =  0.690111517906189\n",
            "Batch Training Loss =  0.7159884572029114\n",
            "Batch Training Loss =  0.6920416951179504\n",
            "Batch Training Loss =  0.6951835751533508\n",
            "Batch Training Loss =  0.6909165382385254\n",
            "Batch Training Loss =  0.70218825340271\n",
            "Batch Training Loss =  0.6923555731773376\n",
            "Batch Training Loss =  0.6889275312423706\n",
            "Batch Training Loss =  0.7016750574111938\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.6928657293319702\n",
            "Batch Training Loss =  0.6848888993263245\n",
            "Batch Training Loss =  0.7036699056625366\n",
            "Batch Training Loss =  0.693986177444458\n",
            "Batch Training Loss =  0.6932817697525024\n",
            "Batch Training Loss =  0.6934311389923096\n",
            "Batch Training Loss =  0.6922556161880493\n",
            "Batch Training Loss =  0.6953076124191284\n",
            "Batch Training Loss =  0.6933693885803223\n",
            "Batch Training Loss =  0.696532666683197\n",
            "Batch Training Loss =  0.7081704139709473\n",
            "Batch Training Loss =  0.6908813714981079\n",
            "Batch Training Loss =  0.6979264616966248\n",
            "Batch Training Loss =  0.6936649084091187\n",
            "Batch Training Loss =  0.6945757865905762\n",
            "Batch Training Loss =  0.6937477588653564\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.6966467499732971\n",
            "Batch Training Loss =  0.6983296871185303\n",
            "Batch Training Loss =  0.6925435662269592\n",
            "Batch Training Loss =  0.6952258348464966\n",
            "Batch Training Loss =  0.6925726532936096\n",
            "Batch Training Loss =  0.6907748579978943\n",
            "Batch Training Loss =  0.7023276686668396\n",
            "Batch Training Loss =  0.6959128379821777\n",
            "Batch Training Loss =  0.6942176818847656\n",
            "Batch Training Loss =  0.6948493123054504\n",
            "Batch Training Loss =  0.6945888996124268\n",
            "Batch Training Loss =  0.6951277852058411\n",
            "Batch Training Loss =  0.7023952603340149\n",
            "Batch Training Loss =  0.7165489196777344\n",
            "Batch Training Loss =  0.6935312747955322\n",
            "Batch Training Loss =  0.6959294080734253\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6958374977111816\n",
            "Batch Training Loss =  0.6959335207939148\n",
            "Batch Training Loss =  0.6946746706962585\n",
            "Batch Training Loss =  0.6926627159118652\n",
            "Batch Training Loss =  0.6937075853347778\n",
            "Batch Training Loss =  0.693504810333252\n",
            "Batch Training Loss =  0.6930651664733887\n",
            "Batch Training Loss =  0.6944028735160828\n",
            "Batch Training Loss =  0.692360520362854\n",
            "Batch Training Loss =  0.6900618076324463\n",
            "Batch Training Loss =  0.6965801119804382\n",
            "Batch Training Loss =  0.6952154636383057\n",
            "Batch Training Loss =  0.6945797801017761\n",
            "Batch Training Loss =  0.6959500312805176\n",
            "Batch Training Loss =  0.696525514125824\n",
            "Batch Training Loss =  0.6931707859039307\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6953189373016357\n",
            "Batch Training Loss =  0.7007676959037781\n",
            "Batch Training Loss =  0.6888248920440674\n",
            "Batch Training Loss =  0.6809194087982178\n",
            "Batch Training Loss =  0.7094095349311829\n",
            "Batch Training Loss =  0.6933988928794861\n",
            "Batch Training Loss =  0.6937263011932373\n",
            "Batch Training Loss =  0.6903266310691833\n",
            "Batch Training Loss =  0.7012802362442017\n",
            "Batch Training Loss =  0.6936843395233154\n",
            "Batch Training Loss =  0.6926520466804504\n",
            "Batch Training Loss =  0.6898307800292969\n",
            "Batch Training Loss =  0.6915528774261475\n",
            "Batch Training Loss =  0.6963300108909607\n",
            "Batch Training Loss =  0.6923620700836182\n",
            "Batch Training Loss =  0.6937942504882812\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6918109059333801\n",
            "Batch Training Loss =  0.6991618275642395\n",
            "Batch Training Loss =  0.6929500699043274\n",
            "Batch Training Loss =  0.6946582198143005\n",
            "Batch Training Loss =  0.6903801560401917\n",
            "Batch Training Loss =  0.698944091796875\n",
            "Batch Training Loss =  0.6934368014335632\n",
            "Batch Training Loss =  0.6879149079322815\n",
            "Batch Training Loss =  0.7238009572029114\n",
            "Batch Training Loss =  0.6978863477706909\n",
            "Batch Training Loss =  0.6929200887680054\n",
            "Batch Training Loss =  0.6942979693412781\n",
            "Batch Training Loss =  0.6945294141769409\n",
            "Batch Training Loss =  0.6880204677581787\n",
            "Batch Training Loss =  0.6998178958892822\n",
            "Batch Training Loss =  0.6947915554046631\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6891679167747498\n",
            "Batch Training Loss =  0.6928551197052002\n",
            "Batch Training Loss =  0.7011770606040955\n",
            "Batch Training Loss =  0.6984544992446899\n",
            "Batch Training Loss =  0.6933432817459106\n",
            "Batch Training Loss =  0.6928615570068359\n",
            "Batch Training Loss =  0.6929523348808289\n",
            "Batch Training Loss =  0.6914058923721313\n",
            "Batch Training Loss =  0.6948114633560181\n",
            "Batch Training Loss =  0.6938766241073608\n",
            "Batch Training Loss =  0.6931623220443726\n",
            "Batch Training Loss =  0.6924183368682861\n",
            "Batch Training Loss =  0.6941095590591431\n",
            "Batch Training Loss =  0.6929501295089722\n",
            "Batch Training Loss =  0.6956760883331299\n",
            "Batch Training Loss =  0.6900872588157654\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.6939724683761597\n",
            "Batch Training Loss =  0.6894605755805969\n",
            "Batch Training Loss =  0.700386643409729\n",
            "Batch Training Loss =  0.6931414604187012\n",
            "Batch Training Loss =  0.6957550644874573\n",
            "Batch Training Loss =  0.6924182772636414\n",
            "Batch Training Loss =  0.6980612874031067\n",
            "Batch Training Loss =  0.6863077282905579\n",
            "Batch Training Loss =  0.7130976319313049\n",
            "Batch Training Loss =  0.6932012438774109\n",
            "Batch Training Loss =  0.7025737762451172\n",
            "Batch Training Loss =  0.6938944458961487\n",
            "Batch Training Loss =  0.6948035955429077\n",
            "Batch Training Loss =  0.6911069750785828\n",
            "Batch Training Loss =  0.7020071148872375\n",
            "Batch Training Loss =  0.6909030675888062\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6968133449554443\n",
            "Batch Training Loss =  0.6929287910461426\n",
            "Batch Training Loss =  0.6937758922576904\n",
            "Batch Training Loss =  0.6916765570640564\n",
            "Batch Training Loss =  0.697602391242981\n",
            "Batch Training Loss =  0.6947495341300964\n",
            "Batch Training Loss =  0.6946278214454651\n",
            "Batch Training Loss =  0.6929484605789185\n",
            "Batch Training Loss =  0.694811999797821\n",
            "Batch Training Loss =  0.6930257678031921\n",
            "Batch Training Loss =  0.6915702819824219\n",
            "Batch Training Loss =  0.6894364953041077\n",
            "Batch Training Loss =  0.6913925409317017\n",
            "Batch Training Loss =  0.7052533626556396\n",
            "Batch Training Loss =  0.6913686394691467\n",
            "Batch Training Loss =  0.6924871206283569\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.6923506855964661\n",
            "Batch Training Loss =  0.6968041062355042\n",
            "Batch Training Loss =  0.6909484267234802\n",
            "Batch Training Loss =  0.6961511969566345\n",
            "Batch Training Loss =  0.6929648518562317\n",
            "Batch Training Loss =  0.6926440596580505\n",
            "Batch Training Loss =  0.6919167041778564\n",
            "Batch Training Loss =  0.6955873370170593\n",
            "Batch Training Loss =  0.6922454237937927\n",
            "Batch Training Loss =  0.6859303116798401\n",
            "Batch Training Loss =  0.723024308681488\n",
            "Batch Training Loss =  0.7046098113059998\n",
            "Batch Training Loss =  0.6975681781768799\n",
            "Batch Training Loss =  0.6943067312240601\n",
            "Batch Training Loss =  0.6951795816421509\n",
            "Batch Training Loss =  0.6985201835632324\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.6983139514923096\n",
            "Batch Training Loss =  0.7062404751777649\n",
            "Batch Training Loss =  0.6937931776046753\n",
            "Batch Training Loss =  0.6941214799880981\n",
            "Batch Training Loss =  0.6929560899734497\n",
            "Batch Training Loss =  0.6929475665092468\n",
            "Batch Training Loss =  0.6954517960548401\n",
            "Batch Training Loss =  0.695600152015686\n",
            "Batch Training Loss =  0.6932094693183899\n",
            "Batch Training Loss =  0.6930678486824036\n",
            "Batch Training Loss =  0.6936880350112915\n",
            "Batch Training Loss =  0.6914297342300415\n",
            "Batch Training Loss =  0.6860750317573547\n",
            "Batch Training Loss =  0.7035850286483765\n",
            "Batch Training Loss =  0.692135751247406\n",
            "Batch Training Loss =  0.7002418637275696\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6932531595230103\n",
            "Batch Training Loss =  0.6932806372642517\n",
            "Batch Training Loss =  0.6921026706695557\n",
            "Batch Training Loss =  0.7016596794128418\n",
            "Batch Training Loss =  0.6921336650848389\n",
            "Batch Training Loss =  0.691362738609314\n",
            "Batch Training Loss =  0.6986243724822998\n",
            "Batch Training Loss =  0.69256192445755\n",
            "Batch Training Loss =  0.6913139820098877\n",
            "Batch Training Loss =  0.6944516897201538\n",
            "Batch Training Loss =  0.6941748261451721\n",
            "Batch Training Loss =  0.6920353174209595\n",
            "Batch Training Loss =  0.6913602352142334\n",
            "Batch Training Loss =  0.7007244825363159\n",
            "Batch Training Loss =  0.6915630102157593\n",
            "Batch Training Loss =  0.6932770013809204\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6918498873710632\n",
            "Batch Training Loss =  0.6989590525627136\n",
            "Batch Training Loss =  0.6940912008285522\n",
            "Batch Training Loss =  0.6943423748016357\n",
            "Batch Training Loss =  0.6952558159828186\n",
            "Batch Training Loss =  0.6997773051261902\n",
            "Batch Training Loss =  0.6847109794616699\n",
            "Batch Training Loss =  0.7028763294219971\n",
            "Batch Training Loss =  0.6951763033866882\n",
            "Batch Training Loss =  0.6948741674423218\n",
            "Batch Training Loss =  0.6933268904685974\n",
            "Batch Training Loss =  0.6930320262908936\n",
            "Batch Training Loss =  0.6927170753479004\n",
            "Batch Training Loss =  0.6909509301185608\n",
            "Batch Training Loss =  0.6961501836776733\n",
            "Batch Training Loss =  0.6946512460708618\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6905813813209534\n",
            "Batch Training Loss =  0.6936593651771545\n",
            "Batch Training Loss =  0.6950218081474304\n",
            "Batch Training Loss =  0.6938435435295105\n",
            "Batch Training Loss =  0.696157693862915\n",
            "Batch Training Loss =  0.6930843591690063\n",
            "Batch Training Loss =  0.6954354643821716\n",
            "Batch Training Loss =  0.6947321891784668\n",
            "Batch Training Loss =  0.695659339427948\n",
            "Batch Training Loss =  0.695984423160553\n",
            "Batch Training Loss =  0.6972936391830444\n",
            "Batch Training Loss =  0.6902356147766113\n",
            "Batch Training Loss =  0.7005021572113037\n",
            "Batch Training Loss =  0.6956968903541565\n",
            "Batch Training Loss =  0.6988331079483032\n",
            "Batch Training Loss =  0.691652238368988\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.7016898989677429\n",
            "Batch Training Loss =  0.6924985647201538\n",
            "Batch Training Loss =  0.7028107643127441\n",
            "Batch Training Loss =  0.6929013133049011\n",
            "Batch Training Loss =  0.6952190399169922\n",
            "Batch Training Loss =  0.693625271320343\n",
            "Batch Training Loss =  0.6942135691642761\n",
            "Batch Training Loss =  0.6965651512145996\n",
            "Batch Training Loss =  0.6945233345031738\n",
            "Batch Training Loss =  0.6930012702941895\n",
            "Batch Training Loss =  0.6940735578536987\n",
            "Batch Training Loss =  0.6939356923103333\n",
            "Batch Training Loss =  0.6931429505348206\n",
            "Batch Training Loss =  0.6923911571502686\n",
            "Batch Training Loss =  0.6923550367355347\n",
            "Batch Training Loss =  0.6961127519607544\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6962468028068542\n",
            "Batch Training Loss =  0.6768758893013\n",
            "Batch Training Loss =  0.7276619672775269\n",
            "Batch Training Loss =  0.6921325922012329\n",
            "Batch Training Loss =  0.6969266533851624\n",
            "Batch Training Loss =  0.6934528946876526\n",
            "Batch Training Loss =  0.6931962370872498\n",
            "Batch Training Loss =  0.6930757761001587\n",
            "Batch Training Loss =  0.6915924549102783\n",
            "Batch Training Loss =  0.692944347858429\n",
            "Batch Training Loss =  0.6943929195404053\n",
            "Batch Training Loss =  0.6941465735435486\n",
            "Batch Training Loss =  0.6908185482025146\n",
            "Batch Training Loss =  0.7007248401641846\n",
            "Batch Training Loss =  0.6923781037330627\n",
            "Batch Training Loss =  0.6904016733169556\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6932274103164673\n",
            "Batch Training Loss =  0.6934887766838074\n",
            "Batch Training Loss =  0.6922128796577454\n",
            "Batch Training Loss =  0.6926221251487732\n",
            "Batch Training Loss =  0.6914526224136353\n",
            "Batch Training Loss =  0.6973682641983032\n",
            "Batch Training Loss =  0.6932423114776611\n",
            "Batch Training Loss =  0.6929928064346313\n",
            "Batch Training Loss =  0.6946955919265747\n",
            "Batch Training Loss =  0.692954421043396\n",
            "Batch Training Loss =  0.6945369243621826\n",
            "Batch Training Loss =  0.6956915855407715\n",
            "Batch Training Loss =  0.6932036876678467\n",
            "Batch Training Loss =  0.6935131549835205\n",
            "Batch Training Loss =  0.6916942000389099\n",
            "Batch Training Loss =  0.7004857063293457\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.6931237578392029\n",
            "Batch Training Loss =  0.6893658638000488\n",
            "Batch Training Loss =  0.698183000087738\n",
            "Batch Training Loss =  0.6943102478981018\n",
            "Batch Training Loss =  0.6945440769195557\n",
            "Batch Training Loss =  0.6902413964271545\n",
            "Batch Training Loss =  0.7025609016418457\n",
            "Batch Training Loss =  0.6881239414215088\n",
            "Batch Training Loss =  0.7070043087005615\n",
            "Batch Training Loss =  0.6951863169670105\n",
            "Batch Training Loss =  0.6959713697433472\n",
            "Batch Training Loss =  0.691190779209137\n",
            "Batch Training Loss =  0.6913779377937317\n",
            "Batch Training Loss =  0.6893539428710938\n",
            "Batch Training Loss =  0.6913992166519165\n",
            "Batch Training Loss =  0.6925119757652283\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.6923525333404541\n",
            "Batch Training Loss =  0.6923505663871765\n",
            "Batch Training Loss =  0.6902628540992737\n",
            "Batch Training Loss =  0.6951931715011597\n",
            "Batch Training Loss =  0.6929629445075989\n",
            "Batch Training Loss =  0.6913266181945801\n",
            "Batch Training Loss =  0.6959965229034424\n",
            "Batch Training Loss =  0.6932352185249329\n",
            "Batch Training Loss =  0.6926300525665283\n",
            "Batch Training Loss =  0.6945051550865173\n",
            "Batch Training Loss =  0.6916952729225159\n",
            "Batch Training Loss =  0.6943155527114868\n",
            "Batch Training Loss =  0.6941089630126953\n",
            "Batch Training Loss =  0.694692850112915\n",
            "Batch Training Loss =  0.6968683004379272\n",
            "Batch Training Loss =  0.6934800744056702\n",
            "Validation Loss in this epoch is 0.690\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.7121126651763916\n",
            "Batch Training Loss =  0.7063940167427063\n",
            "Batch Training Loss =  0.695585310459137\n",
            "Batch Training Loss =  0.6954253315925598\n",
            "Batch Training Loss =  0.6977067589759827\n",
            "Batch Training Loss =  0.6915809512138367\n",
            "Batch Training Loss =  0.6959899067878723\n",
            "Batch Training Loss =  0.6931490302085876\n",
            "Batch Training Loss =  0.6931913495063782\n",
            "Batch Training Loss =  0.6971530318260193\n",
            "Batch Training Loss =  0.6933419704437256\n",
            "Batch Training Loss =  0.696501612663269\n",
            "Batch Training Loss =  0.6908168792724609\n",
            "Batch Training Loss =  0.6935703158378601\n",
            "Batch Training Loss =  0.6943073868751526\n",
            "Batch Training Loss =  0.6932657361030579\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.695587158203125\n",
            "Batch Training Loss =  0.6955715417861938\n",
            "Batch Training Loss =  0.6934277415275574\n",
            "Batch Training Loss =  0.6929389834403992\n",
            "Batch Training Loss =  0.6880571842193604\n",
            "Batch Training Loss =  0.7040095329284668\n",
            "Batch Training Loss =  0.6940252780914307\n",
            "Batch Training Loss =  0.6966434717178345\n",
            "Batch Training Loss =  0.6844318509101868\n",
            "Batch Training Loss =  0.6860148906707764\n",
            "Batch Training Loss =  0.6963977813720703\n",
            "Batch Training Loss =  0.6913830637931824\n",
            "Batch Training Loss =  0.6934151649475098\n",
            "Batch Training Loss =  0.6911841034889221\n",
            "Batch Training Loss =  0.6965887546539307\n",
            "Batch Training Loss =  0.6932170391082764\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6867954730987549\n",
            "Batch Training Loss =  0.6945064067840576\n",
            "Batch Training Loss =  0.70635986328125\n",
            "Batch Training Loss =  0.6861625909805298\n",
            "Batch Training Loss =  0.6977501511573792\n",
            "Batch Training Loss =  0.687721312046051\n",
            "Batch Training Loss =  0.6824248433113098\n",
            "Batch Training Loss =  0.6953976154327393\n",
            "Batch Training Loss =  0.6981346011161804\n",
            "Batch Training Loss =  0.6944488286972046\n",
            "Batch Training Loss =  0.7091440558433533\n",
            "Batch Training Loss =  0.6958619952201843\n",
            "Batch Training Loss =  0.6935946941375732\n",
            "Batch Training Loss =  0.692793071269989\n",
            "Batch Training Loss =  0.7006428241729736\n",
            "Batch Training Loss =  0.6947111487388611\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6936256289482117\n",
            "Batch Training Loss =  0.6885805726051331\n",
            "Batch Training Loss =  0.7035694122314453\n",
            "Batch Training Loss =  0.6938682794570923\n",
            "Batch Training Loss =  0.699420690536499\n",
            "Batch Training Loss =  0.6953188180923462\n",
            "Batch Training Loss =  0.6924341320991516\n",
            "Batch Training Loss =  0.694827139377594\n",
            "Batch Training Loss =  0.6925145983695984\n",
            "Batch Training Loss =  0.7029491662979126\n",
            "Batch Training Loss =  0.6854539513587952\n",
            "Batch Training Loss =  0.6860719919204712\n",
            "Batch Training Loss =  0.7125298976898193\n",
            "Batch Training Loss =  0.6877378821372986\n",
            "Batch Training Loss =  0.6904358863830566\n",
            "Batch Training Loss =  0.701840877532959\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6935615539550781\n",
            "Batch Training Loss =  0.6991543769836426\n",
            "Batch Training Loss =  0.6936161518096924\n",
            "Batch Training Loss =  0.6910159587860107\n",
            "Batch Training Loss =  0.6900515556335449\n",
            "Batch Training Loss =  0.6977563500404358\n",
            "Batch Training Loss =  0.6925216913223267\n",
            "Batch Training Loss =  0.6901304125785828\n",
            "Batch Training Loss =  0.7028366923332214\n",
            "Batch Training Loss =  0.6916658878326416\n",
            "Batch Training Loss =  0.6972383260726929\n",
            "Batch Training Loss =  0.6927806735038757\n",
            "Batch Training Loss =  0.6924717426300049\n",
            "Batch Training Loss =  0.691806435585022\n",
            "Batch Training Loss =  0.6898360252380371\n",
            "Batch Training Loss =  0.7073920369148254\n",
            "Validation Loss in this epoch is 0.690\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.6913511753082275\n",
            "Batch Training Loss =  0.6968913078308105\n",
            "Batch Training Loss =  0.693236231803894\n",
            "Batch Training Loss =  0.6940955519676208\n",
            "Batch Training Loss =  0.6961862444877625\n",
            "Batch Training Loss =  0.7043546438217163\n",
            "Batch Training Loss =  0.6945433616638184\n",
            "Batch Training Loss =  0.6945203542709351\n",
            "Batch Training Loss =  0.7006336450576782\n",
            "Batch Training Loss =  0.6953930854797363\n",
            "Batch Training Loss =  0.7004047632217407\n",
            "Batch Training Loss =  0.693136990070343\n",
            "Batch Training Loss =  0.6937474012374878\n",
            "Batch Training Loss =  0.6957697868347168\n",
            "Batch Training Loss =  0.6909888982772827\n",
            "Batch Training Loss =  0.6976051330566406\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6925333142280579\n",
            "Batch Training Loss =  0.6929814219474792\n",
            "Batch Training Loss =  0.6946534514427185\n",
            "Batch Training Loss =  0.6912305951118469\n",
            "Batch Training Loss =  0.700721800327301\n",
            "Batch Training Loss =  0.6956215500831604\n",
            "Batch Training Loss =  0.6976928114891052\n",
            "Batch Training Loss =  0.6983367204666138\n",
            "Batch Training Loss =  0.6938363909721375\n",
            "Batch Training Loss =  0.6990002393722534\n",
            "Batch Training Loss =  0.699218213558197\n",
            "Batch Training Loss =  0.6930500864982605\n",
            "Batch Training Loss =  0.6964969038963318\n",
            "Batch Training Loss =  0.6931006908416748\n",
            "Batch Training Loss =  0.6923985481262207\n",
            "Batch Training Loss =  0.6931487917900085\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.6955702900886536\n",
            "Batch Training Loss =  0.6995001435279846\n",
            "Batch Training Loss =  0.6917389631271362\n",
            "Batch Training Loss =  0.6965907216072083\n",
            "Batch Training Loss =  0.6926304697990417\n",
            "Batch Training Loss =  0.6987936496734619\n",
            "Batch Training Loss =  0.7060787677764893\n",
            "Batch Training Loss =  0.6932468414306641\n",
            "Batch Training Loss =  0.6935020685195923\n",
            "Batch Training Loss =  0.6963061690330505\n",
            "Batch Training Loss =  0.6923476457595825\n",
            "Batch Training Loss =  0.693068265914917\n",
            "Batch Training Loss =  0.6929561495780945\n",
            "Batch Training Loss =  0.6921594142913818\n",
            "Batch Training Loss =  0.6923491954803467\n",
            "Batch Training Loss =  0.693061888217926\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.692955493927002\n",
            "Batch Training Loss =  0.6925545334815979\n",
            "Batch Training Loss =  0.6901925802230835\n",
            "Batch Training Loss =  0.6942530870437622\n",
            "Batch Training Loss =  0.6954296827316284\n",
            "Batch Training Loss =  0.6931577920913696\n",
            "Batch Training Loss =  0.6932222247123718\n",
            "Batch Training Loss =  0.6902500987052917\n",
            "Batch Training Loss =  0.6899718642234802\n",
            "Batch Training Loss =  0.7004831433296204\n",
            "Batch Training Loss =  0.6931381225585938\n",
            "Batch Training Loss =  0.6929977536201477\n",
            "Batch Training Loss =  0.6938301920890808\n",
            "Batch Training Loss =  0.6932191252708435\n",
            "Batch Training Loss =  0.6935828924179077\n",
            "Batch Training Loss =  0.695107102394104\n",
            "Validation Loss in this epoch is 0.690\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6899558305740356\n",
            "Batch Training Loss =  0.7110476493835449\n",
            "Batch Training Loss =  0.6932200789451599\n",
            "Batch Training Loss =  0.6966561079025269\n",
            "Batch Training Loss =  0.6929159760475159\n",
            "Batch Training Loss =  0.7062222957611084\n",
            "Batch Training Loss =  0.6931825876235962\n",
            "Batch Training Loss =  0.6894037127494812\n",
            "Batch Training Loss =  0.7067869305610657\n",
            "Batch Training Loss =  0.6934571862220764\n",
            "Batch Training Loss =  0.6905410885810852\n",
            "Batch Training Loss =  0.7050604820251465\n",
            "Batch Training Loss =  0.7001634836196899\n",
            "Batch Training Loss =  0.6924195885658264\n",
            "Batch Training Loss =  0.6955046653747559\n",
            "Batch Training Loss =  0.6947020888328552\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6956708431243896\n",
            "Batch Training Loss =  0.6953040957450867\n",
            "Batch Training Loss =  0.6935003399848938\n",
            "Batch Training Loss =  0.6879974603652954\n",
            "Batch Training Loss =  0.6881412267684937\n",
            "Batch Training Loss =  0.6936519145965576\n",
            "Batch Training Loss =  0.7004087567329407\n",
            "Batch Training Loss =  0.6957070231437683\n",
            "Batch Training Loss =  0.6906630992889404\n",
            "Batch Training Loss =  0.6938127279281616\n",
            "Batch Training Loss =  0.6944425106048584\n",
            "Batch Training Loss =  0.6929217576980591\n",
            "Batch Training Loss =  0.6930138468742371\n",
            "Batch Training Loss =  0.695213258266449\n",
            "Batch Training Loss =  0.6861985921859741\n",
            "Batch Training Loss =  0.7151227593421936\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6928856372833252\n",
            "Batch Training Loss =  0.6940013766288757\n",
            "Batch Training Loss =  0.6956004500389099\n",
            "Batch Training Loss =  0.690083920955658\n",
            "Batch Training Loss =  0.6939760446548462\n",
            "Batch Training Loss =  0.6952506303787231\n",
            "Batch Training Loss =  0.6927217841148376\n",
            "Batch Training Loss =  0.6957257986068726\n",
            "Batch Training Loss =  0.692903459072113\n",
            "Batch Training Loss =  0.6824494004249573\n",
            "Batch Training Loss =  0.7084752917289734\n",
            "Batch Training Loss =  0.7020167708396912\n",
            "Batch Training Loss =  0.6904372572898865\n",
            "Batch Training Loss =  0.6996750831604004\n",
            "Batch Training Loss =  0.6929545402526855\n",
            "Batch Training Loss =  0.6926242113113403\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6876512169837952\n",
            "Batch Training Loss =  0.7028192281723022\n",
            "Batch Training Loss =  0.6923479437828064\n",
            "Batch Training Loss =  0.6973174810409546\n",
            "Batch Training Loss =  0.6973708271980286\n",
            "Batch Training Loss =  0.698062539100647\n",
            "Batch Training Loss =  0.6978504061698914\n",
            "Batch Training Loss =  0.6920806765556335\n",
            "Batch Training Loss =  0.696215808391571\n",
            "Batch Training Loss =  0.6978985667228699\n",
            "Batch Training Loss =  0.6926528215408325\n",
            "Batch Training Loss =  0.712688148021698\n",
            "Batch Training Loss =  0.6935648322105408\n",
            "Batch Training Loss =  0.6939080953598022\n",
            "Batch Training Loss =  0.6949912309646606\n",
            "Batch Training Loss =  0.6909623146057129\n",
            "Validation Loss in this epoch is 0.690\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.7022474408149719\n",
            "Batch Training Loss =  0.692495584487915\n",
            "Batch Training Loss =  0.6900776624679565\n",
            "Batch Training Loss =  0.7028766870498657\n",
            "Batch Training Loss =  0.6874152421951294\n",
            "Batch Training Loss =  0.7130456566810608\n",
            "Batch Training Loss =  0.6953923106193542\n",
            "Batch Training Loss =  0.6929492950439453\n",
            "Batch Training Loss =  0.6926068663597107\n",
            "Batch Training Loss =  0.6902825832366943\n",
            "Batch Training Loss =  0.6942332983016968\n",
            "Batch Training Loss =  0.691581130027771\n",
            "Batch Training Loss =  0.6914541125297546\n",
            "Batch Training Loss =  0.6933431029319763\n",
            "Batch Training Loss =  0.6906476020812988\n",
            "Batch Training Loss =  0.7012222409248352\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6934665441513062\n",
            "Batch Training Loss =  0.6927936673164368\n",
            "Batch Training Loss =  0.6929553747177124\n",
            "Batch Training Loss =  0.6968713998794556\n",
            "Batch Training Loss =  0.6932318210601807\n",
            "Batch Training Loss =  0.6977255940437317\n",
            "Batch Training Loss =  0.6959999799728394\n",
            "Batch Training Loss =  0.6990817785263062\n",
            "Batch Training Loss =  0.6882840991020203\n",
            "Batch Training Loss =  0.691616415977478\n",
            "Batch Training Loss =  0.6964277625083923\n",
            "Batch Training Loss =  0.6945001482963562\n",
            "Batch Training Loss =  0.6903361678123474\n",
            "Batch Training Loss =  0.6977059245109558\n",
            "Batch Training Loss =  0.6927253603935242\n",
            "Batch Training Loss =  0.6923492550849915\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.7012043595314026\n",
            "Batch Training Loss =  0.6916612386703491\n",
            "Batch Training Loss =  0.7106141448020935\n",
            "Batch Training Loss =  0.6994702219963074\n",
            "Batch Training Loss =  0.6913067698478699\n",
            "Batch Training Loss =  0.6903628706932068\n",
            "Batch Training Loss =  0.694974958896637\n",
            "Batch Training Loss =  0.6958634853363037\n",
            "Batch Training Loss =  0.6883165836334229\n",
            "Batch Training Loss =  0.6934067010879517\n",
            "Batch Training Loss =  0.6946350336074829\n",
            "Batch Training Loss =  0.6916378736495972\n",
            "Batch Training Loss =  0.6923849582672119\n",
            "Batch Training Loss =  0.6947181224822998\n",
            "Batch Training Loss =  0.6936293840408325\n",
            "Batch Training Loss =  0.6998844146728516\n",
            "Validation Loss in this epoch is 0.701\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.6913577318191528\n",
            "Batch Training Loss =  0.6924745440483093\n",
            "Batch Training Loss =  0.6965934038162231\n",
            "Batch Training Loss =  0.6953139305114746\n",
            "Batch Training Loss =  0.7004294395446777\n",
            "Batch Training Loss =  0.6931474804878235\n",
            "Batch Training Loss =  0.6941964626312256\n",
            "Batch Training Loss =  0.6951249837875366\n",
            "Batch Training Loss =  0.6940241456031799\n",
            "Batch Training Loss =  0.6933014392852783\n",
            "Batch Training Loss =  0.6935582160949707\n",
            "Batch Training Loss =  0.6932129859924316\n",
            "Batch Training Loss =  0.6939835548400879\n",
            "Batch Training Loss =  0.7199514508247375\n",
            "Batch Training Loss =  0.6976168751716614\n",
            "Batch Training Loss =  0.6915892958641052\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6860318779945374\n",
            "Batch Training Loss =  0.705464243888855\n",
            "Batch Training Loss =  0.6951205730438232\n",
            "Batch Training Loss =  0.697818398475647\n",
            "Batch Training Loss =  0.6935181617736816\n",
            "Batch Training Loss =  0.6957405805587769\n",
            "Batch Training Loss =  0.6953023672103882\n",
            "Batch Training Loss =  0.6947904825210571\n",
            "Batch Training Loss =  0.7008585333824158\n",
            "Batch Training Loss =  0.6958997249603271\n",
            "Batch Training Loss =  0.6982579231262207\n",
            "Batch Training Loss =  0.6974372863769531\n",
            "Batch Training Loss =  0.6826645731925964\n",
            "Batch Training Loss =  0.6952857375144958\n",
            "Batch Training Loss =  0.7087926268577576\n",
            "Batch Training Loss =  0.6958200335502625\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6947498321533203\n",
            "Batch Training Loss =  0.6918588280677795\n",
            "Batch Training Loss =  0.6982794404029846\n",
            "Batch Training Loss =  0.6920163631439209\n",
            "Batch Training Loss =  0.6954967379570007\n",
            "Batch Training Loss =  0.6934953927993774\n",
            "Batch Training Loss =  0.6931144595146179\n",
            "Batch Training Loss =  0.6929613351821899\n",
            "Batch Training Loss =  0.6929470896720886\n",
            "Batch Training Loss =  0.6965600848197937\n",
            "Batch Training Loss =  0.7015359401702881\n",
            "Batch Training Loss =  0.6933446526527405\n",
            "Batch Training Loss =  0.693559467792511\n",
            "Batch Training Loss =  0.6920640468597412\n",
            "Batch Training Loss =  0.6924527883529663\n",
            "Batch Training Loss =  0.6973726749420166\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6929523348808289\n",
            "Batch Training Loss =  0.6933333873748779\n",
            "Batch Training Loss =  0.6934857368469238\n",
            "Batch Training Loss =  0.6915990710258484\n",
            "Batch Training Loss =  0.6924868822097778\n",
            "Batch Training Loss =  0.6940601468086243\n",
            "Batch Training Loss =  0.6926091909408569\n",
            "Batch Training Loss =  0.6913598775863647\n",
            "Batch Training Loss =  0.6934190988540649\n",
            "Batch Training Loss =  0.6954258680343628\n",
            "Batch Training Loss =  0.6933068633079529\n",
            "Batch Training Loss =  0.6938875317573547\n",
            "Batch Training Loss =  0.6914294362068176\n",
            "Batch Training Loss =  0.6953185200691223\n",
            "Batch Training Loss =  0.6926189661026001\n",
            "Batch Training Loss =  0.6971020698547363\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.7032918334007263\n",
            "Batch Training Loss =  0.7205650210380554\n",
            "Batch Training Loss =  0.7010697722434998\n",
            "Batch Training Loss =  0.6963534355163574\n",
            "Batch Training Loss =  0.6938632130622864\n",
            "Batch Training Loss =  0.6954808235168457\n",
            "Batch Training Loss =  0.6944246888160706\n",
            "Batch Training Loss =  0.6915503740310669\n",
            "Batch Training Loss =  0.7015532851219177\n",
            "Batch Training Loss =  0.7024363875389099\n",
            "Batch Training Loss =  0.6902389526367188\n",
            "Batch Training Loss =  0.6939196586608887\n",
            "Batch Training Loss =  0.6930701732635498\n",
            "Batch Training Loss =  0.6953849196434021\n",
            "Batch Training Loss =  0.6929998993873596\n",
            "Batch Training Loss =  0.6916223168373108\n",
            "Validation Loss in this epoch is 0.690\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6972707509994507\n",
            "Batch Training Loss =  0.6932016015052795\n",
            "Batch Training Loss =  0.6929052472114563\n",
            "Batch Training Loss =  0.6980825066566467\n",
            "Batch Training Loss =  0.6993754506111145\n",
            "Batch Training Loss =  0.6926108598709106\n",
            "Batch Training Loss =  0.697797417640686\n",
            "Batch Training Loss =  0.7177110314369202\n",
            "Batch Training Loss =  0.6965754628181458\n",
            "Batch Training Loss =  0.6958295702934265\n",
            "Batch Training Loss =  0.690640926361084\n",
            "Batch Training Loss =  0.7024633288383484\n",
            "Batch Training Loss =  0.6937687397003174\n",
            "Batch Training Loss =  0.6938111782073975\n",
            "Batch Training Loss =  0.6941115856170654\n",
            "Batch Training Loss =  0.6953819394111633\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6958969831466675\n",
            "Batch Training Loss =  0.6958144307136536\n",
            "Batch Training Loss =  0.6929601430892944\n",
            "Batch Training Loss =  0.6911929249763489\n",
            "Batch Training Loss =  0.6943556070327759\n",
            "Batch Training Loss =  0.6916446685791016\n",
            "Batch Training Loss =  0.6959290504455566\n",
            "Batch Training Loss =  0.6925992369651794\n",
            "Batch Training Loss =  0.7001394629478455\n",
            "Batch Training Loss =  0.6942599415779114\n",
            "Batch Training Loss =  0.6930071115493774\n",
            "Batch Training Loss =  0.693450391292572\n",
            "Batch Training Loss =  0.697823703289032\n",
            "Batch Training Loss =  0.7008708715438843\n",
            "Batch Training Loss =  0.6940622925758362\n",
            "Batch Training Loss =  0.6931694746017456\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.693747341632843\n",
            "Batch Training Loss =  0.6933609247207642\n",
            "Batch Training Loss =  0.7012964487075806\n",
            "Batch Training Loss =  0.6964522004127502\n",
            "Batch Training Loss =  0.6931459307670593\n",
            "Batch Training Loss =  0.6935989856719971\n",
            "Batch Training Loss =  0.6935973167419434\n",
            "Batch Training Loss =  0.6925774216651917\n",
            "Batch Training Loss =  0.6752654910087585\n",
            "Batch Training Loss =  0.7226234674453735\n",
            "Batch Training Loss =  0.6913895606994629\n",
            "Batch Training Loss =  0.6953431963920593\n",
            "Batch Training Loss =  0.6929685473442078\n",
            "Batch Training Loss =  0.6978939175605774\n",
            "Batch Training Loss =  0.702392578125\n",
            "Batch Training Loss =  0.6921125650405884\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.6938825845718384\n",
            "Batch Training Loss =  0.6929581165313721\n",
            "Batch Training Loss =  0.6939231157302856\n",
            "Batch Training Loss =  0.6948642134666443\n",
            "Batch Training Loss =  0.6913612484931946\n",
            "Batch Training Loss =  0.6944915652275085\n",
            "Batch Training Loss =  0.6925321221351624\n",
            "Batch Training Loss =  0.6963787078857422\n",
            "Batch Training Loss =  0.6892539858818054\n",
            "Batch Training Loss =  0.6869354248046875\n",
            "Batch Training Loss =  0.6954102516174316\n",
            "Batch Training Loss =  0.6932907104492188\n",
            "Batch Training Loss =  0.6946938037872314\n",
            "Batch Training Loss =  0.6947309970855713\n",
            "Batch Training Loss =  0.7055454850196838\n",
            "Batch Training Loss =  0.693569004535675\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.6930768489837646\n",
            "Batch Training Loss =  0.6927516460418701\n",
            "Batch Training Loss =  0.6964049339294434\n",
            "Batch Training Loss =  0.6883862018585205\n",
            "Batch Training Loss =  0.6995534300804138\n",
            "Batch Training Loss =  0.6963708996772766\n",
            "Batch Training Loss =  0.7034614682197571\n",
            "Batch Training Loss =  0.6915392279624939\n",
            "Batch Training Loss =  0.6969824433326721\n",
            "Batch Training Loss =  0.6927526593208313\n",
            "Batch Training Loss =  0.6949264407157898\n",
            "Batch Training Loss =  0.6917377710342407\n",
            "Batch Training Loss =  0.692379355430603\n",
            "Batch Training Loss =  0.6962715983390808\n",
            "Batch Training Loss =  0.6932492852210999\n",
            "Batch Training Loss =  0.6935065388679504\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.6918683648109436\n",
            "Batch Training Loss =  0.6931694149971008\n",
            "Batch Training Loss =  0.6908521056175232\n",
            "Batch Training Loss =  0.691346287727356\n",
            "Batch Training Loss =  0.6946189999580383\n",
            "Batch Training Loss =  0.6946874260902405\n",
            "Batch Training Loss =  0.6912436485290527\n",
            "Batch Training Loss =  0.701881468296051\n",
            "Batch Training Loss =  0.6938152313232422\n",
            "Batch Training Loss =  0.6953027248382568\n",
            "Batch Training Loss =  0.7016178369522095\n",
            "Batch Training Loss =  0.6929564476013184\n",
            "Batch Training Loss =  0.6901862621307373\n",
            "Batch Training Loss =  0.6952466368675232\n",
            "Batch Training Loss =  0.6977930665016174\n",
            "Batch Training Loss =  0.6925112009048462\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.7004612684249878\n",
            "Batch Training Loss =  0.7055309414863586\n",
            "Batch Training Loss =  0.6940564513206482\n",
            "Batch Training Loss =  0.6944308280944824\n",
            "Batch Training Loss =  0.691774308681488\n",
            "Batch Training Loss =  0.703295111656189\n",
            "Batch Training Loss =  0.6929473280906677\n",
            "Batch Training Loss =  0.6911365389823914\n",
            "Batch Training Loss =  0.6980966925621033\n",
            "Batch Training Loss =  0.6933103203773499\n",
            "Batch Training Loss =  0.6918736100196838\n",
            "Batch Training Loss =  0.6974552273750305\n",
            "Batch Training Loss =  0.6968252658843994\n",
            "Batch Training Loss =  0.6929601430892944\n",
            "Batch Training Loss =  0.6921457052230835\n",
            "Batch Training Loss =  0.6976709961891174\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6986220479011536\n",
            "Batch Training Loss =  0.6958402991294861\n",
            "Batch Training Loss =  0.6942217946052551\n",
            "Batch Training Loss =  0.6944602727890015\n",
            "Batch Training Loss =  0.6920315623283386\n",
            "Batch Training Loss =  0.7031400203704834\n",
            "Batch Training Loss =  0.6959195733070374\n",
            "Batch Training Loss =  0.6929725408554077\n",
            "Batch Training Loss =  0.6953828930854797\n",
            "Batch Training Loss =  0.6951898336410522\n",
            "Batch Training Loss =  0.6924504637718201\n",
            "Batch Training Loss =  0.7008647918701172\n",
            "Batch Training Loss =  0.7097461819648743\n",
            "Batch Training Loss =  0.6938325762748718\n",
            "Batch Training Loss =  0.6866922974586487\n",
            "Batch Training Loss =  0.7178266048431396\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6853204369544983\n",
            "Batch Training Loss =  0.6881458163261414\n",
            "Batch Training Loss =  0.6955217719078064\n",
            "Batch Training Loss =  0.7054927349090576\n",
            "Batch Training Loss =  0.7002876996994019\n",
            "Batch Training Loss =  0.6899373531341553\n",
            "Batch Training Loss =  0.6943112015724182\n",
            "Batch Training Loss =  0.6931268572807312\n",
            "Batch Training Loss =  0.6934746503829956\n",
            "Batch Training Loss =  0.6938141584396362\n",
            "Batch Training Loss =  0.6945186853408813\n",
            "Batch Training Loss =  0.6908811330795288\n",
            "Batch Training Loss =  0.7007975578308105\n",
            "Batch Training Loss =  0.6943894028663635\n",
            "Batch Training Loss =  0.6941449642181396\n",
            "Batch Training Loss =  0.6927388906478882\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.692958652973175\n",
            "Batch Training Loss =  0.6889621019363403\n",
            "Batch Training Loss =  0.7048084735870361\n",
            "Batch Training Loss =  0.6924257874488831\n",
            "Batch Training Loss =  0.6967260241508484\n",
            "Batch Training Loss =  0.6885896921157837\n",
            "Batch Training Loss =  0.6933574676513672\n",
            "Batch Training Loss =  0.689198911190033\n",
            "Batch Training Loss =  0.7028811573982239\n",
            "Batch Training Loss =  0.6929481029510498\n",
            "Batch Training Loss =  0.6955397129058838\n",
            "Batch Training Loss =  0.698737382888794\n",
            "Batch Training Loss =  0.6963565349578857\n",
            "Batch Training Loss =  0.7030097246170044\n",
            "Batch Training Loss =  0.6950443983078003\n",
            "Batch Training Loss =  0.6923823356628418\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6878030300140381\n",
            "Batch Training Loss =  0.6963878870010376\n",
            "Batch Training Loss =  0.6944680213928223\n",
            "Batch Training Loss =  0.6974760293960571\n",
            "Batch Training Loss =  0.6973515152931213\n",
            "Batch Training Loss =  0.6930887699127197\n",
            "Batch Training Loss =  0.6964223980903625\n",
            "Batch Training Loss =  0.6913955211639404\n",
            "Batch Training Loss =  0.6893934607505798\n",
            "Batch Training Loss =  0.7040039300918579\n",
            "Batch Training Loss =  0.6955722570419312\n",
            "Batch Training Loss =  0.692606508731842\n",
            "Batch Training Loss =  0.6897439360618591\n",
            "Batch Training Loss =  0.6996351480484009\n",
            "Batch Training Loss =  0.6934716701507568\n",
            "Batch Training Loss =  0.6927564144134521\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.7007524371147156\n",
            "Batch Training Loss =  0.6888236403465271\n",
            "Batch Training Loss =  0.6839304566383362\n",
            "Batch Training Loss =  0.6992067694664001\n",
            "Batch Training Loss =  0.6932421326637268\n",
            "Batch Training Loss =  0.6940863728523254\n",
            "Batch Training Loss =  0.6931697130203247\n",
            "Batch Training Loss =  0.6930016875267029\n",
            "Batch Training Loss =  0.6926895380020142\n",
            "Batch Training Loss =  0.6955154538154602\n",
            "Batch Training Loss =  0.6947821974754333\n",
            "Batch Training Loss =  0.6926948428153992\n",
            "Batch Training Loss =  0.6923701763153076\n",
            "Batch Training Loss =  0.6970313191413879\n",
            "Batch Training Loss =  0.6930860877037048\n",
            "Batch Training Loss =  0.6974002122879028\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.705421507358551\n",
            "Batch Training Loss =  0.6889972686767578\n",
            "Batch Training Loss =  0.705303430557251\n",
            "Batch Training Loss =  0.694742739200592\n",
            "Batch Training Loss =  0.6955082416534424\n",
            "Batch Training Loss =  0.7024584412574768\n",
            "Batch Training Loss =  0.6979696154594421\n",
            "Batch Training Loss =  0.6939722299575806\n",
            "Batch Training Loss =  0.6989257335662842\n",
            "Batch Training Loss =  0.6948856115341187\n",
            "Batch Training Loss =  0.6931754946708679\n",
            "Batch Training Loss =  0.6931514739990234\n",
            "Batch Training Loss =  0.6932917237281799\n",
            "Batch Training Loss =  0.6949172019958496\n",
            "Batch Training Loss =  0.6920008659362793\n",
            "Batch Training Loss =  0.688393235206604\n",
            "Validation Loss in this epoch is 0.702\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6813770532608032\n",
            "Batch Training Loss =  0.7008693218231201\n",
            "Batch Training Loss =  0.7004125714302063\n",
            "Batch Training Loss =  0.6947436332702637\n",
            "Batch Training Loss =  0.6936442852020264\n",
            "Batch Training Loss =  0.6863330006599426\n",
            "Batch Training Loss =  0.6836439371109009\n",
            "Batch Training Loss =  0.702217161655426\n",
            "Batch Training Loss =  0.7011772394180298\n",
            "Batch Training Loss =  0.693368136882782\n",
            "Batch Training Loss =  0.6870258450508118\n",
            "Batch Training Loss =  0.702849805355072\n",
            "Batch Training Loss =  0.6930339932441711\n",
            "Batch Training Loss =  0.6922100782394409\n",
            "Batch Training Loss =  0.6896544098854065\n",
            "Batch Training Loss =  0.714515209197998\n",
            "Validation Loss in this epoch is 0.706\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6994876265525818\n",
            "Batch Training Loss =  0.6869215369224548\n",
            "Batch Training Loss =  0.6940300464630127\n",
            "Batch Training Loss =  0.6996645331382751\n",
            "Batch Training Loss =  0.6919825673103333\n",
            "Batch Training Loss =  0.6951566338539124\n",
            "Batch Training Loss =  0.6928398609161377\n",
            "Batch Training Loss =  0.6976418495178223\n",
            "Batch Training Loss =  0.7076802849769592\n",
            "Batch Training Loss =  0.696116030216217\n",
            "Batch Training Loss =  0.694153904914856\n",
            "Batch Training Loss =  0.6943970322608948\n",
            "Batch Training Loss =  0.6927856206893921\n",
            "Batch Training Loss =  0.6905401349067688\n",
            "Batch Training Loss =  0.6969651579856873\n",
            "Batch Training Loss =  0.6933078765869141\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.693459689617157\n",
            "Batch Training Loss =  0.6902103424072266\n",
            "Batch Training Loss =  0.7017043232917786\n",
            "Batch Training Loss =  0.6937035322189331\n",
            "Batch Training Loss =  0.6932966709136963\n",
            "Batch Training Loss =  0.6889246106147766\n",
            "Batch Training Loss =  0.7010023593902588\n",
            "Batch Training Loss =  0.6939588189125061\n",
            "Batch Training Loss =  0.6919063329696655\n",
            "Batch Training Loss =  0.692463755607605\n",
            "Batch Training Loss =  0.6906630992889404\n",
            "Batch Training Loss =  0.6879388689994812\n",
            "Batch Training Loss =  0.6816692352294922\n",
            "Batch Training Loss =  0.7007178664207458\n",
            "Batch Training Loss =  0.6973517537117004\n",
            "Batch Training Loss =  0.6940099596977234\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6916365027427673\n",
            "Batch Training Loss =  0.6896933913230896\n",
            "Batch Training Loss =  0.6832435131072998\n",
            "Batch Training Loss =  0.7210834622383118\n",
            "Batch Training Loss =  0.693208634853363\n",
            "Batch Training Loss =  0.6907998919487\n",
            "Batch Training Loss =  0.6925636529922485\n",
            "Batch Training Loss =  0.6976825594902039\n",
            "Batch Training Loss =  0.6947282552719116\n",
            "Batch Training Loss =  0.6954805254936218\n",
            "Batch Training Loss =  0.6940104961395264\n",
            "Batch Training Loss =  0.6946164965629578\n",
            "Batch Training Loss =  0.6990383863449097\n",
            "Batch Training Loss =  0.6953670382499695\n",
            "Batch Training Loss =  0.6956408023834229\n",
            "Batch Training Loss =  0.6970942616462708\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.6952770948410034\n",
            "Batch Training Loss =  0.6939365863800049\n",
            "Batch Training Loss =  0.6959809064865112\n",
            "Batch Training Loss =  0.6920284032821655\n",
            "Batch Training Loss =  0.695137619972229\n",
            "Batch Training Loss =  0.693523108959198\n",
            "Batch Training Loss =  0.6926209330558777\n",
            "Batch Training Loss =  0.693501889705658\n",
            "Batch Training Loss =  0.6938436031341553\n",
            "Batch Training Loss =  0.6955372095108032\n",
            "Batch Training Loss =  0.6918061971664429\n",
            "Batch Training Loss =  0.6973749399185181\n",
            "Batch Training Loss =  0.6906291246414185\n",
            "Batch Training Loss =  0.69554603099823\n",
            "Batch Training Loss =  0.6929773688316345\n",
            "Batch Training Loss =  0.6899920701980591\n",
            "Validation Loss in this epoch is 0.702\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6935155987739563\n",
            "Batch Training Loss =  0.6969059109687805\n",
            "Batch Training Loss =  0.6930608153343201\n",
            "Batch Training Loss =  0.6929475665092468\n",
            "Batch Training Loss =  0.6914799213409424\n",
            "Batch Training Loss =  0.681027889251709\n",
            "Batch Training Loss =  0.720488965511322\n",
            "Batch Training Loss =  0.6930102705955505\n",
            "Batch Training Loss =  0.6923883557319641\n",
            "Batch Training Loss =  0.6982040405273438\n",
            "Batch Training Loss =  0.6993366479873657\n",
            "Batch Training Loss =  0.69398033618927\n",
            "Batch Training Loss =  0.6934961676597595\n",
            "Batch Training Loss =  0.6936328411102295\n",
            "Batch Training Loss =  0.7000758647918701\n",
            "Batch Training Loss =  0.6939769983291626\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.6932801604270935\n",
            "Batch Training Loss =  0.6932990550994873\n",
            "Batch Training Loss =  0.6942091584205627\n",
            "Batch Training Loss =  0.7020972371101379\n",
            "Batch Training Loss =  0.6934149861335754\n",
            "Batch Training Loss =  0.6928197741508484\n",
            "Batch Training Loss =  0.6967463493347168\n",
            "Batch Training Loss =  0.7010287642478943\n",
            "Batch Training Loss =  0.6909032464027405\n",
            "Batch Training Loss =  0.6889340281486511\n",
            "Batch Training Loss =  0.6958943009376526\n",
            "Batch Training Loss =  0.6923997402191162\n",
            "Batch Training Loss =  0.6949318647384644\n",
            "Batch Training Loss =  0.6924497485160828\n",
            "Batch Training Loss =  0.6977192163467407\n",
            "Batch Training Loss =  0.6923041343688965\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.6924380660057068\n",
            "Batch Training Loss =  0.6940054297447205\n",
            "Batch Training Loss =  0.6939481496810913\n",
            "Batch Training Loss =  0.692167341709137\n",
            "Batch Training Loss =  0.6905472278594971\n",
            "Batch Training Loss =  0.6866956949234009\n",
            "Batch Training Loss =  0.7180173993110657\n",
            "Batch Training Loss =  0.6928744316101074\n",
            "Batch Training Loss =  0.6933695077896118\n",
            "Batch Training Loss =  0.6947728991508484\n",
            "Batch Training Loss =  0.6922040581703186\n",
            "Batch Training Loss =  0.7014718651771545\n",
            "Batch Training Loss =  0.6929581165313721\n",
            "Batch Training Loss =  0.68976891040802\n",
            "Batch Training Loss =  0.6861827373504639\n",
            "Batch Training Loss =  0.6980729103088379\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6923468708992004\n",
            "Batch Training Loss =  0.6959521770477295\n",
            "Batch Training Loss =  0.6926447153091431\n",
            "Batch Training Loss =  0.6966406106948853\n",
            "Batch Training Loss =  0.6943718194961548\n",
            "Batch Training Loss =  0.6933435797691345\n",
            "Batch Training Loss =  0.6928613781929016\n",
            "Batch Training Loss =  0.6920254230499268\n",
            "Batch Training Loss =  0.6923473477363586\n",
            "Batch Training Loss =  0.6952136754989624\n",
            "Batch Training Loss =  0.6923646330833435\n",
            "Batch Training Loss =  0.7000942826271057\n",
            "Batch Training Loss =  0.693004310131073\n",
            "Batch Training Loss =  0.6928073167800903\n",
            "Batch Training Loss =  0.6943092942237854\n",
            "Batch Training Loss =  0.6937196254730225\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6933319568634033\n",
            "Batch Training Loss =  0.6931291818618774\n",
            "Batch Training Loss =  0.6939889788627625\n",
            "Batch Training Loss =  0.6931421756744385\n",
            "Batch Training Loss =  0.6915813684463501\n",
            "Batch Training Loss =  0.6966744065284729\n",
            "Batch Training Loss =  0.6923667788505554\n",
            "Batch Training Loss =  0.695717453956604\n",
            "Batch Training Loss =  0.6932849287986755\n",
            "Batch Training Loss =  0.6929035782814026\n",
            "Batch Training Loss =  0.6934038400650024\n",
            "Batch Training Loss =  0.6948199272155762\n",
            "Batch Training Loss =  0.7141478657722473\n",
            "Batch Training Loss =  0.7031219005584717\n",
            "Batch Training Loss =  0.6903373003005981\n",
            "Batch Training Loss =  0.7053292989730835\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6924523115158081\n",
            "Batch Training Loss =  0.6923486590385437\n",
            "Batch Training Loss =  0.6982323527336121\n",
            "Batch Training Loss =  0.6905490159988403\n",
            "Batch Training Loss =  0.702974259853363\n",
            "Batch Training Loss =  0.697040855884552\n",
            "Batch Training Loss =  0.6927591562271118\n",
            "Batch Training Loss =  0.6929572820663452\n",
            "Batch Training Loss =  0.6925508975982666\n",
            "Batch Training Loss =  0.6924199461936951\n",
            "Batch Training Loss =  0.6961268782615662\n",
            "Batch Training Loss =  0.6984806060791016\n",
            "Batch Training Loss =  0.697843074798584\n",
            "Batch Training Loss =  0.6933820843696594\n",
            "Batch Training Loss =  0.6921440362930298\n",
            "Batch Training Loss =  0.7040456533432007\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.6950315237045288\n",
            "Batch Training Loss =  0.6938052177429199\n",
            "Batch Training Loss =  0.6960383057594299\n",
            "Batch Training Loss =  0.6935684084892273\n",
            "Batch Training Loss =  0.6930767297744751\n",
            "Batch Training Loss =  0.6915954351425171\n",
            "Batch Training Loss =  0.6944364905357361\n",
            "Batch Training Loss =  0.6891534924507141\n",
            "Batch Training Loss =  0.6914721131324768\n",
            "Batch Training Loss =  0.6961833238601685\n",
            "Batch Training Loss =  0.6915416121482849\n",
            "Batch Training Loss =  0.6966169476509094\n",
            "Batch Training Loss =  0.6962185502052307\n",
            "Batch Training Loss =  0.6982961297035217\n",
            "Batch Training Loss =  0.6906046867370605\n",
            "Batch Training Loss =  0.6969411373138428\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6933059096336365\n",
            "Batch Training Loss =  0.6928874850273132\n",
            "Batch Training Loss =  0.6947793364524841\n",
            "Batch Training Loss =  0.6946161389350891\n",
            "Batch Training Loss =  0.6929484605789185\n",
            "Batch Training Loss =  0.691081702709198\n",
            "Batch Training Loss =  0.6927229166030884\n",
            "Batch Training Loss =  0.6951937675476074\n",
            "Batch Training Loss =  0.6945993304252625\n",
            "Batch Training Loss =  0.6868917942047119\n",
            "Batch Training Loss =  0.6899983286857605\n",
            "Batch Training Loss =  0.6991174221038818\n",
            "Batch Training Loss =  0.6934162974357605\n",
            "Batch Training Loss =  0.6927038431167603\n",
            "Batch Training Loss =  0.699963390827179\n",
            "Batch Training Loss =  0.6903092265129089\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6936725378036499\n",
            "Batch Training Loss =  0.6895044445991516\n",
            "Batch Training Loss =  0.7075151801109314\n",
            "Batch Training Loss =  0.6946464776992798\n",
            "Batch Training Loss =  0.6885748505592346\n",
            "Batch Training Loss =  0.701530933380127\n",
            "Batch Training Loss =  0.6921951770782471\n",
            "Batch Training Loss =  0.6958035826683044\n",
            "Batch Training Loss =  0.695536732673645\n",
            "Batch Training Loss =  0.6993669867515564\n",
            "Batch Training Loss =  0.6923719048500061\n",
            "Batch Training Loss =  0.692364513874054\n",
            "Batch Training Loss =  0.6923948526382446\n",
            "Batch Training Loss =  0.6964767575263977\n",
            "Batch Training Loss =  0.6973052024841309\n",
            "Batch Training Loss =  0.6929762363433838\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.6918514966964722\n",
            "Batch Training Loss =  0.6932716369628906\n",
            "Batch Training Loss =  0.6981590986251831\n",
            "Batch Training Loss =  0.703031063079834\n",
            "Batch Training Loss =  0.6930293440818787\n",
            "Batch Training Loss =  0.6954513788223267\n",
            "Batch Training Loss =  0.6912822723388672\n",
            "Batch Training Loss =  0.6981030106544495\n",
            "Batch Training Loss =  0.6929709911346436\n",
            "Batch Training Loss =  0.6926869750022888\n",
            "Batch Training Loss =  0.6910213232040405\n",
            "Batch Training Loss =  0.70051109790802\n",
            "Batch Training Loss =  0.6917173266410828\n",
            "Batch Training Loss =  0.6939405798912048\n",
            "Batch Training Loss =  0.6941758990287781\n",
            "Batch Training Loss =  0.6938098073005676\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.6923322081565857\n",
            "Batch Training Loss =  0.6923522353172302\n",
            "Batch Training Loss =  0.6956713795661926\n",
            "Batch Training Loss =  0.6921433210372925\n",
            "Batch Training Loss =  0.6937958598136902\n",
            "Batch Training Loss =  0.6942874789237976\n",
            "Batch Training Loss =  0.7026557326316833\n",
            "Batch Training Loss =  0.689952552318573\n",
            "Batch Training Loss =  0.6913500428199768\n",
            "Batch Training Loss =  0.6962980628013611\n",
            "Batch Training Loss =  0.6926302313804626\n",
            "Batch Training Loss =  0.689318060874939\n",
            "Batch Training Loss =  0.687430739402771\n",
            "Batch Training Loss =  0.7122589945793152\n",
            "Batch Training Loss =  0.7021135091781616\n",
            "Batch Training Loss =  0.693636417388916\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.6930617690086365\n",
            "Batch Training Loss =  0.694587767124176\n",
            "Batch Training Loss =  0.7004022002220154\n",
            "Batch Training Loss =  0.6884113550186157\n",
            "Batch Training Loss =  0.7025778889656067\n",
            "Batch Training Loss =  0.6928024291992188\n",
            "Batch Training Loss =  0.6949986219406128\n",
            "Batch Training Loss =  0.6929488182067871\n",
            "Batch Training Loss =  0.6932671070098877\n",
            "Batch Training Loss =  0.6926931142807007\n",
            "Batch Training Loss =  0.699606716632843\n",
            "Batch Training Loss =  0.6944352984428406\n",
            "Batch Training Loss =  0.6931098103523254\n",
            "Batch Training Loss =  0.6923978924751282\n",
            "Batch Training Loss =  0.6986691951751709\n",
            "Batch Training Loss =  0.6984835863113403\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.6956451535224915\n",
            "Batch Training Loss =  0.7054920792579651\n",
            "Batch Training Loss =  0.6929883360862732\n",
            "Batch Training Loss =  0.6940323710441589\n",
            "Batch Training Loss =  0.6919246912002563\n",
            "Batch Training Loss =  0.6915932297706604\n",
            "Batch Training Loss =  0.6948288083076477\n",
            "Batch Training Loss =  0.6930533647537231\n",
            "Batch Training Loss =  0.6933618783950806\n",
            "Batch Training Loss =  0.6935396790504456\n",
            "Batch Training Loss =  0.687916100025177\n",
            "Batch Training Loss =  0.7011844515800476\n",
            "Batch Training Loss =  0.6970449686050415\n",
            "Batch Training Loss =  0.7060666680335999\n",
            "Batch Training Loss =  0.687580406665802\n",
            "Batch Training Loss =  0.701770544052124\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6936153173446655\n",
            "Batch Training Loss =  0.6953015923500061\n",
            "Batch Training Loss =  0.6927892565727234\n",
            "Batch Training Loss =  0.6948307156562805\n",
            "Batch Training Loss =  0.6931502819061279\n",
            "Batch Training Loss =  0.6930745840072632\n",
            "Batch Training Loss =  0.6964163780212402\n",
            "Batch Training Loss =  0.6929628252983093\n",
            "Batch Training Loss =  0.6877945065498352\n",
            "Batch Training Loss =  0.7133423686027527\n",
            "Batch Training Loss =  0.6931481957435608\n",
            "Batch Training Loss =  0.6931393146514893\n",
            "Batch Training Loss =  0.6927934288978577\n",
            "Batch Training Loss =  0.6938732862472534\n",
            "Batch Training Loss =  0.6930901408195496\n",
            "Batch Training Loss =  0.6927698254585266\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6906247138977051\n",
            "Batch Training Loss =  0.6927096843719482\n",
            "Batch Training Loss =  0.6939528584480286\n",
            "Batch Training Loss =  0.6944250464439392\n",
            "Batch Training Loss =  0.6914395689964294\n",
            "Batch Training Loss =  0.688839852809906\n",
            "Batch Training Loss =  0.7002151012420654\n",
            "Batch Training Loss =  0.6931789517402649\n",
            "Batch Training Loss =  0.6934370994567871\n",
            "Batch Training Loss =  0.6984604001045227\n",
            "Batch Training Loss =  0.6932212710380554\n",
            "Batch Training Loss =  0.6932998895645142\n",
            "Batch Training Loss =  0.6955941915512085\n",
            "Batch Training Loss =  0.6957088708877563\n",
            "Batch Training Loss =  0.6921552419662476\n",
            "Batch Training Loss =  0.7046868801116943\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6954953074455261\n",
            "Batch Training Loss =  0.6975594162940979\n",
            "Batch Training Loss =  0.694410502910614\n",
            "Batch Training Loss =  0.6947715878486633\n",
            "Batch Training Loss =  0.6878149509429932\n",
            "Batch Training Loss =  0.6882768273353577\n",
            "Batch Training Loss =  0.7004832625389099\n",
            "Batch Training Loss =  0.6933217644691467\n",
            "Batch Training Loss =  0.6941120028495789\n",
            "Batch Training Loss =  0.6890425682067871\n",
            "Batch Training Loss =  0.6913723945617676\n",
            "Batch Training Loss =  0.6964191198348999\n",
            "Batch Training Loss =  0.693159282207489\n",
            "Batch Training Loss =  0.693324863910675\n",
            "Batch Training Loss =  0.7032579183578491\n",
            "Batch Training Loss =  0.6965875029563904\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.6931564807891846\n",
            "Batch Training Loss =  0.6932523846626282\n",
            "Batch Training Loss =  0.6947237253189087\n",
            "Batch Training Loss =  0.6931111812591553\n",
            "Batch Training Loss =  0.6921461224555969\n",
            "Batch Training Loss =  0.6966869831085205\n",
            "Batch Training Loss =  0.6931552290916443\n",
            "Batch Training Loss =  0.6930511593818665\n",
            "Batch Training Loss =  0.6915397644042969\n",
            "Batch Training Loss =  0.6915068626403809\n",
            "Batch Training Loss =  0.6932032704353333\n",
            "Batch Training Loss =  0.6947757005691528\n",
            "Batch Training Loss =  0.6892473101615906\n",
            "Batch Training Loss =  0.713351309299469\n",
            "Batch Training Loss =  0.6984961032867432\n",
            "Batch Training Loss =  0.6954334378242493\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6910719871520996\n",
            "Batch Training Loss =  0.6844300031661987\n",
            "Batch Training Loss =  0.7089360952377319\n",
            "Batch Training Loss =  0.6933713555335999\n",
            "Batch Training Loss =  0.6940537095069885\n",
            "Batch Training Loss =  0.6923836469650269\n",
            "Batch Training Loss =  0.6964840888977051\n",
            "Batch Training Loss =  0.692400336265564\n",
            "Batch Training Loss =  0.6952999234199524\n",
            "Batch Training Loss =  0.6920951008796692\n",
            "Batch Training Loss =  0.698905348777771\n",
            "Batch Training Loss =  0.6889837384223938\n",
            "Batch Training Loss =  0.7035714983940125\n",
            "Batch Training Loss =  0.6939321160316467\n",
            "Batch Training Loss =  0.6913144588470459\n",
            "Batch Training Loss =  0.6882780194282532\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.7019832134246826\n",
            "Batch Training Loss =  0.6939934492111206\n",
            "Batch Training Loss =  0.6917779445648193\n",
            "Batch Training Loss =  0.6935394406318665\n",
            "Batch Training Loss =  0.6945489645004272\n",
            "Batch Training Loss =  0.6910441517829895\n",
            "Batch Training Loss =  0.6993964910507202\n",
            "Batch Training Loss =  0.6947711110115051\n",
            "Batch Training Loss =  0.6958151459693909\n",
            "Batch Training Loss =  0.694517970085144\n",
            "Batch Training Loss =  0.6963310241699219\n",
            "Batch Training Loss =  0.697704553604126\n",
            "Batch Training Loss =  0.6918326020240784\n",
            "Batch Training Loss =  0.6967874765396118\n",
            "Batch Training Loss =  0.6930059790611267\n",
            "Batch Training Loss =  0.6950585246086121\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.689673900604248\n",
            "Batch Training Loss =  0.6956026554107666\n",
            "Batch Training Loss =  0.6916824579238892\n",
            "Batch Training Loss =  0.6942198276519775\n",
            "Batch Training Loss =  0.692130982875824\n",
            "Batch Training Loss =  0.6958895921707153\n",
            "Batch Training Loss =  0.6930351853370667\n",
            "Batch Training Loss =  0.6937001943588257\n",
            "Batch Training Loss =  0.6932814121246338\n",
            "Batch Training Loss =  0.6921753883361816\n",
            "Batch Training Loss =  0.7206116318702698\n",
            "Batch Training Loss =  0.6931191086769104\n",
            "Batch Training Loss =  0.6888425350189209\n",
            "Batch Training Loss =  0.6914318203926086\n",
            "Batch Training Loss =  0.695540189743042\n",
            "Batch Training Loss =  0.6929378509521484\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6912134289741516\n",
            "Batch Training Loss =  0.6936127543449402\n",
            "Batch Training Loss =  0.6924319267272949\n",
            "Batch Training Loss =  0.693541944026947\n",
            "Batch Training Loss =  0.6933513879776001\n",
            "Batch Training Loss =  0.6922786831855774\n",
            "Batch Training Loss =  0.6901037693023682\n",
            "Batch Training Loss =  0.6866499185562134\n",
            "Batch Training Loss =  0.7039951086044312\n",
            "Batch Training Loss =  0.690520703792572\n",
            "Batch Training Loss =  0.6985535621643066\n",
            "Batch Training Loss =  0.6930122375488281\n",
            "Batch Training Loss =  0.6956673264503479\n",
            "Batch Training Loss =  0.7021975517272949\n",
            "Batch Training Loss =  0.6978126764297485\n",
            "Batch Training Loss =  0.6924116611480713\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6862158179283142\n",
            "Batch Training Loss =  0.6859298944473267\n",
            "Batch Training Loss =  0.7132014632225037\n",
            "Batch Training Loss =  0.6903688311576843\n",
            "Batch Training Loss =  0.7013121843338013\n",
            "Batch Training Loss =  0.6888821125030518\n",
            "Batch Training Loss =  0.6972718834877014\n",
            "Batch Training Loss =  0.6934581995010376\n",
            "Batch Training Loss =  0.6932958960533142\n",
            "Batch Training Loss =  0.6933413147926331\n",
            "Batch Training Loss =  0.6922667622566223\n",
            "Batch Training Loss =  0.6950851678848267\n",
            "Batch Training Loss =  0.6918246746063232\n",
            "Batch Training Loss =  0.6923627257347107\n",
            "Batch Training Loss =  0.6889967918395996\n",
            "Batch Training Loss =  0.702762246131897\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6935908794403076\n",
            "Batch Training Loss =  0.6921138763427734\n",
            "Batch Training Loss =  0.7023593187332153\n",
            "Batch Training Loss =  0.6929908990859985\n",
            "Batch Training Loss =  0.6924425363540649\n",
            "Batch Training Loss =  0.690327525138855\n",
            "Batch Training Loss =  0.6889383792877197\n",
            "Batch Training Loss =  0.6968259215354919\n",
            "Batch Training Loss =  0.6934117674827576\n",
            "Batch Training Loss =  0.6933076977729797\n",
            "Batch Training Loss =  0.6936553120613098\n",
            "Batch Training Loss =  0.6896621584892273\n",
            "Batch Training Loss =  0.6993402242660522\n",
            "Batch Training Loss =  0.6924402713775635\n",
            "Batch Training Loss =  0.7043102383613586\n",
            "Batch Training Loss =  0.6930782198905945\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6956056952476501\n",
            "Batch Training Loss =  0.6904155611991882\n",
            "Batch Training Loss =  0.6991109251976013\n",
            "Batch Training Loss =  0.6930921077728271\n",
            "Batch Training Loss =  0.6934242844581604\n",
            "Batch Training Loss =  0.6935738325119019\n",
            "Batch Training Loss =  0.6924193501472473\n",
            "Batch Training Loss =  0.6963901519775391\n",
            "Batch Training Loss =  0.688200056552887\n",
            "Batch Training Loss =  0.6866481900215149\n",
            "Batch Training Loss =  0.6984375715255737\n",
            "Batch Training Loss =  0.6985552310943604\n",
            "Batch Training Loss =  0.7298998832702637\n",
            "Batch Training Loss =  0.6925541758537292\n",
            "Batch Training Loss =  0.6955544352531433\n",
            "Batch Training Loss =  0.6968891024589539\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.6996057629585266\n",
            "Batch Training Loss =  0.6901905536651611\n",
            "Batch Training Loss =  0.6924976110458374\n",
            "Batch Training Loss =  0.6960759162902832\n",
            "Batch Training Loss =  0.6933242678642273\n",
            "Batch Training Loss =  0.6936149597167969\n",
            "Batch Training Loss =  0.6946468353271484\n",
            "Batch Training Loss =  0.6932685375213623\n",
            "Batch Training Loss =  0.6955111026763916\n",
            "Batch Training Loss =  0.694317638874054\n",
            "Batch Training Loss =  0.6913741230964661\n",
            "Batch Training Loss =  0.6944128274917603\n",
            "Batch Training Loss =  0.6932613849639893\n",
            "Batch Training Loss =  0.6934294104576111\n",
            "Batch Training Loss =  0.6987122893333435\n",
            "Batch Training Loss =  0.7045972943305969\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6965305805206299\n",
            "Batch Training Loss =  0.6913757920265198\n",
            "Batch Training Loss =  0.7005944848060608\n",
            "Batch Training Loss =  0.7026063799858093\n",
            "Batch Training Loss =  0.6934940218925476\n",
            "Batch Training Loss =  0.6971172094345093\n",
            "Batch Training Loss =  0.701302170753479\n",
            "Batch Training Loss =  0.6936368346214294\n",
            "Batch Training Loss =  0.6922518014907837\n",
            "Batch Training Loss =  0.6924288868904114\n",
            "Batch Training Loss =  0.6966599822044373\n",
            "Batch Training Loss =  0.6894819736480713\n",
            "Batch Training Loss =  0.7009731531143188\n",
            "Batch Training Loss =  0.6927898526191711\n",
            "Batch Training Loss =  0.6968441009521484\n",
            "Batch Training Loss =  0.689921498298645\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.7037761807441711\n",
            "Batch Training Loss =  0.696516215801239\n",
            "Batch Training Loss =  0.6931576728820801\n",
            "Batch Training Loss =  0.6929556131362915\n",
            "Batch Training Loss =  0.6942158341407776\n",
            "Batch Training Loss =  0.6973751187324524\n",
            "Batch Training Loss =  0.6843007802963257\n",
            "Batch Training Loss =  0.6974654197692871\n",
            "Batch Training Loss =  0.6986328959465027\n",
            "Batch Training Loss =  0.685045063495636\n",
            "Batch Training Loss =  0.6859831213951111\n",
            "Batch Training Loss =  0.6961118578910828\n",
            "Batch Training Loss =  0.6955519318580627\n",
            "Batch Training Loss =  0.6919037699699402\n",
            "Batch Training Loss =  0.6948896646499634\n",
            "Batch Training Loss =  0.6944491863250732\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.6931561827659607\n",
            "Batch Training Loss =  0.6922092437744141\n",
            "Batch Training Loss =  0.6934896111488342\n",
            "Batch Training Loss =  0.6945193409919739\n",
            "Batch Training Loss =  0.6939553618431091\n",
            "Batch Training Loss =  0.6903339624404907\n",
            "Batch Training Loss =  0.6873023509979248\n",
            "Batch Training Loss =  0.7108537554740906\n",
            "Batch Training Loss =  0.7097824811935425\n",
            "Batch Training Loss =  0.6863853335380554\n",
            "Batch Training Loss =  0.7196118831634521\n",
            "Batch Training Loss =  0.6881446242332458\n",
            "Batch Training Loss =  0.6997010111808777\n",
            "Batch Training Loss =  0.6931573748588562\n",
            "Batch Training Loss =  0.6929599046707153\n",
            "Batch Training Loss =  0.6885831356048584\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.697101891040802\n",
            "Batch Training Loss =  0.6913858652114868\n",
            "Batch Training Loss =  0.696622908115387\n",
            "Batch Training Loss =  0.6984875202178955\n",
            "Batch Training Loss =  0.688169538974762\n",
            "Batch Training Loss =  0.6977332830429077\n",
            "Batch Training Loss =  0.6945813894271851\n",
            "Batch Training Loss =  0.6997196078300476\n",
            "Batch Training Loss =  0.6969852447509766\n",
            "Batch Training Loss =  0.6923096179962158\n",
            "Batch Training Loss =  0.6874828934669495\n",
            "Batch Training Loss =  0.6881387233734131\n",
            "Batch Training Loss =  0.6995805501937866\n",
            "Batch Training Loss =  0.6937408447265625\n",
            "Batch Training Loss =  0.6982584595680237\n",
            "Batch Training Loss =  0.6930797696113586\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6923985481262207\n",
            "Batch Training Loss =  0.6993756294250488\n",
            "Batch Training Loss =  0.6983367204666138\n",
            "Batch Training Loss =  0.6930935382843018\n",
            "Batch Training Loss =  0.6960268616676331\n",
            "Batch Training Loss =  0.6923885941505432\n",
            "Batch Training Loss =  0.6953324675559998\n",
            "Batch Training Loss =  0.6971902251243591\n",
            "Batch Training Loss =  0.7016586065292358\n",
            "Batch Training Loss =  0.6929664611816406\n",
            "Batch Training Loss =  0.693805992603302\n",
            "Batch Training Loss =  0.6930822134017944\n",
            "Batch Training Loss =  0.6936512589454651\n",
            "Batch Training Loss =  0.6922854781150818\n",
            "Batch Training Loss =  0.694459855556488\n",
            "Batch Training Loss =  0.6931678056716919\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6929471492767334\n",
            "Batch Training Loss =  0.6955081820487976\n",
            "Batch Training Loss =  0.6861422657966614\n",
            "Batch Training Loss =  0.6989489793777466\n",
            "Batch Training Loss =  0.6939719319343567\n",
            "Batch Training Loss =  0.6920663714408875\n",
            "Batch Training Loss =  0.6994721293449402\n",
            "Batch Training Loss =  0.6961005330085754\n",
            "Batch Training Loss =  0.6930453777313232\n",
            "Batch Training Loss =  0.6939244270324707\n",
            "Batch Training Loss =  0.6966724991798401\n",
            "Batch Training Loss =  0.6830486059188843\n",
            "Batch Training Loss =  0.7085578441619873\n",
            "Batch Training Loss =  0.6944627165794373\n",
            "Batch Training Loss =  0.6977065801620483\n",
            "Batch Training Loss =  0.694571852684021\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.6886852979660034\n",
            "Batch Training Loss =  0.7017742991447449\n",
            "Batch Training Loss =  0.6911706328392029\n",
            "Batch Training Loss =  0.6939733624458313\n",
            "Batch Training Loss =  0.6941797137260437\n",
            "Batch Training Loss =  0.6933019161224365\n",
            "Batch Training Loss =  0.6941789388656616\n",
            "Batch Training Loss =  0.6985794901847839\n",
            "Batch Training Loss =  0.6984732151031494\n",
            "Batch Training Loss =  0.693173348903656\n",
            "Batch Training Loss =  0.6949247717857361\n",
            "Batch Training Loss =  0.696936845779419\n",
            "Batch Training Loss =  0.6887675523757935\n",
            "Batch Training Loss =  0.6959719061851501\n",
            "Batch Training Loss =  0.6970893144607544\n",
            "Batch Training Loss =  0.6890414357185364\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.7094354033470154\n",
            "Batch Training Loss =  0.6958206295967102\n",
            "Batch Training Loss =  0.6930318474769592\n",
            "Batch Training Loss =  0.6929784417152405\n",
            "Batch Training Loss =  0.6913321614265442\n",
            "Batch Training Loss =  0.6884805560112\n",
            "Batch Training Loss =  0.6986498832702637\n",
            "Batch Training Loss =  0.6933586597442627\n",
            "Batch Training Loss =  0.7008621096611023\n",
            "Batch Training Loss =  0.6955904960632324\n",
            "Batch Training Loss =  0.6941685676574707\n",
            "Batch Training Loss =  0.6924099922180176\n",
            "Batch Training Loss =  0.6985141634941101\n",
            "Batch Training Loss =  0.6937358379364014\n",
            "Batch Training Loss =  0.6940972805023193\n",
            "Batch Training Loss =  0.6931805610656738\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6925342679023743\n",
            "Batch Training Loss =  0.6980915069580078\n",
            "Batch Training Loss =  0.6974424719810486\n",
            "Batch Training Loss =  0.6933102607727051\n",
            "Batch Training Loss =  0.6923084855079651\n",
            "Batch Training Loss =  0.6946150064468384\n",
            "Batch Training Loss =  0.6932658553123474\n",
            "Batch Training Loss =  0.6934231519699097\n",
            "Batch Training Loss =  0.6906682848930359\n",
            "Batch Training Loss =  0.6925243139266968\n",
            "Batch Training Loss =  0.6983672976493835\n",
            "Batch Training Loss =  0.6982569098472595\n",
            "Batch Training Loss =  0.6924536228179932\n",
            "Batch Training Loss =  0.6940812468528748\n",
            "Batch Training Loss =  0.6930105090141296\n",
            "Batch Training Loss =  0.7004539370536804\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.6873051524162292\n",
            "Batch Training Loss =  0.7012012004852295\n",
            "Batch Training Loss =  0.6940920948982239\n",
            "Batch Training Loss =  0.686815083026886\n",
            "Batch Training Loss =  0.7133459448814392\n",
            "Batch Training Loss =  0.6899577379226685\n",
            "Batch Training Loss =  0.6913490295410156\n",
            "Batch Training Loss =  0.7011688351631165\n",
            "Batch Training Loss =  0.6935708522796631\n",
            "Batch Training Loss =  0.6940343379974365\n",
            "Batch Training Loss =  0.6933497786521912\n",
            "Batch Training Loss =  0.6953547596931458\n",
            "Batch Training Loss =  0.6927717328071594\n",
            "Batch Training Loss =  0.6973222494125366\n",
            "Batch Training Loss =  0.6946288347244263\n",
            "Batch Training Loss =  0.6936851739883423\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.6936125755310059\n",
            "Batch Training Loss =  0.6931599378585815\n",
            "Batch Training Loss =  0.6932695508003235\n",
            "Batch Training Loss =  0.6907650828361511\n",
            "Batch Training Loss =  0.6934545040130615\n",
            "Batch Training Loss =  0.6934744119644165\n",
            "Batch Training Loss =  0.6927158236503601\n",
            "Batch Training Loss =  0.6962532997131348\n",
            "Batch Training Loss =  0.6863334774971008\n",
            "Batch Training Loss =  0.6885474324226379\n",
            "Batch Training Loss =  0.6971100568771362\n",
            "Batch Training Loss =  0.6961917877197266\n",
            "Batch Training Loss =  0.6915098428726196\n",
            "Batch Training Loss =  0.6966096758842468\n",
            "Batch Training Loss =  0.6912301778793335\n",
            "Batch Training Loss =  0.6952689290046692\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.6917715668678284\n",
            "Batch Training Loss =  0.6868585348129272\n",
            "Batch Training Loss =  0.7012891173362732\n",
            "Batch Training Loss =  0.6936776041984558\n",
            "Batch Training Loss =  0.6935256719589233\n",
            "Batch Training Loss =  0.6940070986747742\n",
            "Batch Training Loss =  0.6999787092208862\n",
            "Batch Training Loss =  0.6932778358459473\n",
            "Batch Training Loss =  0.6967513561248779\n",
            "Batch Training Loss =  0.6913502216339111\n",
            "Batch Training Loss =  0.6865177154541016\n",
            "Batch Training Loss =  0.697623610496521\n",
            "Batch Training Loss =  0.6964095830917358\n",
            "Batch Training Loss =  0.6906906366348267\n",
            "Batch Training Loss =  0.698716938495636\n",
            "Batch Training Loss =  0.6968352794647217\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.7013854384422302\n",
            "Batch Training Loss =  0.6923863887786865\n",
            "Batch Training Loss =  0.6900559067726135\n",
            "Batch Training Loss =  0.7010173797607422\n",
            "Batch Training Loss =  0.6910169720649719\n",
            "Batch Training Loss =  0.688730776309967\n",
            "Batch Training Loss =  0.7039543390274048\n",
            "Batch Training Loss =  0.6931275725364685\n",
            "Batch Training Loss =  0.693382978439331\n",
            "Batch Training Loss =  0.6927776336669922\n",
            "Batch Training Loss =  0.7006876468658447\n",
            "Batch Training Loss =  0.700187623500824\n",
            "Batch Training Loss =  0.6953820586204529\n",
            "Batch Training Loss =  0.6932327151298523\n",
            "Batch Training Loss =  0.6929522752761841\n",
            "Batch Training Loss =  0.6912561058998108\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6952638030052185\n",
            "Batch Training Loss =  0.6960787773132324\n",
            "Batch Training Loss =  0.6913543939590454\n",
            "Batch Training Loss =  0.6894329786300659\n",
            "Batch Training Loss =  0.7107986211776733\n",
            "Batch Training Loss =  0.7082414031028748\n",
            "Batch Training Loss =  0.6968828439712524\n",
            "Batch Training Loss =  0.6714505553245544\n",
            "Batch Training Loss =  0.7119454145431519\n",
            "Batch Training Loss =  0.6897438168525696\n",
            "Batch Training Loss =  0.6963952779769897\n",
            "Batch Training Loss =  0.6939041018486023\n",
            "Batch Training Loss =  0.6930969953536987\n",
            "Batch Training Loss =  0.6929479837417603\n",
            "Batch Training Loss =  0.6936084032058716\n",
            "Batch Training Loss =  0.6934657096862793\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6941220760345459\n",
            "Batch Training Loss =  0.6932064294815063\n",
            "Batch Training Loss =  0.6945123076438904\n",
            "Batch Training Loss =  0.6934384107589722\n",
            "Batch Training Loss =  0.6940423846244812\n",
            "Batch Training Loss =  0.6959748268127441\n",
            "Batch Training Loss =  0.6928792595863342\n",
            "Batch Training Loss =  0.6933937668800354\n",
            "Batch Training Loss =  0.6942359805107117\n",
            "Batch Training Loss =  0.7117860317230225\n",
            "Batch Training Loss =  0.7021938562393188\n",
            "Batch Training Loss =  0.6936305165290833\n",
            "Batch Training Loss =  0.6935519576072693\n",
            "Batch Training Loss =  0.6892659068107605\n",
            "Batch Training Loss =  0.6914119124412537\n",
            "Batch Training Loss =  0.6924131512641907\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6959055066108704\n",
            "Batch Training Loss =  0.6941081285476685\n",
            "Batch Training Loss =  0.6943833827972412\n",
            "Batch Training Loss =  0.6945006847381592\n",
            "Batch Training Loss =  0.6931037306785583\n",
            "Batch Training Loss =  0.6911860704421997\n",
            "Batch Training Loss =  0.6864993572235107\n",
            "Batch Training Loss =  0.7033127546310425\n",
            "Batch Training Loss =  0.6928321123123169\n",
            "Batch Training Loss =  0.6903979778289795\n",
            "Batch Training Loss =  0.6973021626472473\n",
            "Batch Training Loss =  0.6924958825111389\n",
            "Batch Training Loss =  0.6876605153083801\n",
            "Batch Training Loss =  0.7001498937606812\n",
            "Batch Training Loss =  0.691331684589386\n",
            "Batch Training Loss =  0.6980811953544617\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.6958642601966858\n",
            "Batch Training Loss =  0.7073585987091064\n",
            "Batch Training Loss =  0.6963133811950684\n",
            "Batch Training Loss =  0.7002688646316528\n",
            "Batch Training Loss =  0.6925304532051086\n",
            "Batch Training Loss =  0.6940168738365173\n",
            "Batch Training Loss =  0.6931918859481812\n",
            "Batch Training Loss =  0.6916928291320801\n",
            "Batch Training Loss =  0.6949155330657959\n",
            "Batch Training Loss =  0.6972510814666748\n",
            "Batch Training Loss =  0.6931374073028564\n",
            "Batch Training Loss =  0.6907626390457153\n",
            "Batch Training Loss =  0.6833796501159668\n",
            "Batch Training Loss =  0.716789722442627\n",
            "Batch Training Loss =  0.6979827880859375\n",
            "Batch Training Loss =  0.6950409412384033\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6929494738578796\n",
            "Batch Training Loss =  0.6929528713226318\n",
            "Batch Training Loss =  0.6938596963882446\n",
            "Batch Training Loss =  0.6938871741294861\n",
            "Batch Training Loss =  0.6917301416397095\n",
            "Batch Training Loss =  0.6935452222824097\n",
            "Batch Training Loss =  0.6950802803039551\n",
            "Batch Training Loss =  0.6947152018547058\n",
            "Batch Training Loss =  0.6948230266571045\n",
            "Batch Training Loss =  0.6924918293952942\n",
            "Batch Training Loss =  0.6924331188201904\n",
            "Batch Training Loss =  0.6941026449203491\n",
            "Batch Training Loss =  0.6931169033050537\n",
            "Batch Training Loss =  0.6942493319511414\n",
            "Batch Training Loss =  0.6923705339431763\n",
            "Batch Training Loss =  0.688957154750824\n",
            "Validation Loss in this epoch is 0.701\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.7075942158699036\n",
            "Batch Training Loss =  0.6914470195770264\n",
            "Batch Training Loss =  0.7002323865890503\n",
            "Batch Training Loss =  0.7027223110198975\n",
            "Batch Training Loss =  0.696662962436676\n",
            "Batch Training Loss =  0.693565845489502\n",
            "Batch Training Loss =  0.696286678314209\n",
            "Batch Training Loss =  0.6927335262298584\n",
            "Batch Training Loss =  0.694012463092804\n",
            "Batch Training Loss =  0.6944746375083923\n",
            "Batch Training Loss =  0.693291425704956\n",
            "Batch Training Loss =  0.6958215832710266\n",
            "Batch Training Loss =  0.6889246106147766\n",
            "Batch Training Loss =  0.6914275884628296\n",
            "Batch Training Loss =  0.6872251629829407\n",
            "Batch Training Loss =  0.6916703581809998\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.6835455894470215\n",
            "Batch Training Loss =  0.6999487280845642\n",
            "Batch Training Loss =  0.6867111921310425\n",
            "Batch Training Loss =  0.7036742568016052\n",
            "Batch Training Loss =  0.6945770382881165\n",
            "Batch Training Loss =  0.6969625949859619\n",
            "Batch Training Loss =  0.6936019659042358\n",
            "Batch Training Loss =  0.6928521990776062\n",
            "Batch Training Loss =  0.694275438785553\n",
            "Batch Training Loss =  0.6930835843086243\n",
            "Batch Training Loss =  0.7033780813217163\n",
            "Batch Training Loss =  0.6996858716011047\n",
            "Batch Training Loss =  0.6958029270172119\n",
            "Batch Training Loss =  0.6937339901924133\n",
            "Batch Training Loss =  0.6935654282569885\n",
            "Batch Training Loss =  0.6953114867210388\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6931580305099487\n",
            "Batch Training Loss =  0.6931760907173157\n",
            "Batch Training Loss =  0.6933846473693848\n",
            "Batch Training Loss =  0.6943126916885376\n",
            "Batch Training Loss =  0.7055190205574036\n",
            "Batch Training Loss =  0.6989341974258423\n",
            "Batch Training Loss =  0.6913468241691589\n",
            "Batch Training Loss =  0.6952411532402039\n",
            "Batch Training Loss =  0.6932433247566223\n",
            "Batch Training Loss =  0.6926631927490234\n",
            "Batch Training Loss =  0.6943310499191284\n",
            "Batch Training Loss =  0.6931431293487549\n",
            "Batch Training Loss =  0.6927953362464905\n",
            "Batch Training Loss =  0.6961743831634521\n",
            "Batch Training Loss =  0.6934940814971924\n",
            "Batch Training Loss =  0.6971172094345093\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6826387047767639\n",
            "Batch Training Loss =  0.6883056163787842\n",
            "Batch Training Loss =  0.7002665996551514\n",
            "Batch Training Loss =  0.6935166716575623\n",
            "Batch Training Loss =  0.6935457587242126\n",
            "Batch Training Loss =  0.693691611289978\n",
            "Batch Training Loss =  0.6935241222381592\n",
            "Batch Training Loss =  0.693840503692627\n",
            "Batch Training Loss =  0.6968533396720886\n",
            "Batch Training Loss =  0.6929853558540344\n",
            "Batch Training Loss =  0.6940655708312988\n",
            "Batch Training Loss =  0.6935393810272217\n",
            "Batch Training Loss =  0.693390429019928\n",
            "Batch Training Loss =  0.6947781443595886\n",
            "Batch Training Loss =  0.698573887348175\n",
            "Batch Training Loss =  0.6985030174255371\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6937029957771301\n",
            "Batch Training Loss =  0.6936088800430298\n",
            "Batch Training Loss =  0.6942789554595947\n",
            "Batch Training Loss =  0.7120358943939209\n",
            "Batch Training Loss =  0.6897038817405701\n",
            "Batch Training Loss =  0.6790010333061218\n",
            "Batch Training Loss =  0.69669508934021\n",
            "Batch Training Loss =  0.6973916888237\n",
            "Batch Training Loss =  0.6934045553207397\n",
            "Batch Training Loss =  0.701113224029541\n",
            "Batch Training Loss =  0.692183256149292\n",
            "Batch Training Loss =  0.6980737447738647\n",
            "Batch Training Loss =  0.6973170638084412\n",
            "Batch Training Loss =  0.6935665011405945\n",
            "Batch Training Loss =  0.6934885382652283\n",
            "Batch Training Loss =  0.6924147009849548\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6901319622993469\n",
            "Batch Training Loss =  0.7009801268577576\n",
            "Batch Training Loss =  0.6929474472999573\n",
            "Batch Training Loss =  0.6926395893096924\n",
            "Batch Training Loss =  0.6979073882102966\n",
            "Batch Training Loss =  0.7064614295959473\n",
            "Batch Training Loss =  0.6958723664283752\n",
            "Batch Training Loss =  0.6952815055847168\n",
            "Batch Training Loss =  0.6905890703201294\n",
            "Batch Training Loss =  0.6913692355155945\n",
            "Batch Training Loss =  0.6886844635009766\n",
            "Batch Training Loss =  0.6973455548286438\n",
            "Batch Training Loss =  0.6934658885002136\n",
            "Batch Training Loss =  0.6939277052879333\n",
            "Batch Training Loss =  0.6983199119567871\n",
            "Batch Training Loss =  0.6924735307693481\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.6957427263259888\n",
            "Batch Training Loss =  0.6933446526527405\n",
            "Batch Training Loss =  0.6930457949638367\n",
            "Batch Training Loss =  0.6925094127655029\n",
            "Batch Training Loss =  0.6957217454910278\n",
            "Batch Training Loss =  0.6925485134124756\n",
            "Batch Training Loss =  0.6971220374107361\n",
            "Batch Training Loss =  0.6888604760169983\n",
            "Batch Training Loss =  0.6996272802352905\n",
            "Batch Training Loss =  0.6924439072608948\n",
            "Batch Training Loss =  0.6966309547424316\n",
            "Batch Training Loss =  0.690207839012146\n",
            "Batch Training Loss =  0.7090839147567749\n",
            "Batch Training Loss =  0.697394847869873\n",
            "Batch Training Loss =  0.701511025428772\n",
            "Batch Training Loss =  0.693551778793335\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6871452331542969\n",
            "Batch Training Loss =  0.7064273953437805\n",
            "Batch Training Loss =  0.6940717101097107\n",
            "Batch Training Loss =  0.69440096616745\n",
            "Batch Training Loss =  0.6944975256919861\n",
            "Batch Training Loss =  0.6932471394538879\n",
            "Batch Training Loss =  0.6969760656356812\n",
            "Batch Training Loss =  0.6925479769706726\n",
            "Batch Training Loss =  0.6863758563995361\n",
            "Batch Training Loss =  0.7051293253898621\n",
            "Batch Training Loss =  0.69197678565979\n",
            "Batch Training Loss =  0.6950519680976868\n",
            "Batch Training Loss =  0.6930138468742371\n",
            "Batch Training Loss =  0.6954730749130249\n",
            "Batch Training Loss =  0.694101095199585\n",
            "Batch Training Loss =  0.6942827701568604\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.6959609985351562\n",
            "Batch Training Loss =  0.6963987946510315\n",
            "Batch Training Loss =  0.6840212941169739\n",
            "Batch Training Loss =  0.6963282227516174\n",
            "Batch Training Loss =  0.6988883018493652\n",
            "Batch Training Loss =  0.6961945295333862\n",
            "Batch Training Loss =  0.6939563751220703\n",
            "Batch Training Loss =  0.6927458047866821\n",
            "Batch Training Loss =  0.6996448636054993\n",
            "Batch Training Loss =  0.6925049424171448\n",
            "Batch Training Loss =  0.696835994720459\n",
            "Batch Training Loss =  0.6941366791725159\n",
            "Batch Training Loss =  0.6930117607116699\n",
            "Batch Training Loss =  0.6903843879699707\n",
            "Batch Training Loss =  0.6979754567146301\n",
            "Batch Training Loss =  0.6927001476287842\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.6924746632575989\n",
            "Batch Training Loss =  0.6935173273086548\n",
            "Batch Training Loss =  0.6941604614257812\n",
            "Batch Training Loss =  0.6971042156219482\n",
            "Batch Training Loss =  0.6932590007781982\n",
            "Batch Training Loss =  0.6925637125968933\n",
            "Batch Training Loss =  0.6939939856529236\n",
            "Batch Training Loss =  0.6935582160949707\n",
            "Batch Training Loss =  0.6971307992935181\n",
            "Batch Training Loss =  0.6892032623291016\n",
            "Batch Training Loss =  0.705828845500946\n",
            "Batch Training Loss =  0.6910760402679443\n",
            "Batch Training Loss =  0.6970548033714294\n",
            "Batch Training Loss =  0.6926940679550171\n",
            "Batch Training Loss =  0.7028810381889343\n",
            "Batch Training Loss =  0.6910781264305115\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.7019101977348328\n",
            "Batch Training Loss =  0.6950528025627136\n",
            "Batch Training Loss =  0.6941478848457336\n",
            "Batch Training Loss =  0.6921624541282654\n",
            "Batch Training Loss =  0.6861905455589294\n",
            "Batch Training Loss =  0.7108520269393921\n",
            "Batch Training Loss =  0.6952944397926331\n",
            "Batch Training Loss =  0.6984967589378357\n",
            "Batch Training Loss =  0.7018573880195618\n",
            "Batch Training Loss =  0.6944159865379333\n",
            "Batch Training Loss =  0.6934819221496582\n",
            "Batch Training Loss =  0.6935377717018127\n",
            "Batch Training Loss =  0.6963573694229126\n",
            "Batch Training Loss =  0.6840376257896423\n",
            "Batch Training Loss =  0.7098456025123596\n",
            "Batch Training Loss =  0.6929957866668701\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6932321786880493\n",
            "Batch Training Loss =  0.6926072835922241\n",
            "Batch Training Loss =  0.7099127173423767\n",
            "Batch Training Loss =  0.7198083400726318\n",
            "Batch Training Loss =  0.7086878418922424\n",
            "Batch Training Loss =  0.6930163502693176\n",
            "Batch Training Loss =  0.6939733028411865\n",
            "Batch Training Loss =  0.6919164061546326\n",
            "Batch Training Loss =  0.6900763511657715\n",
            "Batch Training Loss =  0.6888920068740845\n",
            "Batch Training Loss =  0.6900119781494141\n",
            "Batch Training Loss =  0.701040506362915\n",
            "Batch Training Loss =  0.6898378729820251\n",
            "Batch Training Loss =  0.6927815079689026\n",
            "Batch Training Loss =  0.6890740990638733\n",
            "Batch Training Loss =  0.6928660869598389\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6923485398292542\n",
            "Batch Training Loss =  0.6891212463378906\n",
            "Batch Training Loss =  0.6947298645973206\n",
            "Batch Training Loss =  0.694405734539032\n",
            "Batch Training Loss =  0.6931512951850891\n",
            "Batch Training Loss =  0.6927991509437561\n",
            "Batch Training Loss =  0.6961688995361328\n",
            "Batch Training Loss =  0.7019909620285034\n",
            "Batch Training Loss =  0.6913642287254333\n",
            "Batch Training Loss =  0.7006893754005432\n",
            "Batch Training Loss =  0.6879871487617493\n",
            "Batch Training Loss =  0.7007099986076355\n",
            "Batch Training Loss =  0.6924840807914734\n",
            "Batch Training Loss =  0.6986664533615112\n",
            "Batch Training Loss =  0.6914951205253601\n",
            "Batch Training Loss =  0.6980506777763367\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.6929638385772705\n",
            "Batch Training Loss =  0.6911757588386536\n",
            "Batch Training Loss =  0.6952793002128601\n",
            "Batch Training Loss =  0.6925544142723083\n",
            "Batch Training Loss =  0.6960719227790833\n",
            "Batch Training Loss =  0.6855927109718323\n",
            "Batch Training Loss =  0.7084968090057373\n",
            "Batch Training Loss =  0.6928074359893799\n",
            "Batch Training Loss =  0.6949239373207092\n",
            "Batch Training Loss =  0.6925743222236633\n",
            "Batch Training Loss =  0.6930333971977234\n",
            "Batch Training Loss =  0.6965378522872925\n",
            "Batch Training Loss =  0.7062457799911499\n",
            "Batch Training Loss =  0.6961947679519653\n",
            "Batch Training Loss =  0.6931631565093994\n",
            "Batch Training Loss =  0.6934545636177063\n",
            "Validation Loss in this epoch is 0.702\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6989359259605408\n",
            "Batch Training Loss =  0.6946484446525574\n",
            "Batch Training Loss =  0.6972473859786987\n",
            "Batch Training Loss =  0.689975917339325\n",
            "Batch Training Loss =  0.7012463212013245\n",
            "Batch Training Loss =  0.6961519718170166\n",
            "Batch Training Loss =  0.6966721415519714\n",
            "Batch Training Loss =  0.7029613256454468\n",
            "Batch Training Loss =  0.7076903581619263\n",
            "Batch Training Loss =  0.6954097747802734\n",
            "Batch Training Loss =  0.6941080689430237\n",
            "Batch Training Loss =  0.6921806335449219\n",
            "Batch Training Loss =  0.700265645980835\n",
            "Batch Training Loss =  0.6958955526351929\n",
            "Batch Training Loss =  0.6937377452850342\n",
            "Batch Training Loss =  0.6946219801902771\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6926429271697998\n",
            "Batch Training Loss =  0.6892753839492798\n",
            "Batch Training Loss =  0.7009155750274658\n",
            "Batch Training Loss =  0.6934152245521545\n",
            "Batch Training Loss =  0.6941365599632263\n",
            "Batch Training Loss =  0.683695912361145\n",
            "Batch Training Loss =  0.7060709595680237\n",
            "Batch Training Loss =  0.6945213079452515\n",
            "Batch Training Loss =  0.6923547387123108\n",
            "Batch Training Loss =  0.6892854571342468\n",
            "Batch Training Loss =  0.7042722105979919\n",
            "Batch Training Loss =  0.6961085796356201\n",
            "Batch Training Loss =  0.6957321763038635\n",
            "Batch Training Loss =  0.6922957897186279\n",
            "Batch Training Loss =  0.7100112438201904\n",
            "Batch Training Loss =  0.6982762813568115\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.6917798519134521\n",
            "Batch Training Loss =  0.6954817771911621\n",
            "Batch Training Loss =  0.6926868557929993\n",
            "Batch Training Loss =  0.6962718963623047\n",
            "Batch Training Loss =  0.6869487166404724\n",
            "Batch Training Loss =  0.6954575181007385\n",
            "Batch Training Loss =  0.6958348751068115\n",
            "Batch Training Loss =  0.6928973197937012\n",
            "Batch Training Loss =  0.6933901309967041\n",
            "Batch Training Loss =  0.694492518901825\n",
            "Batch Training Loss =  0.703823983669281\n",
            "Batch Training Loss =  0.6931822896003723\n",
            "Batch Training Loss =  0.6835759878158569\n",
            "Batch Training Loss =  0.7117096185684204\n",
            "Batch Training Loss =  0.6948003172874451\n",
            "Batch Training Loss =  0.7025701403617859\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.69562166929245\n",
            "Batch Training Loss =  0.6940006017684937\n",
            "Batch Training Loss =  0.6954715251922607\n",
            "Batch Training Loss =  0.6929474472999573\n",
            "Batch Training Loss =  0.6919851899147034\n",
            "Batch Training Loss =  0.6977400779724121\n",
            "Batch Training Loss =  0.6929686069488525\n",
            "Batch Training Loss =  0.6936677694320679\n",
            "Batch Training Loss =  0.6938032507896423\n",
            "Batch Training Loss =  0.7009890675544739\n",
            "Batch Training Loss =  0.6968196630477905\n",
            "Batch Training Loss =  0.7034223675727844\n",
            "Batch Training Loss =  0.6923757195472717\n",
            "Batch Training Loss =  0.6923626661300659\n",
            "Batch Training Loss =  0.6978522539138794\n",
            "Batch Training Loss =  0.6889102458953857\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.6968444585800171\n",
            "Batch Training Loss =  0.6929917931556702\n",
            "Batch Training Loss =  0.6924440264701843\n",
            "Batch Training Loss =  0.6971184015274048\n",
            "Batch Training Loss =  0.6971242427825928\n",
            "Batch Training Loss =  0.6935403943061829\n",
            "Batch Training Loss =  0.6925092339515686\n",
            "Batch Training Loss =  0.6949016451835632\n",
            "Batch Training Loss =  0.6928738355636597\n",
            "Batch Training Loss =  0.6903722286224365\n",
            "Batch Training Loss =  0.7002010345458984\n",
            "Batch Training Loss =  0.6937045454978943\n",
            "Batch Training Loss =  0.6927964091300964\n",
            "Batch Training Loss =  0.6975530385971069\n",
            "Batch Training Loss =  0.7082290053367615\n",
            "Batch Training Loss =  0.6914596557617188\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.7001844644546509\n",
            "Batch Training Loss =  0.6924896836280823\n",
            "Batch Training Loss =  0.6953200697898865\n",
            "Batch Training Loss =  0.6930093169212341\n",
            "Batch Training Loss =  0.6947460770606995\n",
            "Batch Training Loss =  0.704842209815979\n",
            "Batch Training Loss =  0.7045905590057373\n",
            "Batch Training Loss =  0.6976693868637085\n",
            "Batch Training Loss =  0.6915760636329651\n",
            "Batch Training Loss =  0.6940116882324219\n",
            "Batch Training Loss =  0.6942242383956909\n",
            "Batch Training Loss =  0.6959792971611023\n",
            "Batch Training Loss =  0.6918007135391235\n",
            "Batch Training Loss =  0.696936845779419\n",
            "Batch Training Loss =  0.6932794451713562\n",
            "Batch Training Loss =  0.6926711201667786\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6934415698051453\n",
            "Batch Training Loss =  0.6924528479576111\n",
            "Batch Training Loss =  0.6929765939712524\n",
            "Batch Training Loss =  0.6933146715164185\n",
            "Batch Training Loss =  0.6922839283943176\n",
            "Batch Training Loss =  0.7046413421630859\n",
            "Batch Training Loss =  0.6955035328865051\n",
            "Batch Training Loss =  0.6917288303375244\n",
            "Batch Training Loss =  0.6947301626205444\n",
            "Batch Training Loss =  0.6930335164070129\n",
            "Batch Training Loss =  0.6954490542411804\n",
            "Batch Training Loss =  0.6900324821472168\n",
            "Batch Training Loss =  0.6951369643211365\n",
            "Batch Training Loss =  0.6973923444747925\n",
            "Batch Training Loss =  0.6939154267311096\n",
            "Batch Training Loss =  0.6939225792884827\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6982617974281311\n",
            "Batch Training Loss =  0.6997921466827393\n",
            "Batch Training Loss =  0.6937234401702881\n",
            "Batch Training Loss =  0.6901755332946777\n",
            "Batch Training Loss =  0.7063277959823608\n",
            "Batch Training Loss =  0.6866395473480225\n",
            "Batch Training Loss =  0.6933508515357971\n",
            "Batch Training Loss =  0.6896282434463501\n",
            "Batch Training Loss =  0.6951738595962524\n",
            "Batch Training Loss =  0.6963944435119629\n",
            "Batch Training Loss =  0.6869909763336182\n",
            "Batch Training Loss =  0.6981803774833679\n",
            "Batch Training Loss =  0.6932564377784729\n",
            "Batch Training Loss =  0.6926244497299194\n",
            "Batch Training Loss =  0.7036133408546448\n",
            "Batch Training Loss =  0.6965383887290955\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6927064657211304\n",
            "Batch Training Loss =  0.6924110651016235\n",
            "Batch Training Loss =  0.6944832801818848\n",
            "Batch Training Loss =  0.6932109594345093\n",
            "Batch Training Loss =  0.6963794231414795\n",
            "Batch Training Loss =  0.6930590867996216\n",
            "Batch Training Loss =  0.6939041614532471\n",
            "Batch Training Loss =  0.6977468729019165\n",
            "Batch Training Loss =  0.7029426693916321\n",
            "Batch Training Loss =  0.6924152970314026\n",
            "Batch Training Loss =  0.6889930963516235\n",
            "Batch Training Loss =  0.7023763060569763\n",
            "Batch Training Loss =  0.6934182047843933\n",
            "Batch Training Loss =  0.6962478756904602\n",
            "Batch Training Loss =  0.6936801671981812\n",
            "Batch Training Loss =  0.6930670738220215\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.6952552199363708\n",
            "Batch Training Loss =  0.709014892578125\n",
            "Batch Training Loss =  0.6870847940444946\n",
            "Batch Training Loss =  0.693755567073822\n",
            "Batch Training Loss =  0.703921377658844\n",
            "Batch Training Loss =  0.7002244591712952\n",
            "Batch Training Loss =  0.6929330229759216\n",
            "Batch Training Loss =  0.6933833360671997\n",
            "Batch Training Loss =  0.6932986974716187\n",
            "Batch Training Loss =  0.6935012340545654\n",
            "Batch Training Loss =  0.6929501295089722\n",
            "Batch Training Loss =  0.6935694217681885\n",
            "Batch Training Loss =  0.693053662776947\n",
            "Batch Training Loss =  0.6918196678161621\n",
            "Batch Training Loss =  0.6926217079162598\n",
            "Batch Training Loss =  0.693911075592041\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6922771334648132\n",
            "Batch Training Loss =  0.6914826035499573\n",
            "Batch Training Loss =  0.6906289458274841\n",
            "Batch Training Loss =  0.6956062316894531\n",
            "Batch Training Loss =  0.6941094994544983\n",
            "Batch Training Loss =  0.7024621367454529\n",
            "Batch Training Loss =  0.690321147441864\n",
            "Batch Training Loss =  0.7071405649185181\n",
            "Batch Training Loss =  0.6920931339263916\n",
            "Batch Training Loss =  0.6945381760597229\n",
            "Batch Training Loss =  0.6930356621742249\n",
            "Batch Training Loss =  0.6901405453681946\n",
            "Batch Training Loss =  0.693672776222229\n",
            "Batch Training Loss =  0.6913267374038696\n",
            "Batch Training Loss =  0.7020295858383179\n",
            "Batch Training Loss =  0.6887405514717102\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.6774523854255676\n",
            "Batch Training Loss =  0.7135576009750366\n",
            "Batch Training Loss =  0.6920043230056763\n",
            "Batch Training Loss =  0.6887776255607605\n",
            "Batch Training Loss =  0.6987795233726501\n",
            "Batch Training Loss =  0.693569004535675\n",
            "Batch Training Loss =  0.6960887908935547\n",
            "Batch Training Loss =  0.6955447196960449\n",
            "Batch Training Loss =  0.6926994323730469\n",
            "Batch Training Loss =  0.6910800933837891\n",
            "Batch Training Loss =  0.6963719725608826\n",
            "Batch Training Loss =  0.6924511790275574\n",
            "Batch Training Loss =  0.6999936699867249\n",
            "Batch Training Loss =  0.6928439140319824\n",
            "Batch Training Loss =  0.6938521862030029\n",
            "Batch Training Loss =  0.6928289532661438\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.6964845061302185\n",
            "Batch Training Loss =  0.6923145055770874\n",
            "Batch Training Loss =  0.6845220327377319\n",
            "Batch Training Loss =  0.6959465146064758\n",
            "Batch Training Loss =  0.6978657245635986\n",
            "Batch Training Loss =  0.6885321140289307\n",
            "Batch Training Loss =  0.6964613199234009\n",
            "Batch Training Loss =  0.6929574012756348\n",
            "Batch Training Loss =  0.692603588104248\n",
            "Batch Training Loss =  0.6894313097000122\n",
            "Batch Training Loss =  0.698727548122406\n",
            "Batch Training Loss =  0.6945918202400208\n",
            "Batch Training Loss =  0.6909557580947876\n",
            "Batch Training Loss =  0.6944234371185303\n",
            "Batch Training Loss =  0.6914405226707458\n",
            "Batch Training Loss =  0.7016848921775818\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6954503655433655\n",
            "Batch Training Loss =  0.6966947913169861\n",
            "Batch Training Loss =  0.6943893432617188\n",
            "Batch Training Loss =  0.6943092346191406\n",
            "Batch Training Loss =  0.6931875348091125\n",
            "Batch Training Loss =  0.6901481747627258\n",
            "Batch Training Loss =  0.6948287487030029\n",
            "Batch Training Loss =  0.6932991743087769\n",
            "Batch Training Loss =  0.6939979791641235\n",
            "Batch Training Loss =  0.6899454593658447\n",
            "Batch Training Loss =  0.6926245093345642\n",
            "Batch Training Loss =  0.6954771280288696\n",
            "Batch Training Loss =  0.6926862597465515\n",
            "Batch Training Loss =  0.6956229209899902\n",
            "Batch Training Loss =  0.6913270354270935\n",
            "Batch Training Loss =  0.6902324557304382\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6936253905296326\n",
            "Batch Training Loss =  0.6956808567047119\n",
            "Batch Training Loss =  0.69556725025177\n",
            "Batch Training Loss =  0.6913127303123474\n",
            "Batch Training Loss =  0.6924765110015869\n",
            "Batch Training Loss =  0.695301353931427\n",
            "Batch Training Loss =  0.6942587494850159\n",
            "Batch Training Loss =  0.7073996663093567\n",
            "Batch Training Loss =  0.7048661112785339\n",
            "Batch Training Loss =  0.6922566890716553\n",
            "Batch Training Loss =  0.6940977573394775\n",
            "Batch Training Loss =  0.6919231414794922\n",
            "Batch Training Loss =  0.704308032989502\n",
            "Batch Training Loss =  0.6988195776939392\n",
            "Batch Training Loss =  0.6931054592132568\n",
            "Batch Training Loss =  0.6934173107147217\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6933061480522156\n",
            "Batch Training Loss =  0.6936566233634949\n",
            "Batch Training Loss =  0.6918718218803406\n",
            "Batch Training Loss =  0.6946457028388977\n",
            "Batch Training Loss =  0.6931782364845276\n",
            "Batch Training Loss =  0.6949119567871094\n",
            "Batch Training Loss =  0.6934530735015869\n",
            "Batch Training Loss =  0.6969914436340332\n",
            "Batch Training Loss =  0.6926324963569641\n",
            "Batch Training Loss =  0.6939523220062256\n",
            "Batch Training Loss =  0.6933035254478455\n",
            "Batch Training Loss =  0.6959132552146912\n",
            "Batch Training Loss =  0.6936346292495728\n",
            "Batch Training Loss =  0.6918789744377136\n",
            "Batch Training Loss =  0.707445502281189\n",
            "Batch Training Loss =  0.6901863217353821\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6844542622566223\n",
            "Batch Training Loss =  0.7096165418624878\n",
            "Batch Training Loss =  0.6936547756195068\n",
            "Batch Training Loss =  0.6928755044937134\n",
            "Batch Training Loss =  0.6955253481864929\n",
            "Batch Training Loss =  0.6917131543159485\n",
            "Batch Training Loss =  0.6932893395423889\n",
            "Batch Training Loss =  0.6910449862480164\n",
            "Batch Training Loss =  0.6913503408432007\n",
            "Batch Training Loss =  0.6980340480804443\n",
            "Batch Training Loss =  0.6934806704521179\n",
            "Batch Training Loss =  0.6928672790527344\n",
            "Batch Training Loss =  0.6956291198730469\n",
            "Batch Training Loss =  0.6932182908058167\n",
            "Batch Training Loss =  0.6938716769218445\n",
            "Batch Training Loss =  0.6930900812149048\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.6932069659233093\n",
            "Batch Training Loss =  0.6928247213363647\n",
            "Batch Training Loss =  0.6913873553276062\n",
            "Batch Training Loss =  0.6977990865707397\n",
            "Batch Training Loss =  0.6929674744606018\n",
            "Batch Training Loss =  0.6943830251693726\n",
            "Batch Training Loss =  0.690002977848053\n",
            "Batch Training Loss =  0.700782060623169\n",
            "Batch Training Loss =  0.6926332116127014\n",
            "Batch Training Loss =  0.6881476640701294\n",
            "Batch Training Loss =  0.6932514309883118\n",
            "Batch Training Loss =  0.694153904914856\n",
            "Batch Training Loss =  0.6913511157035828\n",
            "Batch Training Loss =  0.7032903432846069\n",
            "Batch Training Loss =  0.6940858364105225\n",
            "Batch Training Loss =  0.693148672580719\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6931573748588562\n",
            "Batch Training Loss =  0.692801833152771\n",
            "Batch Training Loss =  0.6920334696769714\n",
            "Batch Training Loss =  0.6945651173591614\n",
            "Batch Training Loss =  0.6930403113365173\n",
            "Batch Training Loss =  0.6964256167411804\n",
            "Batch Training Loss =  0.6925731897354126\n",
            "Batch Training Loss =  0.6950348019599915\n",
            "Batch Training Loss =  0.6940569877624512\n",
            "Batch Training Loss =  0.6926735043525696\n",
            "Batch Training Loss =  0.6987975239753723\n",
            "Batch Training Loss =  0.6758179664611816\n",
            "Batch Training Loss =  0.7256115674972534\n",
            "Batch Training Loss =  0.6945852041244507\n",
            "Batch Training Loss =  0.6931478381156921\n",
            "Batch Training Loss =  0.6931259632110596\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.6899877190589905\n",
            "Batch Training Loss =  0.6969089508056641\n",
            "Batch Training Loss =  0.692989706993103\n",
            "Batch Training Loss =  0.692176342010498\n",
            "Batch Training Loss =  0.6879577040672302\n",
            "Batch Training Loss =  0.6932765245437622\n",
            "Batch Training Loss =  0.6950633525848389\n",
            "Batch Training Loss =  0.6929414868354797\n",
            "Batch Training Loss =  0.6917659044265747\n",
            "Batch Training Loss =  0.6939201354980469\n",
            "Batch Training Loss =  0.695106029510498\n",
            "Batch Training Loss =  0.6853630542755127\n",
            "Batch Training Loss =  0.6956062912940979\n",
            "Batch Training Loss =  0.6893059611320496\n",
            "Batch Training Loss =  0.6885535717010498\n",
            "Batch Training Loss =  0.6914110779762268\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.7016840577125549\n",
            "Batch Training Loss =  0.6946329474449158\n",
            "Batch Training Loss =  0.6942645907402039\n",
            "Batch Training Loss =  0.6939634084701538\n",
            "Batch Training Loss =  0.6926560401916504\n",
            "Batch Training Loss =  0.6982131004333496\n",
            "Batch Training Loss =  0.6978639364242554\n",
            "Batch Training Loss =  0.6901288628578186\n",
            "Batch Training Loss =  0.6936795711517334\n",
            "Batch Training Loss =  0.6891222596168518\n",
            "Batch Training Loss =  0.6952718496322632\n",
            "Batch Training Loss =  0.691620945930481\n",
            "Batch Training Loss =  0.6898882985115051\n",
            "Batch Training Loss =  0.6938408017158508\n",
            "Batch Training Loss =  0.697007954120636\n",
            "Batch Training Loss =  0.691383957862854\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6969051957130432\n",
            "Batch Training Loss =  0.6916454434394836\n",
            "Batch Training Loss =  0.6902613639831543\n",
            "Batch Training Loss =  0.700392484664917\n",
            "Batch Training Loss =  0.6936923861503601\n",
            "Batch Training Loss =  0.6935927867889404\n",
            "Batch Training Loss =  0.6939732432365417\n",
            "Batch Training Loss =  0.6932580471038818\n",
            "Batch Training Loss =  0.6924424767494202\n",
            "Batch Training Loss =  0.6948249936103821\n",
            "Batch Training Loss =  0.697191059589386\n",
            "Batch Training Loss =  0.6931398510932922\n",
            "Batch Training Loss =  0.6933872103691101\n",
            "Batch Training Loss =  0.6927744150161743\n",
            "Batch Training Loss =  0.6936582326889038\n",
            "Batch Training Loss =  0.6933850646018982\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6918906569480896\n",
            "Batch Training Loss =  0.688172459602356\n",
            "Batch Training Loss =  0.7132778167724609\n",
            "Batch Training Loss =  0.6916961073875427\n",
            "Batch Training Loss =  0.7025710940361023\n",
            "Batch Training Loss =  0.6998732089996338\n",
            "Batch Training Loss =  0.6944068670272827\n",
            "Batch Training Loss =  0.6931560039520264\n",
            "Batch Training Loss =  0.6925485134124756\n",
            "Batch Training Loss =  0.6929653286933899\n",
            "Batch Training Loss =  0.6943733096122742\n",
            "Batch Training Loss =  0.69542396068573\n",
            "Batch Training Loss =  0.7021543979644775\n",
            "Batch Training Loss =  0.681121289730072\n",
            "Batch Training Loss =  0.7115359306335449\n",
            "Batch Training Loss =  0.6929605007171631\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6938295960426331\n",
            "Batch Training Loss =  0.6935831308364868\n",
            "Batch Training Loss =  0.6927374005317688\n",
            "Batch Training Loss =  0.6973389387130737\n",
            "Batch Training Loss =  0.6907745599746704\n",
            "Batch Training Loss =  0.693678081035614\n",
            "Batch Training Loss =  0.6984771490097046\n",
            "Batch Training Loss =  0.6932146549224854\n",
            "Batch Training Loss =  0.7039570808410645\n",
            "Batch Training Loss =  0.7016696929931641\n",
            "Batch Training Loss =  0.693736732006073\n",
            "Batch Training Loss =  0.6978805661201477\n",
            "Batch Training Loss =  0.7043420672416687\n",
            "Batch Training Loss =  0.6931161284446716\n",
            "Batch Training Loss =  0.6955316662788391\n",
            "Batch Training Loss =  0.6958364248275757\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.6934481263160706\n",
            "Batch Training Loss =  0.6911571621894836\n",
            "Batch Training Loss =  0.6952828764915466\n",
            "Batch Training Loss =  0.6953064799308777\n",
            "Batch Training Loss =  0.6950012445449829\n",
            "Batch Training Loss =  0.6936068534851074\n",
            "Batch Training Loss =  0.6913858652114868\n",
            "Batch Training Loss =  0.6944351196289062\n",
            "Batch Training Loss =  0.6941784024238586\n",
            "Batch Training Loss =  0.6919448971748352\n",
            "Batch Training Loss =  0.6908445954322815\n",
            "Batch Training Loss =  0.6893326640129089\n",
            "Batch Training Loss =  0.6887438297271729\n",
            "Batch Training Loss =  0.7081127762794495\n",
            "Batch Training Loss =  0.6927669644355774\n",
            "Batch Training Loss =  0.6931623816490173\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6920599341392517\n",
            "Batch Training Loss =  0.6930861473083496\n",
            "Batch Training Loss =  0.6958890557289124\n",
            "Batch Training Loss =  0.6935140490531921\n",
            "Batch Training Loss =  0.6919206380844116\n",
            "Batch Training Loss =  0.6900789141654968\n",
            "Batch Training Loss =  0.6925740838050842\n",
            "Batch Training Loss =  0.6938861012458801\n",
            "Batch Training Loss =  0.6939064860343933\n",
            "Batch Training Loss =  0.6950914263725281\n",
            "Batch Training Loss =  0.6960343718528748\n",
            "Batch Training Loss =  0.6892330050468445\n",
            "Batch Training Loss =  0.6979678869247437\n",
            "Batch Training Loss =  0.6931611895561218\n",
            "Batch Training Loss =  0.6854020953178406\n",
            "Batch Training Loss =  0.7070510387420654\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.6912437677383423\n",
            "Batch Training Loss =  0.7036145925521851\n",
            "Batch Training Loss =  0.7076480984687805\n",
            "Batch Training Loss =  0.6905274391174316\n",
            "Batch Training Loss =  0.6891337633132935\n",
            "Batch Training Loss =  0.6954440474510193\n",
            "Batch Training Loss =  0.6945462226867676\n",
            "Batch Training Loss =  0.691986083984375\n",
            "Batch Training Loss =  0.6960719227790833\n",
            "Batch Training Loss =  0.692179262638092\n",
            "Batch Training Loss =  0.6916379332542419\n",
            "Batch Training Loss =  0.6947964429855347\n",
            "Batch Training Loss =  0.6929709911346436\n",
            "Batch Training Loss =  0.6923472881317139\n",
            "Batch Training Loss =  0.693666398525238\n",
            "Batch Training Loss =  0.693193793296814\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.692861020565033\n",
            "Batch Training Loss =  0.6934233903884888\n",
            "Batch Training Loss =  0.6944781541824341\n",
            "Batch Training Loss =  0.6969484090805054\n",
            "Batch Training Loss =  0.7096595168113708\n",
            "Batch Training Loss =  0.6934448480606079\n",
            "Batch Training Loss =  0.6925883293151855\n",
            "Batch Training Loss =  0.6931931972503662\n",
            "Batch Training Loss =  0.6934040784835815\n",
            "Batch Training Loss =  0.6933065056800842\n",
            "Batch Training Loss =  0.6944522857666016\n",
            "Batch Training Loss =  0.6992605328559875\n",
            "Batch Training Loss =  0.6931528449058533\n",
            "Batch Training Loss =  0.6931074261665344\n",
            "Batch Training Loss =  0.6974136829376221\n",
            "Batch Training Loss =  0.693793773651123\n",
            "Validation Loss in this epoch is 0.690\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.6991446018218994\n",
            "Batch Training Loss =  0.6916473507881165\n",
            "Batch Training Loss =  0.6923471689224243\n",
            "Batch Training Loss =  0.6910892724990845\n",
            "Batch Training Loss =  0.69537353515625\n",
            "Batch Training Loss =  0.6930608153343201\n",
            "Batch Training Loss =  0.6939015984535217\n",
            "Batch Training Loss =  0.6950933337211609\n",
            "Batch Training Loss =  0.6982364058494568\n",
            "Batch Training Loss =  0.7150247693061829\n",
            "Batch Training Loss =  0.6953135132789612\n",
            "Batch Training Loss =  0.6920397877693176\n",
            "Batch Training Loss =  0.6932140588760376\n",
            "Batch Training Loss =  0.7087141275405884\n",
            "Batch Training Loss =  0.6995359659194946\n",
            "Batch Training Loss =  0.6931554675102234\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.693367600440979\n",
            "Batch Training Loss =  0.7115004062652588\n",
            "Batch Training Loss =  0.6956104040145874\n",
            "Batch Training Loss =  0.6932831406593323\n",
            "Batch Training Loss =  0.6926877498626709\n",
            "Batch Training Loss =  0.6969208717346191\n",
            "Batch Training Loss =  0.6899316906929016\n",
            "Batch Training Loss =  0.7037655115127563\n",
            "Batch Training Loss =  0.6883041858673096\n",
            "Batch Training Loss =  0.70353102684021\n",
            "Batch Training Loss =  0.6907895803451538\n",
            "Batch Training Loss =  0.6971672773361206\n",
            "Batch Training Loss =  0.6927488446235657\n",
            "Batch Training Loss =  0.6947891712188721\n",
            "Batch Training Loss =  0.6916770339012146\n",
            "Batch Training Loss =  0.6907349228858948\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.699713945388794\n",
            "Batch Training Loss =  0.6951106190681458\n",
            "Batch Training Loss =  0.693000316619873\n",
            "Batch Training Loss =  0.6906490325927734\n",
            "Batch Training Loss =  0.7026873826980591\n",
            "Batch Training Loss =  0.6934384703636169\n",
            "Batch Training Loss =  0.693300724029541\n",
            "Batch Training Loss =  0.6930190324783325\n",
            "Batch Training Loss =  0.6924797892570496\n",
            "Batch Training Loss =  0.6889944672584534\n",
            "Batch Training Loss =  0.6851362586021423\n",
            "Batch Training Loss =  0.7069823741912842\n",
            "Batch Training Loss =  0.6964084506034851\n",
            "Batch Training Loss =  0.7040541172027588\n",
            "Batch Training Loss =  0.6924059987068176\n",
            "Batch Training Loss =  0.6937698125839233\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6934152841567993\n",
            "Batch Training Loss =  0.6934434771537781\n",
            "Batch Training Loss =  0.6958218216896057\n",
            "Batch Training Loss =  0.6952332854270935\n",
            "Batch Training Loss =  0.6921700835227966\n",
            "Batch Training Loss =  0.6966552138328552\n",
            "Batch Training Loss =  0.6912351250648499\n",
            "Batch Training Loss =  0.6992140412330627\n",
            "Batch Training Loss =  0.6930647492408752\n",
            "Batch Training Loss =  0.6925350427627563\n",
            "Batch Training Loss =  0.6940135955810547\n",
            "Batch Training Loss =  0.6934921145439148\n",
            "Batch Training Loss =  0.6931184530258179\n",
            "Batch Training Loss =  0.6932323575019836\n",
            "Batch Training Loss =  0.6971249580383301\n",
            "Batch Training Loss =  0.7045776844024658\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6953906416893005\n",
            "Batch Training Loss =  0.6942140460014343\n",
            "Batch Training Loss =  0.6924981474876404\n",
            "Batch Training Loss =  0.6923507452011108\n",
            "Batch Training Loss =  0.6923858523368835\n",
            "Batch Training Loss =  0.6982758045196533\n",
            "Batch Training Loss =  0.7070690393447876\n",
            "Batch Training Loss =  0.6925187706947327\n",
            "Batch Training Loss =  0.6953585743904114\n",
            "Batch Training Loss =  0.6938818097114563\n",
            "Batch Training Loss =  0.6916027069091797\n",
            "Batch Training Loss =  0.6956455707550049\n",
            "Batch Training Loss =  0.6928713321685791\n",
            "Batch Training Loss =  0.6925040483474731\n",
            "Batch Training Loss =  0.6988407373428345\n",
            "Batch Training Loss =  0.7047317028045654\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6959227919578552\n",
            "Batch Training Loss =  0.6839038133621216\n",
            "Batch Training Loss =  0.7044754028320312\n",
            "Batch Training Loss =  0.6930184960365295\n",
            "Batch Training Loss =  0.6947582364082336\n",
            "Batch Training Loss =  0.6950966715812683\n",
            "Batch Training Loss =  0.6937792897224426\n",
            "Batch Training Loss =  0.6940552592277527\n",
            "Batch Training Loss =  0.6932238936424255\n",
            "Batch Training Loss =  0.6937247514724731\n",
            "Batch Training Loss =  0.6924887895584106\n",
            "Batch Training Loss =  0.686694324016571\n",
            "Batch Training Loss =  0.7051565647125244\n",
            "Batch Training Loss =  0.6959049105644226\n",
            "Batch Training Loss =  0.6957604885101318\n",
            "Batch Training Loss =  0.6925969123840332\n",
            "Validation Loss in this epoch is 0.698\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.6957221031188965\n",
            "Batch Training Loss =  0.6947886943817139\n",
            "Batch Training Loss =  0.6795424818992615\n",
            "Batch Training Loss =  0.6932846903800964\n",
            "Batch Training Loss =  0.6860048770904541\n",
            "Batch Training Loss =  0.6863865852355957\n",
            "Batch Training Loss =  0.6986854672431946\n",
            "Batch Training Loss =  0.6932793259620667\n",
            "Batch Training Loss =  0.6937448382377625\n",
            "Batch Training Loss =  0.6936888098716736\n",
            "Batch Training Loss =  0.6940797567367554\n",
            "Batch Training Loss =  0.6932517886161804\n",
            "Batch Training Loss =  0.6947612166404724\n",
            "Batch Training Loss =  0.6952165365219116\n",
            "Batch Training Loss =  0.7003630995750427\n",
            "Batch Training Loss =  0.6929217576980591\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.695132315158844\n",
            "Batch Training Loss =  0.6957132816314697\n",
            "Batch Training Loss =  0.6961406469345093\n",
            "Batch Training Loss =  0.6990350484848022\n",
            "Batch Training Loss =  0.690624475479126\n",
            "Batch Training Loss =  0.703184962272644\n",
            "Batch Training Loss =  0.6935189962387085\n",
            "Batch Training Loss =  0.6935082077980042\n",
            "Batch Training Loss =  0.6922484040260315\n",
            "Batch Training Loss =  0.6916530132293701\n",
            "Batch Training Loss =  0.6964057087898254\n",
            "Batch Training Loss =  0.6940182447433472\n",
            "Batch Training Loss =  0.6926683187484741\n",
            "Batch Training Loss =  0.6924689412117004\n",
            "Batch Training Loss =  0.6896952986717224\n",
            "Batch Training Loss =  0.6883817911148071\n",
            "Validation Loss in this epoch is 0.702\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.6929073929786682\n",
            "Batch Training Loss =  0.6847659945487976\n",
            "Batch Training Loss =  0.683478832244873\n",
            "Batch Training Loss =  0.7031557559967041\n",
            "Batch Training Loss =  0.6919899582862854\n",
            "Batch Training Loss =  0.689218282699585\n",
            "Batch Training Loss =  0.6926870942115784\n",
            "Batch Training Loss =  0.6931447386741638\n",
            "Batch Training Loss =  0.6929490566253662\n",
            "Batch Training Loss =  0.6929530501365662\n",
            "Batch Training Loss =  0.6944577693939209\n",
            "Batch Training Loss =  0.6923570036888123\n",
            "Batch Training Loss =  0.6930040717124939\n",
            "Batch Training Loss =  0.6933315396308899\n",
            "Batch Training Loss =  0.6918962597846985\n",
            "Batch Training Loss =  0.703845202922821\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.6924168467521667\n",
            "Batch Training Loss =  0.6971418857574463\n",
            "Batch Training Loss =  0.6915519833564758\n",
            "Batch Training Loss =  0.6856387853622437\n",
            "Batch Training Loss =  0.7152354717254639\n",
            "Batch Training Loss =  0.6923550367355347\n",
            "Batch Training Loss =  0.6960857510566711\n",
            "Batch Training Loss =  0.6917749047279358\n",
            "Batch Training Loss =  0.6931318640708923\n",
            "Batch Training Loss =  0.6959977746009827\n",
            "Batch Training Loss =  0.6838828325271606\n",
            "Batch Training Loss =  0.7207593321800232\n",
            "Batch Training Loss =  0.6847687363624573\n",
            "Batch Training Loss =  0.6966779232025146\n",
            "Batch Training Loss =  0.6965644359588623\n",
            "Batch Training Loss =  0.6943380832672119\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6949424147605896\n",
            "Batch Training Loss =  0.69565749168396\n",
            "Batch Training Loss =  0.6940813660621643\n",
            "Batch Training Loss =  0.6906348466873169\n",
            "Batch Training Loss =  0.6881449222564697\n",
            "Batch Training Loss =  0.6834362149238586\n",
            "Batch Training Loss =  0.7153164148330688\n",
            "Batch Training Loss =  0.6914131045341492\n",
            "Batch Training Loss =  0.6968129873275757\n",
            "Batch Training Loss =  0.6946146488189697\n",
            "Batch Training Loss =  0.6951606273651123\n",
            "Batch Training Loss =  0.6931921243667603\n",
            "Batch Training Loss =  0.6933221220970154\n",
            "Batch Training Loss =  0.6924059987068176\n",
            "Batch Training Loss =  0.6929852962493896\n",
            "Batch Training Loss =  0.6963012218475342\n",
            "Validation Loss in this epoch is 0.702\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6840075850486755\n",
            "Batch Training Loss =  0.6717581748962402\n",
            "Batch Training Loss =  0.7092532515525818\n",
            "Batch Training Loss =  0.6931268572807312\n",
            "Batch Training Loss =  0.6946843862533569\n",
            "Batch Training Loss =  0.6967115998268127\n",
            "Batch Training Loss =  0.6963863968849182\n",
            "Batch Training Loss =  0.6935441493988037\n",
            "Batch Training Loss =  0.6936437487602234\n",
            "Batch Training Loss =  0.6932547092437744\n",
            "Batch Training Loss =  0.6926702260971069\n",
            "Batch Training Loss =  0.6910721659660339\n",
            "Batch Training Loss =  0.6913989186286926\n",
            "Batch Training Loss =  0.6977535486221313\n",
            "Batch Training Loss =  0.695088267326355\n",
            "Batch Training Loss =  0.6951541304588318\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6963751316070557\n",
            "Batch Training Loss =  0.6929969191551208\n",
            "Batch Training Loss =  0.6937075257301331\n",
            "Batch Training Loss =  0.6929845213890076\n",
            "Batch Training Loss =  0.6925220489501953\n",
            "Batch Training Loss =  0.6934952735900879\n",
            "Batch Training Loss =  0.6917531490325928\n",
            "Batch Training Loss =  0.7084930539131165\n",
            "Batch Training Loss =  0.6904484033584595\n",
            "Batch Training Loss =  0.7020836472511292\n",
            "Batch Training Loss =  0.6940318942070007\n",
            "Batch Training Loss =  0.6914584636688232\n",
            "Batch Training Loss =  0.7061979174613953\n",
            "Batch Training Loss =  0.6952451467514038\n",
            "Batch Training Loss =  0.6938537359237671\n",
            "Batch Training Loss =  0.689944863319397\n",
            "Validation Loss in this epoch is 0.701\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6888049840927124\n",
            "Batch Training Loss =  0.7024509310722351\n",
            "Batch Training Loss =  0.6964935064315796\n",
            "Batch Training Loss =  0.6989454030990601\n",
            "Batch Training Loss =  0.6933281421661377\n",
            "Batch Training Loss =  0.6924701929092407\n",
            "Batch Training Loss =  0.6913361549377441\n",
            "Batch Training Loss =  0.7058738470077515\n",
            "Batch Training Loss =  0.7071464657783508\n",
            "Batch Training Loss =  0.6935063004493713\n",
            "Batch Training Loss =  0.6925989389419556\n",
            "Batch Training Loss =  0.6909394860267639\n",
            "Batch Training Loss =  0.6901974678039551\n",
            "Batch Training Loss =  0.7051034569740295\n",
            "Batch Training Loss =  0.7074373364448547\n",
            "Batch Training Loss =  0.689024031162262\n",
            "Validation Loss in this epoch is 0.701\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.7035167217254639\n",
            "Batch Training Loss =  0.6883759498596191\n",
            "Batch Training Loss =  0.688306987285614\n",
            "Batch Training Loss =  0.6914522051811218\n",
            "Batch Training Loss =  0.7029182314872742\n",
            "Batch Training Loss =  0.6823618412017822\n",
            "Batch Training Loss =  0.695475697517395\n",
            "Batch Training Loss =  0.6958401203155518\n",
            "Batch Training Loss =  0.6924854516983032\n",
            "Batch Training Loss =  0.6872135996818542\n",
            "Batch Training Loss =  0.7002354264259338\n",
            "Batch Training Loss =  0.6937436461448669\n",
            "Batch Training Loss =  0.700102686882019\n",
            "Batch Training Loss =  0.6932719945907593\n",
            "Batch Training Loss =  0.6905871629714966\n",
            "Batch Training Loss =  0.6940548419952393\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6942062377929688\n",
            "Batch Training Loss =  0.6931664347648621\n",
            "Batch Training Loss =  0.6930001974105835\n",
            "Batch Training Loss =  0.6950387358665466\n",
            "Batch Training Loss =  0.7032671570777893\n",
            "Batch Training Loss =  0.6915444731712341\n",
            "Batch Training Loss =  0.6965534687042236\n",
            "Batch Training Loss =  0.6936454772949219\n",
            "Batch Training Loss =  0.6926798820495605\n",
            "Batch Training Loss =  0.6975556015968323\n",
            "Batch Training Loss =  0.6932530403137207\n",
            "Batch Training Loss =  0.6935931444168091\n",
            "Batch Training Loss =  0.6884891986846924\n",
            "Batch Training Loss =  0.6944366693496704\n",
            "Batch Training Loss =  0.6936739087104797\n",
            "Batch Training Loss =  0.6928046941757202\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.6938683986663818\n",
            "Batch Training Loss =  0.6938387155532837\n",
            "Batch Training Loss =  0.6969016194343567\n",
            "Batch Training Loss =  0.7000795006752014\n",
            "Batch Training Loss =  0.7107667326927185\n",
            "Batch Training Loss =  0.6859319806098938\n",
            "Batch Training Loss =  0.6939732432365417\n",
            "Batch Training Loss =  0.7002778649330139\n",
            "Batch Training Loss =  0.6941680312156677\n",
            "Batch Training Loss =  0.6926963925361633\n",
            "Batch Training Loss =  0.69488126039505\n",
            "Batch Training Loss =  0.6956841349601746\n",
            "Batch Training Loss =  0.7001217007637024\n",
            "Batch Training Loss =  0.6981953382492065\n",
            "Batch Training Loss =  0.6909155249595642\n",
            "Batch Training Loss =  0.7088083028793335\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.6974542737007141\n",
            "Batch Training Loss =  0.6944208741188049\n",
            "Batch Training Loss =  0.6896222829818726\n",
            "Batch Training Loss =  0.7033746242523193\n",
            "Batch Training Loss =  0.6896544694900513\n",
            "Batch Training Loss =  0.6915116310119629\n",
            "Batch Training Loss =  0.7063674926757812\n",
            "Batch Training Loss =  0.6862812042236328\n",
            "Batch Training Loss =  0.6988074779510498\n",
            "Batch Training Loss =  0.6926206946372986\n",
            "Batch Training Loss =  0.6909609436988831\n",
            "Batch Training Loss =  0.6867496967315674\n",
            "Batch Training Loss =  0.6882231831550598\n",
            "Batch Training Loss =  0.6792700886726379\n",
            "Batch Training Loss =  0.6966423988342285\n",
            "Batch Training Loss =  0.6997849345207214\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6894481778144836\n",
            "Batch Training Loss =  0.7035964131355286\n",
            "Batch Training Loss =  0.6931651830673218\n",
            "Batch Training Loss =  0.6928051114082336\n",
            "Batch Training Loss =  0.6924929618835449\n",
            "Batch Training Loss =  0.6978088617324829\n",
            "Batch Training Loss =  0.6856615543365479\n",
            "Batch Training Loss =  0.6808449625968933\n",
            "Batch Training Loss =  0.6835817098617554\n",
            "Batch Training Loss =  0.6965966820716858\n",
            "Batch Training Loss =  0.6965348124504089\n",
            "Batch Training Loss =  0.6936477422714233\n",
            "Batch Training Loss =  0.6922999620437622\n",
            "Batch Training Loss =  0.7028072476387024\n",
            "Batch Training Loss =  0.6927591562271118\n",
            "Batch Training Loss =  0.6964027285575867\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.6949009895324707\n",
            "Batch Training Loss =  0.692950963973999\n",
            "Batch Training Loss =  0.6932886242866516\n",
            "Batch Training Loss =  0.693260908126831\n",
            "Batch Training Loss =  0.6921650171279907\n",
            "Batch Training Loss =  0.695101797580719\n",
            "Batch Training Loss =  0.6948285102844238\n",
            "Batch Training Loss =  0.6930503249168396\n",
            "Batch Training Loss =  0.69010329246521\n",
            "Batch Training Loss =  0.6985708475112915\n",
            "Batch Training Loss =  0.6951861381530762\n",
            "Batch Training Loss =  0.7046103477478027\n",
            "Batch Training Loss =  0.6884983777999878\n",
            "Batch Training Loss =  0.6870926022529602\n",
            "Batch Training Loss =  0.7013813853263855\n",
            "Batch Training Loss =  0.6928374767303467\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6961765885353088\n",
            "Batch Training Loss =  0.6905618906021118\n",
            "Batch Training Loss =  0.690163254737854\n",
            "Batch Training Loss =  0.7005935907363892\n",
            "Batch Training Loss =  0.6969428062438965\n",
            "Batch Training Loss =  0.6883265972137451\n",
            "Batch Training Loss =  0.691586971282959\n",
            "Batch Training Loss =  0.6968315839767456\n",
            "Batch Training Loss =  0.6931533813476562\n",
            "Batch Training Loss =  0.6929982900619507\n",
            "Batch Training Loss =  0.6998369097709656\n",
            "Batch Training Loss =  0.693540096282959\n",
            "Batch Training Loss =  0.6936445832252502\n",
            "Batch Training Loss =  0.6942135691642761\n",
            "Batch Training Loss =  0.7033710479736328\n",
            "Batch Training Loss =  0.6957850456237793\n",
            "Validation Loss in this epoch is 0.706\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.7016105651855469\n",
            "Batch Training Loss =  0.6926477551460266\n",
            "Batch Training Loss =  0.6943357586860657\n",
            "Batch Training Loss =  0.6931582689285278\n",
            "Batch Training Loss =  0.6941944360733032\n",
            "Batch Training Loss =  0.6932083964347839\n",
            "Batch Training Loss =  0.691806435585022\n",
            "Batch Training Loss =  0.6939042806625366\n",
            "Batch Training Loss =  0.6934491991996765\n",
            "Batch Training Loss =  0.6935577988624573\n",
            "Batch Training Loss =  0.6929795742034912\n",
            "Batch Training Loss =  0.6936851739883423\n",
            "Batch Training Loss =  0.6931572556495667\n",
            "Batch Training Loss =  0.6933091878890991\n",
            "Batch Training Loss =  0.6984979510307312\n",
            "Batch Training Loss =  0.6941869258880615\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.6894436478614807\n",
            "Batch Training Loss =  0.6984967589378357\n",
            "Batch Training Loss =  0.6931214928627014\n",
            "Batch Training Loss =  0.6936172246932983\n",
            "Batch Training Loss =  0.6929519772529602\n",
            "Batch Training Loss =  0.6950685977935791\n",
            "Batch Training Loss =  0.6980143785476685\n",
            "Batch Training Loss =  0.6940357089042664\n",
            "Batch Training Loss =  0.6966053247451782\n",
            "Batch Training Loss =  0.7007724046707153\n",
            "Batch Training Loss =  0.6941290497779846\n",
            "Batch Training Loss =  0.6916386485099792\n",
            "Batch Training Loss =  0.6913539171218872\n",
            "Batch Training Loss =  0.6982983946800232\n",
            "Batch Training Loss =  0.6980021595954895\n",
            "Batch Training Loss =  0.7047845721244812\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  1 th epoch\n",
            "Batch Training Loss =  0.68645840883255\n",
            "Batch Training Loss =  0.7099758386611938\n",
            "Batch Training Loss =  0.6946076154708862\n",
            "Batch Training Loss =  0.693375825881958\n",
            "Batch Training Loss =  0.6953477263450623\n",
            "Batch Training Loss =  0.6950939297676086\n",
            "Batch Training Loss =  0.6934081315994263\n",
            "Batch Training Loss =  0.693877100944519\n",
            "Batch Training Loss =  0.6962356567382812\n",
            "Batch Training Loss =  0.6929343938827515\n",
            "Batch Training Loss =  0.6916430592536926\n",
            "Batch Training Loss =  0.6998192667961121\n",
            "Batch Training Loss =  0.6942094564437866\n",
            "Batch Training Loss =  0.693795919418335\n",
            "Batch Training Loss =  0.6932730674743652\n",
            "Batch Training Loss =  0.6925876140594482\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  2 th epoch\n",
            "Batch Training Loss =  0.6999481320381165\n",
            "Batch Training Loss =  0.6942407488822937\n",
            "Batch Training Loss =  0.6924264430999756\n",
            "Batch Training Loss =  0.6957710385322571\n",
            "Batch Training Loss =  0.6933427453041077\n",
            "Batch Training Loss =  0.6935207843780518\n",
            "Batch Training Loss =  0.6850792169570923\n",
            "Batch Training Loss =  0.6961432099342346\n",
            "Batch Training Loss =  0.6993293166160583\n",
            "Batch Training Loss =  0.6925163269042969\n",
            "Batch Training Loss =  0.6954665184020996\n",
            "Batch Training Loss =  0.6945968866348267\n",
            "Batch Training Loss =  0.6931726932525635\n",
            "Batch Training Loss =  0.6930009722709656\n",
            "Batch Training Loss =  0.6914259195327759\n",
            "Batch Training Loss =  0.6884861588478088\n",
            "Validation Loss in this epoch is 0.691\n",
            "This is  3 th epoch\n",
            "Batch Training Loss =  0.7000870704650879\n",
            "Batch Training Loss =  0.6946699619293213\n",
            "Batch Training Loss =  0.702223539352417\n",
            "Batch Training Loss =  0.6934444308280945\n",
            "Batch Training Loss =  0.7047986388206482\n",
            "Batch Training Loss =  0.6910918354988098\n",
            "Batch Training Loss =  0.7025598287582397\n",
            "Batch Training Loss =  0.6919780969619751\n",
            "Batch Training Loss =  0.6959492564201355\n",
            "Batch Training Loss =  0.6939508318901062\n",
            "Batch Training Loss =  0.7029568552970886\n",
            "Batch Training Loss =  0.7039873003959656\n",
            "Batch Training Loss =  0.698866605758667\n",
            "Batch Training Loss =  0.6934293508529663\n",
            "Batch Training Loss =  0.6925324201583862\n",
            "Batch Training Loss =  0.6900798082351685\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  4 th epoch\n",
            "Batch Training Loss =  0.7023910284042358\n",
            "Batch Training Loss =  0.696503758430481\n",
            "Batch Training Loss =  0.6925997734069824\n",
            "Batch Training Loss =  0.6975071430206299\n",
            "Batch Training Loss =  0.6928703784942627\n",
            "Batch Training Loss =  0.6915127038955688\n",
            "Batch Training Loss =  0.6957541108131409\n",
            "Batch Training Loss =  0.6925955414772034\n",
            "Batch Training Loss =  0.6931926012039185\n",
            "Batch Training Loss =  0.6915907859802246\n",
            "Batch Training Loss =  0.6951781511306763\n",
            "Batch Training Loss =  0.6930742859840393\n",
            "Batch Training Loss =  0.6927613019943237\n",
            "Batch Training Loss =  0.6934201717376709\n",
            "Batch Training Loss =  0.6927509307861328\n",
            "Batch Training Loss =  0.687241792678833\n",
            "Validation Loss in this epoch is 0.706\n",
            "This is  5 th epoch\n",
            "Batch Training Loss =  0.7101091146469116\n",
            "Batch Training Loss =  0.6936275959014893\n",
            "Batch Training Loss =  0.6922717094421387\n",
            "Batch Training Loss =  0.7028143405914307\n",
            "Batch Training Loss =  0.6931988000869751\n",
            "Batch Training Loss =  0.6939318776130676\n",
            "Batch Training Loss =  0.6945839524269104\n",
            "Batch Training Loss =  0.6933121681213379\n",
            "Batch Training Loss =  0.6816411018371582\n",
            "Batch Training Loss =  0.7077009677886963\n",
            "Batch Training Loss =  0.693432092666626\n",
            "Batch Training Loss =  0.6911463141441345\n",
            "Batch Training Loss =  0.7044421434402466\n",
            "Batch Training Loss =  0.6964265704154968\n",
            "Batch Training Loss =  0.693452000617981\n",
            "Batch Training Loss =  0.6946958303451538\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  6 th epoch\n",
            "Batch Training Loss =  0.6934412121772766\n",
            "Batch Training Loss =  0.6939569711685181\n",
            "Batch Training Loss =  0.6948301196098328\n",
            "Batch Training Loss =  0.693648099899292\n",
            "Batch Training Loss =  0.6931586265563965\n",
            "Batch Training Loss =  0.6931482553482056\n",
            "Batch Training Loss =  0.693164587020874\n",
            "Batch Training Loss =  0.6925498247146606\n",
            "Batch Training Loss =  0.6976380944252014\n",
            "Batch Training Loss =  0.6928637027740479\n",
            "Batch Training Loss =  0.6890105605125427\n",
            "Batch Training Loss =  0.6841705441474915\n",
            "Batch Training Loss =  0.7085044980049133\n",
            "Batch Training Loss =  0.6925618648529053\n",
            "Batch Training Loss =  0.6945109367370605\n",
            "Batch Training Loss =  0.6929770708084106\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  7 th epoch\n",
            "Batch Training Loss =  0.6929673552513123\n",
            "Batch Training Loss =  0.6929645538330078\n",
            "Batch Training Loss =  0.693248450756073\n",
            "Batch Training Loss =  0.6926438212394714\n",
            "Batch Training Loss =  0.6987004280090332\n",
            "Batch Training Loss =  0.6922879815101624\n",
            "Batch Training Loss =  0.6897444128990173\n",
            "Batch Training Loss =  0.6913505792617798\n",
            "Batch Training Loss =  0.6972825527191162\n",
            "Batch Training Loss =  0.6920406222343445\n",
            "Batch Training Loss =  0.6905737519264221\n",
            "Batch Training Loss =  0.6881648302078247\n",
            "Batch Training Loss =  0.7036399245262146\n",
            "Batch Training Loss =  0.6940569281578064\n",
            "Batch Training Loss =  0.6929999589920044\n",
            "Batch Training Loss =  0.6923471093177795\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  8 th epoch\n",
            "Batch Training Loss =  0.6865856647491455\n",
            "Batch Training Loss =  0.7146484851837158\n",
            "Batch Training Loss =  0.6948598623275757\n",
            "Batch Training Loss =  0.6934082508087158\n",
            "Batch Training Loss =  0.693856418132782\n",
            "Batch Training Loss =  0.7058020234107971\n",
            "Batch Training Loss =  0.7071603536605835\n",
            "Batch Training Loss =  0.69318026304245\n",
            "Batch Training Loss =  0.6931986212730408\n",
            "Batch Training Loss =  0.6943023800849915\n",
            "Batch Training Loss =  0.6993607878684998\n",
            "Batch Training Loss =  0.6952313780784607\n",
            "Batch Training Loss =  0.6956860423088074\n",
            "Batch Training Loss =  0.6909424662590027\n",
            "Batch Training Loss =  0.6926839351654053\n",
            "Batch Training Loss =  0.6979260444641113\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  9 th epoch\n",
            "Batch Training Loss =  0.6961522102355957\n",
            "Batch Training Loss =  0.6933197975158691\n",
            "Batch Training Loss =  0.691824734210968\n",
            "Batch Training Loss =  0.7029645442962646\n",
            "Batch Training Loss =  0.6902902126312256\n",
            "Batch Training Loss =  0.6995429992675781\n",
            "Batch Training Loss =  0.6931925415992737\n",
            "Batch Training Loss =  0.6930367946624756\n",
            "Batch Training Loss =  0.6971190571784973\n",
            "Batch Training Loss =  0.7083732485771179\n",
            "Batch Training Loss =  0.6896714568138123\n",
            "Batch Training Loss =  0.6926206946372986\n",
            "Batch Training Loss =  0.6985996961593628\n",
            "Batch Training Loss =  0.7010691165924072\n",
            "Batch Training Loss =  0.7024566531181335\n",
            "Batch Training Loss =  0.6988336443901062\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  10 th epoch\n",
            "Batch Training Loss =  0.6934346556663513\n",
            "Batch Training Loss =  0.6933168768882751\n",
            "Batch Training Loss =  0.6933348774909973\n",
            "Batch Training Loss =  0.6911880373954773\n",
            "Batch Training Loss =  0.6912890672683716\n",
            "Batch Training Loss =  0.7059581279754639\n",
            "Batch Training Loss =  0.6894839406013489\n",
            "Batch Training Loss =  0.6956498622894287\n",
            "Batch Training Loss =  0.6916705369949341\n",
            "Batch Training Loss =  0.6895838379859924\n",
            "Batch Training Loss =  0.6977468729019165\n",
            "Batch Training Loss =  0.6931165456771851\n",
            "Batch Training Loss =  0.6957797408103943\n",
            "Batch Training Loss =  0.682736337184906\n",
            "Batch Training Loss =  0.7061380743980408\n",
            "Batch Training Loss =  0.6944045424461365\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  11 th epoch\n",
            "Batch Training Loss =  0.694429874420166\n",
            "Batch Training Loss =  0.6949959397315979\n",
            "Batch Training Loss =  0.7099034190177917\n",
            "Batch Training Loss =  0.7062084674835205\n",
            "Batch Training Loss =  0.6915801763534546\n",
            "Batch Training Loss =  0.6989303827285767\n",
            "Batch Training Loss =  0.6952293515205383\n",
            "Batch Training Loss =  0.6928209662437439\n",
            "Batch Training Loss =  0.690218985080719\n",
            "Batch Training Loss =  0.6991689801216125\n",
            "Batch Training Loss =  0.693714439868927\n",
            "Batch Training Loss =  0.6882016658782959\n",
            "Batch Training Loss =  0.6991695165634155\n",
            "Batch Training Loss =  0.6926517486572266\n",
            "Batch Training Loss =  0.6948003172874451\n",
            "Batch Training Loss =  0.6951124668121338\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  12 th epoch\n",
            "Batch Training Loss =  0.693968653678894\n",
            "Batch Training Loss =  0.690125584602356\n",
            "Batch Training Loss =  0.6937921643257141\n",
            "Batch Training Loss =  0.6918469071388245\n",
            "Batch Training Loss =  0.6846445202827454\n",
            "Batch Training Loss =  0.696514368057251\n",
            "Batch Training Loss =  0.6925430297851562\n",
            "Batch Training Loss =  0.6969077587127686\n",
            "Batch Training Loss =  0.6924120187759399\n",
            "Batch Training Loss =  0.692983865737915\n",
            "Batch Training Loss =  0.689232587814331\n",
            "Batch Training Loss =  0.6836305856704712\n",
            "Batch Training Loss =  0.702839195728302\n",
            "Batch Training Loss =  0.69414883852005\n",
            "Batch Training Loss =  0.6908933520317078\n",
            "Batch Training Loss =  0.696483314037323\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  13 th epoch\n",
            "Batch Training Loss =  0.6935334205627441\n",
            "Batch Training Loss =  0.6931200623512268\n",
            "Batch Training Loss =  0.6959705948829651\n",
            "Batch Training Loss =  0.7020718455314636\n",
            "Batch Training Loss =  0.6870514750480652\n",
            "Batch Training Loss =  0.6847897171974182\n",
            "Batch Training Loss =  0.7015836238861084\n",
            "Batch Training Loss =  0.6940808296203613\n",
            "Batch Training Loss =  0.698051929473877\n",
            "Batch Training Loss =  0.6924481987953186\n",
            "Batch Training Loss =  0.6935318112373352\n",
            "Batch Training Loss =  0.6945140361785889\n",
            "Batch Training Loss =  0.6945977210998535\n",
            "Batch Training Loss =  0.6903697848320007\n",
            "Batch Training Loss =  0.7009468674659729\n",
            "Batch Training Loss =  0.6923795342445374\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  14 th epoch\n",
            "Batch Training Loss =  0.6924012899398804\n",
            "Batch Training Loss =  0.6947195529937744\n",
            "Batch Training Loss =  0.6907185316085815\n",
            "Batch Training Loss =  0.7063672542572021\n",
            "Batch Training Loss =  0.6935823559761047\n",
            "Batch Training Loss =  0.6904357075691223\n",
            "Batch Training Loss =  0.701195478439331\n",
            "Batch Training Loss =  0.701069712638855\n",
            "Batch Training Loss =  0.6779871582984924\n",
            "Batch Training Loss =  0.7193859815597534\n",
            "Batch Training Loss =  0.6935970187187195\n",
            "Batch Training Loss =  0.6991994380950928\n",
            "Batch Training Loss =  0.688819944858551\n",
            "Batch Training Loss =  0.6899438500404358\n",
            "Batch Training Loss =  0.7029239535331726\n",
            "Batch Training Loss =  0.6890932321548462\n",
            "Validation Loss in this epoch is 0.700\n",
            "This is  15 th epoch\n",
            "Batch Training Loss =  0.6899754405021667\n",
            "Batch Training Loss =  0.6925820112228394\n",
            "Batch Training Loss =  0.6931192278862\n",
            "Batch Training Loss =  0.6938114166259766\n",
            "Batch Training Loss =  0.69329434633255\n",
            "Batch Training Loss =  0.6974940299987793\n",
            "Batch Training Loss =  0.7131824493408203\n",
            "Batch Training Loss =  0.6923479437828064\n",
            "Batch Training Loss =  0.6923825144767761\n",
            "Batch Training Loss =  0.6941710114479065\n",
            "Batch Training Loss =  0.6931479573249817\n",
            "Batch Training Loss =  0.6931403279304504\n",
            "Batch Training Loss =  0.6921879053115845\n",
            "Batch Training Loss =  0.6987950801849365\n",
            "Batch Training Loss =  0.6895862817764282\n",
            "Batch Training Loss =  0.69869065284729\n",
            "Validation Loss in this epoch is 0.694\n",
            "This is  16 th epoch\n",
            "Batch Training Loss =  0.6949071288108826\n",
            "Batch Training Loss =  0.6932621598243713\n",
            "Batch Training Loss =  0.6920116543769836\n",
            "Batch Training Loss =  0.6901271939277649\n",
            "Batch Training Loss =  0.6950143575668335\n",
            "Batch Training Loss =  0.694418728351593\n",
            "Batch Training Loss =  0.6890394687652588\n",
            "Batch Training Loss =  0.7037187814712524\n",
            "Batch Training Loss =  0.6931731700897217\n",
            "Batch Training Loss =  0.6925510168075562\n",
            "Batch Training Loss =  0.6960781216621399\n",
            "Batch Training Loss =  0.6991037726402283\n",
            "Batch Training Loss =  0.6961595416069031\n",
            "Batch Training Loss =  0.6943211555480957\n",
            "Batch Training Loss =  0.690106213092804\n",
            "Batch Training Loss =  0.6955697536468506\n",
            "Validation Loss in this epoch is 0.695\n",
            "This is  17 th epoch\n",
            "Batch Training Loss =  0.695880115032196\n",
            "Batch Training Loss =  0.7021090984344482\n",
            "Batch Training Loss =  0.6827476024627686\n",
            "Batch Training Loss =  0.7061346173286438\n",
            "Batch Training Loss =  0.6882200837135315\n",
            "Batch Training Loss =  0.7007797956466675\n",
            "Batch Training Loss =  0.6989438533782959\n",
            "Batch Training Loss =  0.6920049786567688\n",
            "Batch Training Loss =  0.6916087865829468\n",
            "Batch Training Loss =  0.6981078386306763\n",
            "Batch Training Loss =  0.697620153427124\n",
            "Batch Training Loss =  0.690026044845581\n",
            "Batch Training Loss =  0.6938118934631348\n",
            "Batch Training Loss =  0.6946951150894165\n",
            "Batch Training Loss =  0.6922975182533264\n",
            "Batch Training Loss =  0.696695864200592\n",
            "Validation Loss in this epoch is 0.692\n",
            "This is  18 th epoch\n",
            "Batch Training Loss =  0.6943084001541138\n",
            "Batch Training Loss =  0.6944541931152344\n",
            "Batch Training Loss =  0.6954800486564636\n",
            "Batch Training Loss =  0.7089353203773499\n",
            "Batch Training Loss =  0.6861921548843384\n",
            "Batch Training Loss =  0.69846510887146\n",
            "Batch Training Loss =  0.6942551136016846\n",
            "Batch Training Loss =  0.6930174827575684\n",
            "Batch Training Loss =  0.6981580853462219\n",
            "Batch Training Loss =  0.6917904615402222\n",
            "Batch Training Loss =  0.6900031566619873\n",
            "Batch Training Loss =  0.6938166618347168\n",
            "Batch Training Loss =  0.6935548186302185\n",
            "Batch Training Loss =  0.6930124759674072\n",
            "Batch Training Loss =  0.6939806342124939\n",
            "Batch Training Loss =  0.6934911608695984\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  19 th epoch\n",
            "Batch Training Loss =  0.691131591796875\n",
            "Batch Training Loss =  0.701881468296051\n",
            "Batch Training Loss =  0.6883376240730286\n",
            "Batch Training Loss =  0.6833823919296265\n",
            "Batch Training Loss =  0.6923364400863647\n",
            "Batch Training Loss =  0.7085957527160645\n",
            "Batch Training Loss =  0.6826886534690857\n",
            "Batch Training Loss =  0.6924675703048706\n",
            "Batch Training Loss =  0.6913490295410156\n",
            "Batch Training Loss =  0.6923852562904358\n",
            "Batch Training Loss =  0.6979061961174011\n",
            "Batch Training Loss =  0.6983440518379211\n",
            "Batch Training Loss =  0.694466233253479\n",
            "Batch Training Loss =  0.6944715976715088\n",
            "Batch Training Loss =  0.6886656284332275\n",
            "Batch Training Loss =  0.6825737953186035\n",
            "Validation Loss in this epoch is 0.706\n",
            "This is  20 th epoch\n",
            "Batch Training Loss =  0.7016770839691162\n",
            "Batch Training Loss =  0.6905642151832581\n",
            "Batch Training Loss =  0.6980885863304138\n",
            "Batch Training Loss =  0.6931511759757996\n",
            "Batch Training Loss =  0.6931965351104736\n",
            "Batch Training Loss =  0.6930323243141174\n",
            "Batch Training Loss =  0.6912758350372314\n",
            "Batch Training Loss =  0.6969802975654602\n",
            "Batch Training Loss =  0.693213701248169\n",
            "Batch Training Loss =  0.6926440596580505\n",
            "Batch Training Loss =  0.6930279731750488\n",
            "Batch Training Loss =  0.6889826059341431\n",
            "Batch Training Loss =  0.7346386909484863\n",
            "Batch Training Loss =  0.681347131729126\n",
            "Batch Training Loss =  0.6813570261001587\n",
            "Batch Training Loss =  0.6997077465057373\n",
            "Validation Loss in this epoch is 0.696\n",
            "This is  21 th epoch\n",
            "Batch Training Loss =  0.6958265900611877\n",
            "Batch Training Loss =  0.6925553679466248\n",
            "Batch Training Loss =  0.6898588538169861\n",
            "Batch Training Loss =  0.6961680054664612\n",
            "Batch Training Loss =  0.693889319896698\n",
            "Batch Training Loss =  0.6931516528129578\n",
            "Batch Training Loss =  0.6931297183036804\n",
            "Batch Training Loss =  0.6929945349693298\n",
            "Batch Training Loss =  0.6945407390594482\n",
            "Batch Training Loss =  0.6969727873802185\n",
            "Batch Training Loss =  0.6919848918914795\n",
            "Batch Training Loss =  0.6887706518173218\n",
            "Batch Training Loss =  0.6826190948486328\n",
            "Batch Training Loss =  0.6949338316917419\n",
            "Batch Training Loss =  0.6999837756156921\n",
            "Batch Training Loss =  0.6984525918960571\n",
            "Validation Loss in this epoch is 0.699\n",
            "This is  22 th epoch\n",
            "Batch Training Loss =  0.7045130729675293\n",
            "Batch Training Loss =  0.7012696862220764\n",
            "Batch Training Loss =  0.6917572021484375\n",
            "Batch Training Loss =  0.6891942620277405\n",
            "Batch Training Loss =  0.697161853313446\n",
            "Batch Training Loss =  0.6929822564125061\n",
            "Batch Training Loss =  0.6932380199432373\n",
            "Batch Training Loss =  0.6933980584144592\n",
            "Batch Training Loss =  0.6952885985374451\n",
            "Batch Training Loss =  0.6938024759292603\n",
            "Batch Training Loss =  0.6928664445877075\n",
            "Batch Training Loss =  0.6956334710121155\n",
            "Batch Training Loss =  0.692870020866394\n",
            "Batch Training Loss =  0.6942877173423767\n",
            "Batch Training Loss =  0.6948326230049133\n",
            "Batch Training Loss =  0.6913543939590454\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  23 th epoch\n",
            "Batch Training Loss =  0.6942435503005981\n",
            "Batch Training Loss =  0.692402184009552\n",
            "Batch Training Loss =  0.6889416575431824\n",
            "Batch Training Loss =  0.7043927907943726\n",
            "Batch Training Loss =  0.6960640549659729\n",
            "Batch Training Loss =  0.6881248354911804\n",
            "Batch Training Loss =  0.6998693346977234\n",
            "Batch Training Loss =  0.6935511827468872\n",
            "Batch Training Loss =  0.6946598291397095\n",
            "Batch Training Loss =  0.6931169629096985\n",
            "Batch Training Loss =  0.6921549439430237\n",
            "Batch Training Loss =  0.6903071403503418\n",
            "Batch Training Loss =  0.6969512701034546\n",
            "Batch Training Loss =  0.6931522488594055\n",
            "Batch Training Loss =  0.6929376125335693\n",
            "Batch Training Loss =  0.7058337330818176\n",
            "Validation Loss in this epoch is 0.693\n",
            "This is  24 th epoch\n",
            "Batch Training Loss =  0.692557156085968\n",
            "Batch Training Loss =  0.68818199634552\n",
            "Batch Training Loss =  0.680656909942627\n",
            "Batch Training Loss =  0.7056622505187988\n",
            "Batch Training Loss =  0.6943348050117493\n",
            "Batch Training Loss =  0.6944472789764404\n",
            "Batch Training Loss =  0.6939792633056641\n",
            "Batch Training Loss =  0.692966639995575\n",
            "Batch Training Loss =  0.691877007484436\n",
            "Batch Training Loss =  0.6905467510223389\n",
            "Batch Training Loss =  0.6870770454406738\n",
            "Batch Training Loss =  0.7106282711029053\n",
            "Batch Training Loss =  0.6924452185630798\n",
            "Batch Training Loss =  0.6996028423309326\n",
            "Batch Training Loss =  0.7010635137557983\n",
            "Batch Training Loss =  0.6917950510978699\n",
            "Validation Loss in this epoch is 0.697\n",
            "This is  25 th epoch\n",
            "Batch Training Loss =  0.696940004825592\n",
            "Batch Training Loss =  0.691328763961792\n",
            "Batch Training Loss =  0.6978452801704407\n",
            "Batch Training Loss =  0.693137526512146\n",
            "Batch Training Loss =  0.6929959058761597\n",
            "Batch Training Loss =  0.6934934258460999\n",
            "Batch Training Loss =  0.6930438876152039\n",
            "Batch Training Loss =  0.6925073862075806\n",
            "Batch Training Loss =  0.6997484564781189\n",
            "Batch Training Loss =  0.6883454322814941\n",
            "Batch Training Loss =  0.7032936215400696\n",
            "Batch Training Loss =  0.697965145111084\n",
            "Batch Training Loss =  0.7028824090957642\n",
            "Batch Training Loss =  0.697532057762146\n",
            "Batch Training Loss =  0.6974939107894897\n",
            "Batch Training Loss =  0.6933004856109619\n",
            "Validation Loss in this epoch is 0.697\n"
          ]
        }
      ],
      "source": [
        "# TODO [3 points]: Perform cross-validation to get train/val accuracy\n",
        "# for all hyper-parameter settings in the list below.\n",
        "\n",
        "# Changed Learning Rates, Weight Decays and Batch Size as well\n",
        "# Decreased num of epochs to 50\n",
        "\n",
        "learning_rates = [0.25, 0.5, 1]\n",
        "weight_decays = [0.1, 0.2]\n",
        "batch_size = 100\n",
        "n_epochs = 25\n",
        "n_folds = 5\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        val_accs = []  # store validation accuracy for each fold\n",
        "        train_accs = []  # store training accuracy for each fold\n",
        "\n",
        "        # TODO: iterate over folds, remember to use \"shuffle=True\", as datapoints are not shuffled\n",
        "\n",
        "        myKFold = KFold(n_splits = n_folds, shuffle = True, random_state = 0)\n",
        "\n",
        "        # TODO: Split data into train and validation\n",
        "        for trainIndex, valIndex in myKFold.split(X):\n",
        "\n",
        "                # TODO: Create data loaders to pass to training loop\n",
        "                XTrain, XVal = X[trainIndex], X[valIndex]\n",
        "                yTrain, yVal = y[trainIndex], y[valIndex]\n",
        "\n",
        "                trainLoader = DataLoader(TensorDataset(XTrain, yTrain), batch_size = batch_size,shuffle = True)\n",
        "                valLoader = DataLoader(TensorDataset(XVal, yVal), batch_size = len(XVal))\n",
        "\n",
        "                # TODO: Initialize model, criterion (Cross entropy loss), and optimizer (SGD with various hyperparameters)\n",
        "                model = MyMLP(3072, 2).to('cuda')\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = torch.optim.SGD(model.parameters(), lr = lr, weight_decay=wd)\n",
        "\n",
        "                # Call your training function\n",
        "                train(model, trainLoader, valLoader, n_epochs, optimizer, criterion, verbose=True)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # TODO: Use the trained model to estimate train/val accuracy\n",
        "                    # (Hint: our model outputs logits, argmax is good to get the class prediction corresponding to max logit)\n",
        "                    yPredTrain = model(XTrain).argmax(dim = 1)\n",
        "                    yPredVal = model(XVal).argmax(dim = 1)\n",
        "\n",
        "                    yPredTrain = yPredTrain.to('cpu')\n",
        "                    yTrain = yTrain.to('cpu')\n",
        "                    yPredVal = yPredVal.to('cpu')\n",
        "                    yVal = yVal.to('cpu')\n",
        "\n",
        "                    train_acc = accuracy_score(yPredTrain, yTrain)\n",
        "                    train_accs.append(train_acc)\n",
        "\n",
        "                    val_acc = accuracy_score(yPredVal, yVal)\n",
        "                    val_accs.append(val_acc)\n",
        "\n",
        "        # For each hyper-parameter, I'm storing the parameter values and the mean and standard error of accuracy in a list in \"results\".\n",
        "        train_std, train_mean = torch.std_mean(torch.tensor(train_accs))\n",
        "        val_std, val_mean = torch.std_mean(torch.tensor(val_accs))\n",
        "        rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds\n",
        "        train_se, val_se = train_std / rootn, val_std / rootn\n",
        "        # Storing learning rate, weight decay value, train mean accuracy, standard error, val mean accuracy, standard error\n",
        "        results.append((lr, wd, train_mean.item(), train_se.item(), val_mean.item(), val_se.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "0PR_DL29h5Z0",
        "outputId": "bd569022-1d5d-4611-aed4-fa9c28c95569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training results\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07071067811865477,\n        \"min\": 0.1,\n        \"max\": 0.2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.25,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"0.506 +/- 0.002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.5,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.498 +/- 0.003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1.0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.500 +/- 0.003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "pivot_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7d6d27e8-a0c0-4b91-9b80-09fd2a6d860c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.25</th>\n",
              "      <th>0.50</th>\n",
              "      <th>1.00</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.1</th>\n",
              "      <td>0.506 +/- 0.002</td>\n",
              "      <td>0.500 +/- 0.003</td>\n",
              "      <td>0.504 +/- 0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.2</th>\n",
              "      <td>0.506 +/- 0.002</td>\n",
              "      <td>0.498 +/- 0.003</td>\n",
              "      <td>0.500 +/- 0.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d6d27e8-a0c0-4b91-9b80-09fd2a6d860c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7d6d27e8-a0c0-4b91-9b80-09fd2a6d860c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7d6d27e8-a0c0-4b91-9b80-09fd2a6d860c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a7b9bdb3-e1f0-48b7-9f8b-4756560d7308\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7b9bdb3-e1f0-48b7-9f8b-4756560d7308')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a7b9bdb3-e1f0-48b7-9f8b-4756560d7308 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9b3a1575-de9e-4bdd-a4b2-b6f2af919a46\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9b3a1575-de9e-4bdd-a4b2-b6f2af919a46 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "learning_rate             0.25             0.50             1.00\n",
              "weight_decay                                                    \n",
              "0.1            0.506 +/- 0.002  0.500 +/- 0.003  0.504 +/- 0.003\n",
              "0.2            0.506 +/- 0.002  0.498 +/- 0.003  0.500 +/- 0.003"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation results\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"weight_decay\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07071067811865477,\n        \"min\": 0.1,\n        \"max\": 0.2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.25,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"0.477 +/- 0.007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.5,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.506 +/- 0.013\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1.0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.500 +/- 0.014\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "pivot_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ac658dd0-2329-4f2d-8631-2955c479f7db\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.25</th>\n",
              "      <th>0.50</th>\n",
              "      <th>1.00</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.1</th>\n",
              "      <td>0.477 +/- 0.007</td>\n",
              "      <td>0.500 +/- 0.014</td>\n",
              "      <td>0.483 +/- 0.011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.2</th>\n",
              "      <td>0.477 +/- 0.007</td>\n",
              "      <td>0.506 +/- 0.013</td>\n",
              "      <td>0.500 +/- 0.014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac658dd0-2329-4f2d-8631-2955c479f7db')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac658dd0-2329-4f2d-8631-2955c479f7db button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac658dd0-2329-4f2d-8631-2955c479f7db');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-223f3f6c-f584-4100-a5e7-faf1a7bc364a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-223f3f6c-f584-4100-a5e7-faf1a7bc364a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-223f3f6c-f584-4100-a5e7-faf1a7bc364a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_667f49c7-547a-468a-97cc-8bcc482563ff\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pivot_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_667f49c7-547a-468a-97cc-8bcc482563ff button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pivot_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "learning_rate             0.25             0.50             1.00\n",
              "weight_decay                                                    \n",
              "0.1            0.477 +/- 0.007  0.500 +/- 0.014  0.483 +/- 0.011\n",
              "0.2            0.477 +/- 0.007  0.506 +/- 0.013  0.500 +/- 0.014"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO [3 points]. Print the final result (should be no need to modify code)\n",
        "# You should be able to see a best train acc > 95% , and a best val acc > 80%\n",
        "\n",
        "# Create a DataFrame from the list of tuples, with labeled columns\n",
        "column_names = ['learning_rate', 'weight_decay', 'train_mean', 'train_se','val_mean', 'val_se']\n",
        "df = pd.DataFrame(results, columns=column_names)\n",
        "\n",
        "# Make pretty printable strings, with standard error bars\n",
        "df['train_output'] = df.apply(lambda row: f\"{row['train_mean']:.3f} +/- {row['train_se']:.3f}\", axis=1)\n",
        "df['val_output'] = df.apply(lambda row: f\"{row['val_mean']:.3f} +/- {row['val_se']:.3f}\", axis=1)\n",
        "\n",
        "print('Training results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='train_output')\n",
        "display(pivot_df)\n",
        "\n",
        "print('Validation results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='val_output')\n",
        "display(pivot_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2gTNpu5jB7C"
      },
      "source": [
        "***Hyperparameter Tuning***\n",
        "In this part, I varied\n",
        "\n",
        "1. The model architecture - Added more layers, Increased the size of each hidden layer\n",
        "\n",
        "2. Increase the batch size from 50 to 100\n",
        "\n",
        "3. Increased the Learning Rates and Weight Decays\n",
        "\n",
        "4. Decreased num of epochs to 25\n",
        "\n",
        "As can be seen above, this resulted in pretty bad accuracies. One reason could be the short amount of epochs being used and the other could be the extremely high learning rates which may have contributed to the optimizer not being able to converge.\n",
        "\n",
        "Key Takeaways - Train for a lot more epochs and keep the learning rates reasonably small.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
